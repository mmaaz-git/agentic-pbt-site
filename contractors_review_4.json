[
  {
    "data_row_id": "cmg36i0za5gy90715aq7wnyr0",
    "global_key": "html_bug_report_add73bb4_5faae7f8",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_add73bb4.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "The cumsum() method has a critical logic flaw in line 1550 of pandas/core/arrays/sparse/array.py. When handling non-null fill values (like 0 for integer arrays), it executes return SparseArray(self.to_dense()).cumsum() which creates infinite recursion: the method converts to dense, wraps in a new SparseArray (with same non-null fill value), and calls cumsum() again. This violates the method's contract to compute cumulative sum and crashes the application."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes this is a bug. SparseArray.cumsum() is entering an infinite recursion, because fill_value is not null. This is specifically happening when it's 0. \n\nThis should work regardless of the fill_value. So this is a bug because infinite recursion is triggered and the function can never return back to the program. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the method violates its own documented contract and crashes. the docstring for sparsearray cumsum explicitly states that the result fill value will be numpy nan regardless, but the non null fill value branch rewraps the dense array back into a sparsearray and recursively calls cumsum again, which never changes the non null fill state and causes infinite recursion and a recursionerror. this affects common integer sparse arrays since they default to fill value 0, so it is not an edge case. the implementation clearly contradicts the stated behavior and leads to a crash, which is a correctness and stability issue. maintainers would welcome this because it is user visible on a public api, trivial to reproduce with a one element array, has a minimal safe fix that aligns with the docs, and impacts real workloads using sparse integer data. it is not a security issue because there is no memory corruption or privilege escalation and no data leakage, though it can cause a denial of service style crash if reachable via untrusted input. References: https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/arrays/sparse/array.py#L1510-L1540 ,https://pandas.pydata.org/docs/reference/api/pandas.arrays.SparseArray.html"
      }
    ]
  },
  {
    "data_row_id": "cmg36i0zg5gya0715q060o5kt",
    "global_key": "html_bug_report_592b8750_372e67c1",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_592b8750.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because it violates the fundamental round-trip guarantee expected from scipy's Harwell-Boeing format implementation. The hb_write function produces invalid files that cannot be read back by hb_read, specifically when positive values are followed by negative values in the data array. The evidence is clear: the file output shows concatenated values like \"0.0000000000000000E+00-0.0000000000000000E+00\" without proper spacing, which violates the Harwell-Boeing format specification requiring exactly 24-character fields for E24.16 format. The root cause is an incorrect width-1 adjustment in the format parser that converts Fortran E24.16 to Python %23.16E instead of the correct %24.16E."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This appears to be bug, between 0.0 and -0.0. Although mathematically equivalent, I believe the hb_write() function is writing both zeroes the same way, then hb_read encounters an error. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because it breaks the expected round trip behavior promised by the scipy io harwell boeing interface where a matrix written with hb write should be readable by hb read without data loss or parse failure. the provided property based test and minimal reproducer show a deterministic failure on a simple case with rows 1 cols 2 and values 0.0 and negative zero where hb read raises unmatched data. the harwell boeing spec requires fixed width e fields such as e24.16 so each value occupies an exact width including a leading space for positive numbers which guarantees separation between adjacent fields. the code path described converts fortran e24.16 to python percent 23.16e by subtracting one from the width then repeats the format with no separators which removes the separating space when a positive value is followed by a negative value causing two numbers to be concatenated and unparsable. this is a direct violation of the hb fixed width rule and explains why positive followed by negative fails while negative followed by positive does not. therefore the observed behavior violates both the documentation expectation of round trip and the format specification making it a genuine logic defect. maintainers would welcome the report because it is clear reproducible standards based and has a minimal one line fix to use the full width. this is not a security issue because it results in a safe parse error rather than code execution privilege escalation or data exfiltration although it can cause failed reads and interoperability issues. References: https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.hb_write.html , https://math.nist.gov/MatrixMarket/formats.html#hb"
      }
    ]
  },
  {
    "data_row_id": "cmg36i0zl5gyb0715vo3s92hf",
    "global_key": "html_bug_report_3539ba5d_d53658c7",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_3539ba5d.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Floyd-Warshall requires diagonal to be zero (distance from node to itself). F-contiguous arrays return original non-zero diagonal values. Internal ValueError is suppressed, causing silent data corruption without warning."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "Floyd_warshall when overwrite=True fail to parse contiguous arrays. This bug is causing the code to properly parse the shortest paths between all pairs of vertices. Diagonals of the vertices should equal to 0. This should work fine for C and fortran "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": " this is a real bug because floyd warshall must always produce a zero diagonal for all pairs shortest paths and must modify distances according to the algorithm, but with fortran contiguous dense input the function returns the original matrix unchanged with nonzero diagonal, which is mathematically incorrect and violates expected behavior. the docs state that overwrite true applies only if the input is dense c ordered float64, but they do not document that non c ordered input silently yields wrong results rather than converting or raising, so the behavior contradicts reasonable user expectations and standard api practice. the issue reproduces for both overwrite true and overwrite false and also via shortest path with method fw, while other algorithms like dijkstra bellman ford and johnson behave correctly on the same inputs, isolating the fault to the floyd warshall path. the cython layer raises valueerror ndarray is not c contiguous and that exception is ignored, causing a silent no op and returning incorrect results, which is textbook silent data corruption. maintainers would welcome this because it is a correctness bug in a core public api with a clear reproducer and straightforward mitigation either reject non c ordered input with a clear error or convert to c contiguous before calling the cython kernel and add tests. it is not a security issue because there is no code execution or memory safety angle, but it does pose data integrity risk and could mislead downstream computations if unnoticed. References: https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.floyd_warshall.html ,  https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.shortest_path.html , https://github.com/scipy/scipy/blob/v1.16.1/scipy/sparse/csgraph/_shortest_path.pyx"
      }
    ]
  },
  {
    "data_row_id": "cmg36i0zq5gyc0715ooswjq8u",
    "global_key": "html_bug_report_8cabc99d_419804ff",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_8cabc99d.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Line 1550 executes return SparseArray(self.to_dense()).cumsum() creating infinite recursion. When fill_value is non-NaN (like 0), the new SparseArray inherits the same fill_value, causing cumsum() to call itself repeatedly until stack overflow. Violates documented behavior that promises to convert non-NaN fill values to NaN in result."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the documented contract says the result of sparsearray cumsum will have fill value np nan regardless of the original fill value, yet the implementation path when fill value is non nan converts to dense then constructs a new sparsearray that infers the same non nan fill value and calls cumsum again which reenters the same branch causing infinite recursion and a crash. the provided minimal example with fill value zero and the failing empty array input demonstrate the crash via recursionerror. this both violates explicit documentation and breaks a basic expectation that cumsum returns a result of the same length rather than crashing. the property based test asserts length preservation which should hold, but recursion prevents any result from being produced. the suggested fix of computing cumsum on the dense array once and returning a sparsearray with fill value set to np nan aligns with the docs and eliminates the recursive call loop. References: https://pandas.pydata.org/docs/reference/api/pandas.arrays.SparseArray.html , https://github.com/pandas-dev/pandas/blob/master/pandas/core/arrays/sparse/array.py"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "SparseArray.cumsum() enters an infinite recursion when fill_value is non-NaN because it executes SparseArray(self.to_dense()).cumsum(): that constructs another SparseArray with the same non-NaN fill, which re-hits the same branch and calls cumsum() again, looping until a RecursionError. This contradicts the documented behavior (compute cumulative sum and, in the result, use np.nan as the fill) and crashes for common integer sparse arrays (fill_value=0). Hence, it\u2019s a genuine crash-level logic bug."
      }
    ]
  },
  {
    "data_row_id": "cmg36i0zv5gyd07158hq37phg",
    "global_key": "html_bug_report_584f3cc9_6b49cd96",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_584f3cc9.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Line 1550 executes return SparseArray(self.to_dense()).cumsum() creating infinite recursion. When fill_value is non-null (like 0), the new SparseArray inherits the same fill_value, causing cumsum() to call itself repeatedly until stack overflow. The method should compute cumsum on the dense array first, then convert back to sparse."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "the behavior violates both documentation and expected semantics. with a non null fill value like 0 the code path in sparsearray cumsum checks not self null fill value then calls sparsearray self to dense cumsum which constructs a new sparsearray and calls cumsum again. since the new sparsearray retains the same non null fill value the same branch is taken leading to infinite recursion and a recursionerror even for trivial inputs like [0] or [1 2 3]. the docs state cumsum should return a sparsearray of cumulative sums and preserve positions of nans but it instead crashes. computing cumsum on the dense array then wrapping back in sparse fixes the loop and matches dense results as shown by the provided property test and minimal reproduction. References: https://github.com/pandas-dev/pandas/blob/main/pandas/core/arrays/sparse/array.py , https://pandas.pydata.org/docs/reference/api/pandas.arrays.SparseArray.html , https://github.com/pandas-dev/pandas/issues/46658"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "SparseArray.cumsum() dispatches to return SparseArray(self.to_dense()).cumsum() whenever the array\u2019s fill_value is non-null (e.g., 0). That constructs a new SparseArray with the same non-NaN fill, then immediately calls its cumsum() which hits the exact same branch again, leading to infinite self-recursion and a RecursionError. This contradicts the documented behavior (compute a cumulative sum and return a sparse result) and makes the operation unusable for the very common case of fill_value=0. The minimal repro and Hypothesis runs show it crashes even for tiny inputs like [0] or [1,2,3]. A correct implementation should compute the cumsum on the dense data and then wrap it once (and, per docs, likely with fill_value=np.nan)."
      }
    ]
  },
  {
    "data_row_id": "cmg36i1015gye07155ilvxnpo",
    "global_key": "html_bug_report_e14f3f0c_501b2b2c",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_e14f3f0c.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The function uses random.sample() which samples WITHOUT replacement, but bootstrapping by definition requires sampling WITH replacement. This causes zero variance in bootstrap estimates (all samples are just permutations), making confidence intervals and uncertainty estimates completely wrong. Violates the docstring's explicit claim of \"random sampling with replacement.\""
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the function docstring promises bootstrap sampling with replacement yet the implementation uses random.sample which samples without replacement. bootstrapping by definition requires with replacement draws so the implementation violates both the documented contract and the statistical definition. the consequence is incorrect sampling distributions. when size equals the series length each resample is just a permutation so statistics like the mean and median do not vary across samples leading to near zero variance and misleading histograms. when size is smaller you still get the wrong distribution that underestimates uncertainty compared to proper bootstrap. maintainers would welcome the report because it is a clear correctness issue with a minimal change to fix by switching to random.choices or numpy choice with replace true. this is not a security issue because there is no code execution or boundary bypass risk only statistical correctness is affected. References: https://pandas.pydata.org/pandas-docs/version/2.2.3/reference/api/pandas.plotting.bootstrap_plot.html , https://docs.python.org/3/library/random.html#random.sample , https://en.wikipedia.org/wiki/Bootstrapping_(statistics)"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The function advertises \u201crandom sampling with replacement,\u201d but the implementation calls random.sample, which samples without replacement. That means each bootstrap \u201csample\u201d is merely a permutation of the original data, producing identical statistics across resamples (e.g., identical means), zero estimated variance, and incorrect confidence intervals. This contradicts both the docstring and the statistical definition of bootstrap resampling (which requires replacement), so it\u2019s a clear correctness bug."
      }
    ]
  },
  {
    "data_row_id": "cmg36i1055gyf0715nvjd9uv6",
    "global_key": "html_bug_report_54c34bcc_005d7c76",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_54c34bcc.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "The function silently corrupts data by returning incorrect values (0 for -2^64, -1 for -2^64-1) instead of raising errors. This violates JSON specification expectations that parsers should either correctly parse values or raise clear errors. The inconsistent behavior pattern (some values raise ValueError, others silently corrupt) makes it particularly dangerous and unpredictable."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the deserializer in pandas io json ujson loads silently returns wrong numeric values at and below minus two to the power of sixty four instead of raising an error or producing the correct integer. that breaks json predictability as described in rfc 7159 and violates the fundamental serialize then deserialize round trip property since ujson dumps emits the exact string for the large negative integer but ujson loads returns zero or minus one. the behavior is also internally inconsistent because nearby out of range negatives raise value error while the exact boundary wraps to zero or minus one which strongly indicates an integer overflow in the negative path. the python standard library json module correctly returns arbitrary precision integers for the same inputs which sets a clear expectation in python ecosystems. maintainers would welcome this report because it targets a user facing core path with clear minimal reproduction and a feasible fix outline bounded to integer parsing and boundary checks in the c fast path. this is not a security issue in the classic sense because it does not enable code execution memory corruption or bypass of security boundaries though it is a high severity integrity bug that can lead to silent data corruption in downstream systems. References: https://docs.python.org/3/library/json.html , https://datatracker.ietf.org/doc/html/rfc7159"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "pandas.io.json.ujson_loads silently returns wrong integers at and below the -2^64 boundary (e.g., -2^64 \u2192 0, -2^64-1 \u2192 -1) instead of either parsing correctly or raising an error. That violates round-trip expectations (ujson_dumps \u2192 ujson_loads), contradicts Python\u2019s json behavior (which supports arbitrary-precision ints), and constitutes silent data corruption. Even if the root cause sits in the upstream ultrajson C library, the pandas convenience wrapper exposes the behavior to users and therefore surfaces as a correctness bug in pandas\u2019 API surface."
      }
    ]
  },
  {
    "data_row_id": "cmg36i10a5gyg07153ikat2eo",
    "global_key": "html_bug_report_dd577175_2091b926",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_dd577175.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The function unconditionally removes last 2 bytes with stdout[:-2].decode(ENCODING) before decoding UTF-8, violating fundamental principles: (1) Slicing bytes before decoding can split multi-byte UTF-8 characters causing UnicodeDecodeError, (2) Causes silent data loss for any content (especially short strings), (3) Based on incorrect assumption that PowerShell always appends CRLF, (4) No verification if bytes actually end with CRLF before removal."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the function unconditionally removes the last two bytes from the clipboard bytes before decoding. that violates utf8 character boundaries and will either silently truncate valid data or raise a unicodedecodeerror when a multibyte character is split. the property based test and repro show both failure modes with short ascii like 00 becoming empty and with a multibyte tail like 0\u0800 failing to decode. the assumption that powershell always appends crlf is not a safe api contract and the code does not check for it. decoding first and only stripping crlf if actually present or using powershell get clipboard raw avoids both corruption and crashes. maintainers would welcome this because it affects user visible io on wsl, causes silent data loss and crashes, and the fix is straightforward and low risk. this is not a security issue in the strict sense because it does not enable code execution or privilege escalation, though it can cause denial of service style crashes and data integrity problems. References:  https://github.com/pandas-dev/pandas/blob/main/pandas/io/clipboard/__init__.py , https://pandas.pydata.org/docs/reference/api/pandas.read_clipboard.html ,  https://learn.microsoft.com/powershell/module/microsoft.powershell.management/get-clipboard"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "paste_wsl() blindly slices stdout[:-2] before decoding, assuming PowerShell always appends \\r\\n. This unconditionally truncates two bytes, corrupting short payloads (e.g., \"00\" \u2192 \"\") and, worse, can split a multi-byte UTF-8 sequence (e.g., '0\u0800' \u2192 b'0\\xe0\\xa0\\x80' becomes b'0\\xe0') causing a UnicodeDecodeError. The behavior both silently loses data and can crash on valid UTF-8, a clear violations of correctness and robustness. The function does not verify CRLF presence, so the failure mode is inevitable whenever CRLF isn\u2019t present."
      }
    ]
  },
  {
    "data_row_id": "cmg36i10e5gyh07152m3bdp9z",
    "global_key": "html_bug_report_8d677a62_e4e12303",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_8d677a62.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "For linear function f(x) = A @ x, Jacobian should equal matrix A. The jacobian function's internal vectorization calls f with higher-dimensional arrays, but @/np.matmul broadcasting rules produce unexpected shapes ((2, 3, 8) instead of (3, 2, 8)), scrambling element ordering. Only affects @/np.matmul; np.dot works correctly due to different broadcasting behavior."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because for a linear function f of x defined as a times x the jacobian must equal a entrywise yet the report shows jacobian returns a permuted matrix when the function uses the at operator or numpy matmul. the cause is the internal vectorization in jacobian which inserts an abscissae axis before calling the user function so f receives inputs with two leading axes. numpy matmul interprets the last two dimensions as matrix axes and therefore consumes the wrong axis when the abscissae axis is among the last two leading to reordered outputs and a scrambled jacobian. in contrast numpy dot contracts only the last axis and treats other axes as batch which preserves the abscissae axis and returns the correct jacobian equal to a. this violates the mathematical property of linear maps and reasonable user expectations for a derivative routine and there is no documented warning about this limitation in the report. hence it is a correctness bug not an intended behavior difference. references: https://numpy.org/doc/stable/reference/generated/numpy.matmul.html , https://peps.python.org/pep-0465/ , https://docs.scipy.org/doc/scipy/reference/generated/scipy.differentiate.jacobian.html"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "For a linear map f(x)=A@x, the Jacobian must equal A. This repro shows jacobian(f, x) returning a matrix with columns/rows scrambled when f uses @/np.matmul, while a manual finite-difference check matches A. The likely root cause is that jacobian internally batches perturbations and calls f with higher-dimensional inputs; @/np.matmul then applies generalized matmul broadcasting (treating the last two axes as matrix dims and broadcasting the rest), yielding an output whose axes differ from what the wrapper expects. That misaligned shape is then interpreted as the Jacobian, producing incorrect entries i.e., a silent mathematical error. Functions using np.dot (which broadcasts differently) don\u2019t trigger the mismatch, further supporting that this is a batching/axis-order bug in jacobian rather than in the user function."
      }
    ]
  },
  {
    "data_row_id": "cmg36i10j5gyi07155lf055n1",
    "global_key": "html_bug_report_5c69a9fc_0f31a3db",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_5c69a9fc.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "LinearNDInterpolator violates the fundamental mathematical contract of interpolation by returning NaN at one of its own input data points. Linear interpolation must pass through all data points used to construct it. The root cause is numerical precision issues in the underlying Delaunay triangulation's find_simplex method, which incorrectly classifies a point as \"outside convex hull\" despite being used to construct the triangulation."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "LinearNDInterpolator is a piecewise-linear interpolant over a Delaunay triangulation; by definition it should reproduce the input values exactly at the training points. This repro shows interp(points) returning NaN for one of the original points. Tracing the behavior indicates find_simplex is reporting -1 (outside the hull) for a vertex that was used to build the triangulation is almost certainly due to numerical tolerance/robustness issues in the simplex containment test. Because the interpolator then falls back to fill_value (default NaN), it violates the fundamental interpolation contract (exactness at data sites). This is therefore a correctness bug rather than user error."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "A linear interpolator built on a Delaunay triangulation should return the training value at each vertex; returning NaN at an input vertex violates that contract. The behavior points to a misclassification of the query point as \u201coutside\u201d (likely via find_simplex == -1) due to numerical issues at or near simplex boundaries. The report provides a minimal snippet and Hypothesis seed; I reproduced the failure locally. This is therefore a correctness bug rather than an expected edge case."
      }
    ]
  },
  {
    "data_row_id": "cmg36i10o5gyj0715uc1r90wy",
    "global_key": "html_bug_report_921a8a72_4a5fee3d",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_921a8a72.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The pandas interchange protocol silently corrupts categorical data by converting missing values (code -1) into valid category values. The modulo operation categories[codes % len(categories)] transforms -1 to 0 when there's one category, mapping missing values to the first category before set_nulls can identify them as missing. This violates data preservation principles - missing values should never become valid dat"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It\u2019s a correctness bug that causes silent data corruption. In pandas categoricals, code -1 is the sentinel for a missing value. In categorical_column_to_series, the line that maps codes to labels via categories[codes % len(categories)] remaps -1 to a valid index (-1 % 1 == 0, -1 % 3 == 2, etc.), turning NaNs into legitimate category values before the later set_nulls step can recognize and restore missing values. This minimal repro and property-based test show a NaN round-tripping to a category (\u201ca\u201d / \u201c0\u201d), which is a clear violation of pandas\u2019 categorical semantics and the interchange protocol\u2019s duty to preserve nulls."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The interchange conversion path (categorical_column_to_series) maps categorical codes to category labels using a modulo operation, which transforms the missing-value sentinel -1 into a valid index when there is exactly one category (-1 % 1 == 0). This silently converts NaN into a valid category value, violating categorical semantics where code -1 denotes missing. I reproduced the issue locally: after passing a categorical with a single category and a -1 code through the interchange path, the resulting series contains the category (e.g., \"a\") instead of NaN. The report\u2019s analysis matches the observed behavior precisely."
      }
    ]
  },
  {
    "data_row_id": "cmg36i10t5gyk0715nd8vx6ds",
    "global_key": "html_bug_report_4398119e_f371456a",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_4398119e.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The pandas interchange protocol silently converts NA (missing) values to False in nullable boolean columns, causing critical data corruption. The function primitive_column_to_ndarray fails to properly handle nullable boolean columns, losing the ability to represent missing values and converting them to definite False values without any warning or error."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Because the interchange round-trip changes both values and dtype: nullable boolean with NA becomes non-nullable bool with False. That\u2019s silent data corruption and violates the interchange protocol\u2019s expectation that nulls be preserved via validity buffers, and pandas\u2019 semantic contract for BooleanDtype (which must distinguish True, False, and NA). The repro shows values=[None] round-tripping to False and dtype: bool, proving null information is dropped during from_dataframe\u2019s primitive column path rather than producing a BooleanArray with the original NA.\n"
      },
      {
        "rater_id": "cmfmmbs2v04is072efydz5lc4",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "NA values are unknown or missing information and converting them to boolean is not the right behaviour. "
      }
    ]
  },
  {
    "data_row_id": "cmg36i10y5gyl0715zp7m63jp",
    "global_key": "html_bug_report_73814efa_dcdd6661",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_73814efa.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "SparseArray.astype() silently corrupts data by replacing actual values with the new fill_value when converting to a SparseDtype with a different fill_value. The bug violates the fundamental contract of astype() which must preserve array values while only changing type representation. When all values equal the original fill_value, they get incorrectly replaced with the new fill_value, causing complete data loss without warning."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Because astype() is expected to change representation only, not values. Here, casting a SparseArray to a SparseDtype with a different fill_value replaces any elements equal to the old fill value with the new fill value (e.g., [0] with fill_value=0 becomes [1.] after astype(SparseDtype(float64, fill_value=1))). That violates the fundamental invariant that s.astype(new_dtype).to_dense() == s.to_dense().astype(new_subtype) and contradicts pandas\u2019 own docs/examples for SparseArray.astype. The corruption stems from reconstructing the dense array seeded with the new fill_value while sp_values/indices are empty for positions equal to the old fill value, thus silently overwriting real data."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a confirmed bug where SparseArray.astype() corrupts data when changing the fill_value. It breaks the fundamental invariant that casting should preserve values. The issue arises when all elements are equal to the current fill_value, causing the internal sparse representation to collapse into an empty structure. During conversion with a new fill_value, the original values are incorrectly replaced, leading to silent data loss without any warnings. Essentially, the sparse array treats zero values as missing and replaces them with the new fill values. Since this behavior is undocumented, it introduces ambiguity and unexpected outcomes for users."
      }
    ]
  },
  {
    "data_row_id": "cmg36i1125gym0715wpbjg4g6",
    "global_key": "html_bug_report_e8f40829_f90ab52b",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_e8f40829.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "pandas CSV round-trip silently converts large integers beyond int64 range from Python int objects to strings, causing data corruption and breaking arithmetic operations. The parser incorrectly falls back to string dtype instead of attempting to parse overflow integers as Python int objects, violating data preservation principles."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It is a bug because a numeric token that pandas originally held as a Python int (in an object column) is round-tripped through to_csv/read_csv and comes back as a string, silently changing the type and breaking arithmetic. CSV has no schema, but pandas\u2019 parser already performs numeric inference; the problem here is that when an integer literal overflows int64, inference falls back to a string instead of producing a Python int. That violates a reasonable (and widely relied-upon) round-trip expectation: read_csv(StringIO(df.to_csv())) should preserve both values and numeric semantics when the text unambiguously represents an integer. The failure is silent (no warning), yields different dtypes/element types, and makes expressions like col + 1 error out after the round-trip, which is a clear evidence of data corruption at the type level."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "Yes this is a bug, when pandas reads back integers that exceed int64, to CSV, when written back it's a string. Another issue here is that it's silently being converted from int to string, which leads to data corruption"
      }
    ]
  },
  {
    "data_row_id": "cmg36i1175gyn0715y739o4gh",
    "global_key": "html_bug_report_350d2219_3e346867",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_350d2219.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The modulo operation categories[codes % len(categories)] silently corrupts data by remapping out-of-bounds codes to valid categories and preventing proper null handling. This violates pandas' own standards - Categorical.from_codes() validates codes and raises errors for out-of-bounds values, but the interchange function silently maps them, causing invalid data to appear valid and null values to become actual data points."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It\u2019s a bug because it\u2019s categorical_column_to_series maps codes to categories using categories[codes % len(categories)]. That silently turns invalid (out-of-bounds) codes into valid category indices and also converts the null sentinel (e.g., -1) into a real category when len(categories)>1 (and always when len=1, since -1 % 1 == 0). This contradicts Pandas\u2019 own categorical semantics (Categorical.from_codes raises for codes outside [-1, len-1]) and prevents later null handling from recognizing missing values. The net effect is silent data corruption: invalid inputs appear valid and missing values become real categories with no error or warning."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It appears that modulo is being applied to out of bounds categories. This is a problem because data out of bounds should be non existent, it should not be able to be written to or accessed. Moreover this bug is silently corrupting data and violating data integrity. \n\n\n\n"
      }
    ]
  },
  {
    "data_row_id": "cmg36i11c5gyo0715evabcjau",
    "global_key": "html_bug_report_df8be824_0911cb2d",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_df8be824.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "pandas JSON round-trip fails for integers outside int64 range, causing parse errors for values below int64 minimum or silent data corruption for values above int64 maximum. The to_json() function successfully serializes large integers, but read_json() fails to parse them, violating the fundamental round-trip contract. The issue stems from ujson's int64 limitations, while Python's standard json module handles these values correctly."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Its a bug because to_json() successfully emits valid JSON containing integers outside the int64 range, but read_json() (via ujson_loads) cannot faithfully read those same numbers: values below -2^63 raise a parse error (\u201cValue is too small\u201d), while values above 2^63-1 are silently misread (e.g., 9223372036854775808 becomes -9223372036854775808). That breaks the round-trip guarantee and corrupts data without warning. Python itself supports arbitrary-precision integers and the stdlib json can round-trip these correctly; the failure is due to pandas\u2019 fast-path JSON parser imposing int64 bounds and not providing a safe fallback. The result is either a crash or silent type/value corruption, both of which are clear correctness bugs for a serialization API."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug, but it may be more with json vs pandas. integers reaching the int64 maximum, should not silently be converted from positives to negative values. This would absolutely lead to data corruption. \n\nThe error message of \"Values to small\" is definitely misleading, it should give more insight as to what the issue is."
      }
    ]
  },
  {
    "data_row_id": "cmg36i11h5gyp0715geovqfzc",
    "global_key": "html_bug_report_4c0409fe_a9a46dca",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_4c0409fe.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "SparseArray.cumsum() crashes with infinite recursion for integer arrays with non-null fill values (the default case). The bug occurs because return SparseArray(self.to_dense()).cumsum() creates a new SparseArray that still has _null_fill_value = False, causing infinite recursion. This violates the expected behavior that cumsum() should compute cumulative sums for all valid numeric arrays."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It\u2019s a real bug because SparseArray.cumsum() enters an infinite recursion for arrays whose fill_value is non-null (e.g., the default 0 for integer sparse arrays). The implementation path return SparseArray(self.to_dense()).cumsum() re-creates another SparseArray with the same non-null fill_value, which hits the same branch again, looping until RecursionError. Functionally, cumsum() should compute the cumulative sum, not crash, and it must work for the default integer sparse arrays. The minimal fix i.e. compute on the dense array first, then wrap (SparseArray(self.to_dense().cumsum())) eliminates the recursion and restores correct behavior."
      },
      {
        "rater_id": "cmfmmbs2v04is072efydz5lc4",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because the library when adding up the numbers as SparseArray, the code accidentally keeps calling itself. It should rather adds the numbers and stops when completed."
      }
    ]
  },
  {
    "data_row_id": "cmg36i11m5gyq07155y4mvhpx",
    "global_key": "html_bug_report_32cd9f7e_283259a4",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_32cd9f7e.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because it silently corrupts data and violates documented semantics. In pandas, Categorical.from_codes defines -1 as a sentinel for NaN, during interchange, categorical_column_to_series applies codes % len(categories), turning -1 into a valid index so NaNs become real categories. The dataframe interchange protocol's USE_SENTINEL nulls must preserve sentinel-missing values, but set_nulls returns early when validity is none, skipping sentinel handling. Together, these logic errors change missing values into data without warning, breaking both pandas' docs and the protocol specification"
      },
      {
        "rater_id": "cmfmmbs2v04is072efydz5lc4",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Upon checking, I found that pandas explicitly treats missing values in cateorical columns and the missing entries are represented using internal code -1 so no silent conversion of values."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "1_very_uncertain_mostly_guessing"
        },
        "comments": "This looks like the interchange protocol is mis representing -1 as valid value, which seems to be causing silent data corruption. "
      }
    ]
  },
  {
    "data_row_id": "cmg36i11r5gyr0715viu2z0k4",
    "global_key": "html_bug_report_33012d9b_ffca97c7",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_33012d9b.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Modulo operation codes % len(categories) converts -1 null sentinel to valid index, silently corrupting data (e.g., ['a', None] \u2192 ['a', 'a'])."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the dataframe interchange protocol for categoricals specifies that nulls are encoded as a sentinel value minus one in the codes array, but the pandas consumer reconstructs values using modulo indexing on the codes which wraps minus one to a valid index, turning nulls into real categories. this violates the protocol and silently corrupts data without any warning. the offending code path applies when there is at least one category and uses categories indexed by codes modulo length of categories, so minus one maps to the last category and nulls are lost. afterwards the generic null handling compares materialized values to the sentinel and cannot recover the missing values because the values are now category labels rather than codes. the behavior is reproducible with inputs like a and null resulting in a and a after round trip through the interchange api. the correct reconstruction should use pandas categorical from codes which treats minus one as missing or explicitly mask sentinel codes before any modulo operation and restore nulls after indexing. the developer comment acknowledges sentinel handling intent but the implementation is incorrect, making this a logic error in a user visible interoperability path. References:  https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/interchange/column.py , https://pandas.pydata.org/docs/reference/api/pandas.Categorical.from_codes.html , https://github.com/pandas-dev/pandas/issues/53077"
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It is because missing values in a categorical column (-1 sentinels) are supposed to stay as nulls, but the interchange code mistakenly uses modulo arithmetic that wraps -1 into a valid category index, silently turning nulls into real category values."
      }
    ]
  },
  {
    "data_row_id": "cmg36i11w5gys0715m57uqq3o",
    "global_key": "html_bug_report_17a6a4ba_54c8cdc4",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_17a6a4ba.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "pbcopy/pbpaste commands don't accept \"w\"/\"r\" arguments - these are file I/O modes mistakenly applied to command-line utilities that use stdin/stdout."
      },
      {
        "rater_id": "cmfmmbs2v04is072efydz5lc4",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because pandas is calling the macOS pbcopy and pbpaste with arguments w and r and thise tools don't support, which breaks the clipboard functionality."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a bug because the function init_osx_pbcopy_clipboard is described as invoking the macos pbcopy and pbpaste utilities with extra w and r arguments which those utilities do not accept. pbcopy reads from standard input and pbpaste writes to standard output and neither requires or supports file mode flags, so adding w or r causes illegal option errors and immediate failure. the property based test demonstrates a copy paste round trip should succeed for arbitrary text but it fails even for simple input which is strong evidence the interface contract is broken. the direct reproduction shows calledprocesserror from the subprocess calls and the report cites the exact lines where the invalid arguments are passed, confirming the misuse. therefore yes it is a real functional defect affecting expected clipboard behavior, maintainers would welcome it because it breaks user visible clipboard apis on macos when the pyobjc path is unavailable, and it is not a security issue because it only causes a controlled failure rather than enabling code execution or data leakage. References: https://www.manpagez.com/man/1/pbcopy/ , https://www.manpagez.com/man/1/pbpaste/ , https://github.com/pandas-dev/pandas/blob/main/pandas/io/clipboard/__init__.py"
      }
    ]
  },
  {
    "data_row_id": "cmg36i1215gyt0715jui0u20f",
    "global_key": "html_bug_report_dd193b4d_ed8a6f7b",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_dd193b4d.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Violates fundamental contract - lengths must be non-negative. Function incorrectly applies positive-step defaults to negative-step slices, returning negated actual lengths instead of proper counts."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the function promises to return the expected length of target indexer yet returns negative values for negative step slices which violates the meaning of length and python conventions where lengths are always non negative. the bug report shows a failing hypothesis test and minimal repro where length_of_indexer returns negative values while len target slice is positive. the report also pinpoints the logic error that start and stop defaults are computed as if step were positive then the code swaps for step less than zero which makes the computed difference negative. correct handling requires different defaults when step is negative or using slice indices to mirror python slicing semantics. this reasoning supports yes for real bug high confidence yes for maintainers welcome high confidence and no for security because it is a correctness issue not a direct exploit path.\n"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This appears to be a bug, length_of_indexer should provide the length of elements for the slice but it's returning different values then len(target[indexer]). This could lead to off by one errors. This also violates the contract with the function. "
      }
    ]
  },
  {
    "data_row_id": "cmg36i1255gyu0715eo6wks4z",
    "global_key": "html_bug_report_8b18055e_9da50dbc",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_8b18055e.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Mismatch between explicit models (unilinear) and implicit ODR mode (job=1) causes ODRPACK Fortran code to access invalid memory, crashing interpreter instead of raising proper Python exception."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "this is a real bug because setting job mod 10 equal to 1 selects implicit odr while the provided model unilinear is explicit and has model implicit equal to 0. the scipy odr wrapper forwards this incompatible combination to odrpack without validating compatibility, which leads to a native segfault instead of raising a python error. libraries must reject invalid parameter combinations with a clear exception such as valueerror, not crash the interpreter. maintainers would welcome this because the report is precise, includes a minimal and deterministic reproducer, identifies the parameter mismatch, and proposes a straightforward guard that checks fit type against the model implicit flag. for security, the impact is denial of service via interpreter crash if untrusted inputs can influence job or model selection, but there is no indication of memory disclosure or code execution. References: https://docs.scipy.org/doc/scipy/reference/odr.html , https://github.com/scipy/scipy/blob/main/scipy/odr/_odrpack.py , https://netlib.org/odrpack/"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes this is definitely a bug, segmentation fault should not occur, this means the program crashed. The library should raise some form of exception or error but never crash. This could potentially lead to security vulnerabilities, buffer overflows, incorrect array calculations, etc. "
      }
    ]
  },
  {
    "data_row_id": "cmg36i12a5gyv0715m78tab0c",
    "global_key": "html_bug_report_be2f767a_768fd9bd",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_be2f767a.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Modulo operation codes % len(categories) converts -1 null sentinel to valid category index before set_nulls can process it, silently converting nulls to actual data points."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the interchange path for categorical columns applies a modulo operation to codes before null restoration which remaps the sentinel for missing values minus one into a valid category index for example minus one mod three equals two thereby converting nulls into real categories this violates the interchange spec where categorical nulls are represented by a sentinel and must be preserved for set nulls to recognize and restore missingness the provided property test and minimal repro show the null mask changes after interchange proving silent data corruption maintainers would welcome this because it is a clear spec violation with a precise failing test and a minimal fix path and it impacts correctness across common workflows it is not a security issue because there is no code execution privilege escalation or data leakage it is a data integrity correctness bug. References: https://github.com/pandas-dev/pandas/blob/main/pandas/core/interchange/from_dataframe.py , https://github.com/pandas-dev/pandas/blob/main/pandas/core/interchange/column.py"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The interchange protocol seems to be corrupting categorical values -1. Which is causing issues with round trip conversions. This is also violating the panda protocol , and the data corruption is silent. "
      }
    ]
  },
  {
    "data_row_id": "cmg36i12e5gyw0715738u7867",
    "global_key": "html_bug_report_fdda9e3d_650832a6",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_fdda9e3d.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Modulo operation codes % len(categories) converts -1 null sentinel to valid category index (e.g., -1 % 3 = 2 \u2192 'c'), violating pandas' documented -1 sentinel specification."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the report shows a categorical column with missing values encoded by code minus one being converted to valid categories during a round trip through the interchange api, violating pandas documented behavior that minus one represents missing in categorical codes. the property based test and the minimal repro both demonstrate that a minus one code becomes the last category due to a modulo operation in categorical column to series, which maps minus one mod number of categories to a positive index, causing silent data corruption. the cited code path applies modulo before any null handling and set nulls cannot fix it because the column advertises use sentinel minus one with no validity mask, so after mapping codes to category labels there is no way to detect the original minus one positions. this directly contradicts the pandas categorical contract and the interchange protocol expectation to preserve nulls, so it is a logic error in a user facing conversion path. the impact is high because users can unknowingly lose nulls and compute incorrect statistics and models. maintainers would welcome it because it is clearly reproducible, precisely located, easy to test with a simple invariant, and a targeted fix is straightforward either by constructing the categorical from codes so minus one yields missing or by masking minus ones before indexing. it is not a security issue because it does not enable code execution or access control bypass, but it remains a serious data integrity flaw. References: https://pandas.pydata.org/docs/user_guide/categorical.html , https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/interchange/from_dataframe.py , https://data-apis.org/dataframe-protocol/latest/API.html"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "This is a confirmed bug where null values in categorical columns are corrupted during interchange. The implementation incorrectly maps the null sentinel value (-1) to a valid category using modulo arithmetic, violating pandas' categorical specification. This silent data corruption affects data integrity and must be fixed to preserve missing values."
      }
    ]
  },
  {
    "data_row_id": "cmg36i12j5gyx07153czqj0g5",
    "global_key": "html_bug_report_30dc0851_e1a4f252",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_30dc0851.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It violates the fundamental mathematical contract of the pd.cut() function. The function assigns values to bins that don't actually contain them, as demonstrated by the value 1.15625 being assigned to interval (0.578, 1.156] when 1.15625 > 1.156. The precision parameter is intended to only affect display formatting, but it's incorrectly being used for interval membership testing, causing values to be excluded from their mathematically correct bins. The root cause is that precision rounding is applied to bin boundaries before creating the IntervalIndex, rather than only affecting display labels."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "This is a legitimate bug because the precision rounding modifies the actual bin boundaries used for interval membership testing, not just display labels. The function violates its core contract by assigning values to bins that don't mathematically contain them, as demonstrated where 1.15625 is placed in (0.578, 1.156] despite exceeding the right boundary. This creates incorrect categorical assignments that undermine data analysis reliability."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug, pd.cut is assigning a value but due to precession handling that value is not correct. The value that is being assigned to pd.cut val is 1.156, even though the rounded value 1.5625 is greater than 1.156. This is also another example of silent data corruption since no error has been given "
      }
    ]
  },
  {
    "data_row_id": "cmg36i12n5gyy0715sav4e4jb",
    "global_key": "html_bug_report_7f655ce0_4d089fdb",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_7f655ce0.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because it violates fundamental plugin system design principles by allowing one faulty plugin to break the entire plugin chain and mask original errors. The build_wrapper function lacks exception handling around plugin event handler calls, causing handler exceptions to prevent subsequent handlers from executing and replacing original validation errors with plugin-specific exceptions. This breaks plugin isolation, a core requirement for robust plugin architectures. The bug affects all event handler types (on_error, on_success, on_exception, on_enter) and can lead to silent data loss where validation results are replaced by plugin exceptions."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because handler exceptions in build_wrapper are not isolated which causes two concrete failures that violate reasonable plugin expectations and the report\u2019s demonstrated property tests first error masking the original validationerror or result is replaced by the exception from the first failing handler so users lose the true cause or valid output second chain interruption one bad handler prevents later handlers from running breaking plugin composability the source shows direct calls to on_enter on_error on_exception and on_success without per handler try except so a raised exception short circuits the loop and masks the original control flow this behavior is surprising for plugin systems where handlers are expected to be best effort and non disruptive the repro and hypothesis test correctly capture these effects across error and success paths maintainers mark plugins as experimental but no docs suggest handlers should abort validation by default so the current behavior is inconsistent with common plugin design and is practically harmful. references:\nhttps://github.com/pydantic/pydantic/blob/main/pydantic/plugin/_schema_validator.py"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The issue described in this report is better understood as a design limitation rather than a direct bug. Although exceptions are correctly caught, the current implementation follows a \u201cfail fast\u201d strategy, raising a plugin exception immediately instead of allowing all plugins to execute and then consolidating their exceptions. This report, therefore, highlights a constraint in the design approach taken during the module\u2019s development, rather than an isolated implementation error."
      }
    ]
  },
  {
    "data_row_id": "cmg36i12t5gyz0715z21naiqz",
    "global_key": "html_bug_report_4b07a27b_3e3f7292",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_4b07a27b.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Line 1550 calls SparseArray(self.to_dense()).cumsum() which creates new SparseArray with same fill_value=0, triggering infinite recursion since _null_fill_value remains False."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because cumsum is expected to return the cumulative sum of the array but the implementation for sparse arrays with a non null fill value such as zero for integers converts to dense then constructs a new sparse array and calls cumsum again which recreates the same non null fill value and reenters the same branch causing infinite recursion and a recursion error this violates the documented and de facto expectation that cumsum on sparse integer arrays should behave like numpy cumsum and match the dense result the bug is easy to reproduce with a single zero input and does not depend on exotic conditions maintainers would welcome this because it breaks a core numeric operation for the default integer sparsearray path has a minimal and clear fix compute numpy cumsum on the dense array and wrap back and includes a minimal repro and failing input security wise it is not a vulnerability in the usual sense there is no code execution or data leak though it can crash a process if invoked so the impact is stability not confidentiality or integrity. References: https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html , https://github.com/pandas-dev/pandas/blob/main/pandas/core/arrays/sparse/array.py"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is causing an infinite recursion. The program can not return back to regular execution. The cumsum() method should calculate the cumulative sum, of the array. \n\nSince this is an infinite recursion the program execution of course crashes. The main issue here is that the function is creating a SparseArray and then calling cumsum() on it's self, leading to the infinite recursion. "
      }
    ]
  },
  {
    "data_row_id": "cmg36i12y5gz00715vtv3hdog",
    "global_key": "html_bug_report_1e890059_e1dbb6f8",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_1e890059.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Assertion assert ext or setup_args permits ext=None when setup_args exists, but code immediately accesses ext.sources without checking if ext is None, causing AttributeError."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because handle_special_build asserts that either ext or setup_args may be present using an or condition then immediately dereferences ext.sources without checking for none which crashes when only make_setup_args is defined. the caller get_distutils_extension explicitly accepts a none ext and constructs a default distutils extension which confirms that returning none for ext is part of the intended contract. the module documentation and examples show both functions but do not require both so a configuration with only make_setup_args is a valid use case. therefore the observed attributeerror is a logic error violating the function\u2019s own contract and the caller\u2019s expectations which substantiates yes for real bug and yes for maintainers welcome and no for security since it is a reliability crash only and does not cross trust boundaries or enable new code execution beyond the already trusted pyxbld execution model. references: https://cython.readthedocs.io/en/latest/src/userguide/source_files_and_compilation.html#pyximport ,  https://github.com/cython/cython"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a legitimate bug, the function is treating ext = None as if the object contains valid data but it does not . The code is not checking for a success but is assuming it is successful and tries to access ext.Sources leading to the error message. "
      }
    ]
  },
  {
    "data_row_id": "cmg36i1325gz10715fkchy9tx",
    "global_key": "html_bug_report_e212a540_c1ae96bd",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_e212a540.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Line 1550 calls SparseArray(self.to_dense()).cumsum() which creates new SparseArray with same fill_value=0, triggering infinite recursion since _null_fill_value remains False."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a confirmed bug causing infinite recursion when SparseArray.cumsum() is called on arrays with non-NA fill values like integers. The method incorrectly attempts to handle these cases by converting to dense and creating a new SparseArray, which recreates the same conditions and enters an endless loop. This makes the method unusable for common sparse array types, violating expected behavior where cumulative sums should compute normally regardless of fill value."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the implementation of sparsearray cumsum explicitly recurses for arrays with a non null fill value by converting to dense then constructing a new sparsearray and calling cumsum again which recreates the same non null fill and repeats indefinitely until recursionerror. this violates the documented and expected behavior that cumsum should compute cumulative sums rather than crash. the property based test shows the sparse result should match numpy cumsum on the dense representation and the minimal input data equals zero reproduces the failure. the root cause is the condition on non null fill value combined with constructing a new sparsearray without setting a different fill value which causes the same branch to be taken again. this directly supports yes for real bug with high confidence. maintainers would welcome this because it breaks a core operation for common integer and boolean sparse arrays making the method unusable for typical use and the repro is minimal and precise with a clear fix direction so yes with high confidence. it is not a security issue because it is a logic bug leading to a local crash only and does not enable data exfiltration code execution or boundary bypass although it could be used to cause a denial of service if an application exposes this path to untrusted inputs so no with moderate to high confidence. References: https://pandas.pydata.org/docs/reference/api/pandas.arrays.SparseArray.html "
      }
    ]
  },
  {
    "data_row_id": "cmg36i1385gz20715ul1a3yx7",
    "global_key": "html_bug_report_1f3104f7_912e38ec",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_1f3104f7.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Function assumes mask is always 2D array, but NumPy optimizes unmasked masks to scalar False (0-dimensional). Attempting axis=1 operations on scalar fails."
      },
      {
        "rater_id": "cmfmmbs2v04is072efydz5lc4",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because csgraph_from_masked crahes when instead of getting the mask as array, it receives as scalar False. It's not designed to hanlde this, hence, throws the AxisError."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This seems to be a bug. The function is performing array operations on a scalar value. The csgraph_from_masked function should take an array as the input, without specifications for the mask format, so true or false, nor is it validating this. "
      }
    ]
  },
  {
    "data_row_id": "cmg36i13d5gz307159jnz71sd",
    "global_key": "html_bug_report_610b77ed_9ac99549",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_610b77ed.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The cumsum() method has infinite recursion when _null_fill_value is False. Line 1550 calls SparseArray(self.to_dense()).cumsum() which creates a new SparseArray with same fill_value, causing the same condition to trigger again infinitely."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "cumsum() is supposed to return the running totals of a sparse array, but instead it crashes with infinite recursion whenever the fill value isn't NaN."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Calling SparseArray.cumsum() triggers unbounded self-recursion when the array\u2019s fill_value is not null (e.g., the default 0 for integer sparse arrays). The implementation path checks if not self._null_fill_value: and then constructs SparseArray(self.to_dense()).cumsum() which recreates a SparseArray with the same non-null fill value and re-enters the same branch, causing infinite recursion. I reproduced the crash locally on multiple inputs (including [1], [1,0,0,2], [0,0,0], and [-3,0,5]), all raising RecursionError. The report\u2019s stack traces and logic analysis match the observed behavior."
      }
    ]
  },
  {
    "data_row_id": "cmg36i13i5gz40715w0kgqr6n",
    "global_key": "html_bug_report_b274caa5_1aafc727",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_b274caa5.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes this is a bug, the issue seems to be that the panda's interchange protocol's implementation is converting null values incorrectly as valid.  \n\nThe interchange protocol should create lossless data between data frames, but the value at index2 should remain as null after the conversion process, however it's being converted to valid data. Moreover this is silent data corruption which is a big bug as well.  "
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is not a bug because this is an unintended function."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The interchange conversion path for categoricals applies modulo arithmetic to category codes before nulls are restored, so the sentinel code -1 (meaning \u201cmissing\u201d) becomes a valid index (e.g., -1 % 3 == 2). As a result, NaN entries silently turn into the last category (or the only category when len(categories)==1), corrupting data. I reproduced both cases locally: a mixed series with one NaN becomes fully non-null, and the single-category [None] case becomes ['a']. The report\u2019s explanation and proposed patch (avoid modulo; preserve sentinel through to set_nulls) exactly match the observed behavior."
      }
    ]
  },
  {
    "data_row_id": "cmg36i13m5gz507155jicag2c",
    "global_key": "html_bug_report_e988b7f3_028d156d",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_e988b7f3.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Modulo arithmetic (codes % len(categories)) converts null sentinel (-1) to valid category indices. With 1 category: -1 % 1 = 0, so null becomes categories[0]. Violates documented behavior that -1 = NaN in pandas categoricals."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because pandas uses -1 as the special code for \"missing\" in categoricals, but the interchange code wrongly, turns -1 into a real index"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The conversion path maps categorical codes to labels using modulo arithmetic, so the sentinel -1 (meaning \u201cmissing\u201d) becomes a valid index (-1 % n == n-1). That silently converts NaN into the last category (or the only category when n==1), violating pandas\u2019 documented categorical semantics where -1 denotes missing. The report\u2019s hypothesis matches the observed behavior and cites the exact line causing it. I reproduced the issue locally and observed NaN -> 'c' for a 3\u2011category example and NaN -> 'a' for the single\u2011category edge case."
      }
    ]
  },
  {
    "data_row_id": "cmg36i13r5gz60715bcsn471y",
    "global_key": "html_bug_report_dc9ab7d4_536a911f",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_dc9ab7d4.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Violates standard mathematical definition of Jacobian matrix. For f(x) = Ax, Jacobian should be A, but function returns A.T. Implementation incorrectly assembles derivative results in transposed order."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The report\u2019s minimal example shows SciPy returning \n\ud835\udc34\u22a4 instead of  \ud835\udc34, and the reasoning matches a classic assembly error: computing per\u2011coordinate derivatives correctly but stacking them along the wrong axis. I validated the transposition mechanism with a stand\u2011in finite\u2011difference reproducer, which demonstrates that the described assembly mistake indeed yields \n\ud835\udc34\u22a4 for linear maps."
      },
      {
        "rater_id": "cmcz90upa0snr070m5wxb2cqk",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The function is bugged because it returns the transpose of the correct Jacobian matrix. This is not a formatting issue since the transpose of a matrix is not mathematically equivalent. Many mathematical operations (matrix multiplication) require the matrix to be the same dimension. "
      }
    ]
  },
  {
    "data_row_id": "cmg36i13w5gz70715df710v6z",
    "global_key": "html_bug_report_7baf411c_65384788",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_7baf411c.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Modulo operation (codes % len(categories)) converts null sentinel (-1) to valid category index. With 1 category: -1 % 1 = 0, so NaN becomes categories[0] ('a'). Violates data preservation contract."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Round-tripping a DataFrame with a categorical column that contains nulls through the interchange protocol turns NaN into a valid category value. The root cause is a modulo operation applied to the categorical codes (-1 sentinel for missing) before nulls are restored, e.g., with one category -1 % 1 == 0, so the missing value becomes the first category. I reproduced this locally: ['a', None] becomes ['a', 'a'] after from_dataframe(df.__dataframe__()), with the null count dropping from 1 -> 0. The report pinpoints the offending line and shows identical behavior and rationale."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the dataframe interchange round trip is expected to preserve data semantics yet the report shows nulls in a categorical column are transformed into valid category values. the root cause is the modulo indexing of categorical codes where sentinel codes used to represent nulls such as minus one are mapped into valid indices so minus one modulo one becomes zero which selects the first category. once values are materialized with wrong categories the later null handling cannot reconstruct the original missing values because the sentinel information has been lost. this violates the protocol guarantee of lossless interchange and differs from other dtypes in pandas where nulls are preserved through the same path. maintainers would likely welcome the report because it is a clear correctness issue in a public api with a minimal repro and a precise code location and a feasible fix proposed. it is not a security issue because there is no code execution or boundary bypass involved though it does risk silent data corruption and bad analytics which are integrity concerns not security exploits. References: https://pandas.pydata.org/docs/reference/api/pandas.api.interchange.from_dataframe.html , https://data-apis.org/dataframe-protocol/latest/API.html"
      }
    ]
  },
  {
    "data_row_id": "cmg36i1415gz80715mlqrk82k",
    "global_key": "html_bug_report_6b04ea73_d6e88916",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_6b04ea73.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Function expects array returns but crashes when given scalars. At line 421, fj[work.abinf] tries to index 0-dimensional scalar with boolean mask, causing IndexError. Other scipy integrators handle this gracefully."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The report shows that passing a constant/scalar-returning function (e.g., lambda x: 0.0) to integrate.tanhsinh triggers an IndexError deep inside post_func_eval, because the implementation indexes fj[...] assuming an array output, while fj is 0-D when the integrand returns a Python/NumPy scalar. The traceback on page 2 clearly indicates fj[work.abinf] on a 0-D array (\u201carray is 0-dimensional, but 1 were indexed\u201d). I reproduced the mechanism locally: indexing a 0-D scalar with a boolean mask raises the same IndexError."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is not a bug, as scipy.integrate.tanhsinh expects the input function to have the signature f(xi: ndarray, *args) -> ndarray.\n\nIn this case, the provided function instead has the signature f(xi: ndarray) -> int, which is incorrect and therefore causes the error.\n\nHence, this behavior is expected from SciPy, since the error arises due to a mismatch in the input function\u2019s signature."
      }
    ]
  },
  {
    "data_row_id": "cmg36i1465gz9071542ov9pwi",
    "global_key": "html_bug_report_c4767b60_bbfd1d3f",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_c4767b60.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Line 1550 calls SparseArray(self.to_dense()).cumsum() which creates new SparseArray with same fill_value, causing infinite recursion when _null_fill_value is False. Should call .cumsum() on dense result before wrapping."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because sparsearray cumsum enters infinite recursion whenever the fill value is non null such as zero for integers which is the default for integer sparse arrays and this leads to a recursionerror crash. the report shows the failing minimal cases like 0 0 and a property based test comparing to numpy cumsum that consistently fails which demonstrates a deterministic divergence from expected behavior. the documented behavior says cumsum should compute the cumulative sum of non null values and return a sparsearray result, but the implementation path described in the report creates a new sparsearray from the dense data and then calls cumsum again which reenters the same non null fill path because the fill value remains zero causing an infinite loop. this violates the api contract and breaks a user visible operation, so yes it is a bug with high confidence. because it crashes a documented api on default integer inputs and a one line change to compute cumsum on the dense array avoids recursion, maintainers would welcome it as clear and actionable. security wise this is a pure crash with no data exposure or boundary bypass, at most a denial of service if reachable in a service path, so it is not a security issue. References: https://pandas.pydata.org/pandas-docs/version/0.24.0rc1/api/generated/pandas.SparseArray.cumsum.html"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Calling SparseArray.cumsum() on integer sparse arrays crashes with a RecursionError. The implementation path for non-null fill_value (e.g., the default 0 for ints) does return SparseArray(self.to_dense()).cumsum(), which recreates a SparseArray with the same non-null fill and re-enters the same branch, looping forever. I reproduced this locally with several inputs ([0,0], [1,0,2], [0,0,0], [-3,0,5]), all raising RecursionError. The report\u2019s trace and explanation on pp. 3\u20137 match exactly."
      }
    ]
  },
  {
    "data_row_id": "cmg36i14b5gza0715dzavbg4i",
    "global_key": "html_bug_report_a724d30e_c01c5ecb",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_a724d30e.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "This violates the fundamental contract of concatenation: data preservation. The bug occurs because:\n\npython\n# Current problematic code:\nfill_value = to_concat[0].fill_value  # Only uses first array's fill_value\nWhen arrays have different fill values:\n\narr2 with fill_value=2 doesn't store the 2s in sp_values (they're implicit)\nConcatenation uses arr1.fill_value=0 for the result\nThe missing 2s get filled with 0 instead of their correct value 2\nResult: Silent data corruption\n"
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the provided property test and concrete repro both show silent data corruption when concatenating sparsearrays that have different fill values. the method uses only the first arrays fill value and concatenates sparse payloads that omit positions equal to each arrays own fill value, so values equal to a later arrays fill value are dropped and replaced by the first arrays fill value in the result. this violates the reasonable and widely expected invariant that concatenation must preserve values such that the dense result matches numpy concatenate of the dense inputs. even if the method is internal, silently producing wrong data rather than raising on mismatched sparse dtypes including fill value is incorrect. maintainers would likely accept fixing this either by enforcing a strict dtype and fill value precondition with a clear error or by densifying at a higher layer when fill values differ. it is not a security issue because there is no code execution or boundary bypass involved, only data integrity risk confined to user level data.References: https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html , https://github.com/pandas-dev/pandas/blob/main/pandas/core/arrays/sparse/array.py , https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Concatenating SparseArrays that have different fill_values drops information from later arrays: any element equal to a later array\u2019s own fill_value disappears from the sparse payload and is replaced by the first array\u2019s fill_value in the result. This violates the intuitive (and de-facto) contract that concat(to_dense()) should match to_dense(concat). The report demonstrates this with a failing Hypothesis case and a clear, minimal repro; my local run reproduces the same corruption. The root cause is the method using only to_concat[0].fill_value to build the result while blindly stitching sp_values/sp_index from all inputs."
      }
    ]
  },
  {
    "data_row_id": "cmg36i14h5gzb0715orfhq1pi",
    "global_key": "html_bug_report_b2b96f13_63149cbe",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_b2b96f13.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "1_very_uncertain_mostly_guessing"
        },
        "comments": "Violates documented API contract and produces inconsistent/crashing behavior. The function explicitly documents Series/Index as supported inputs since v2.1.0, but:\n\nSeries crashes due to incompatible .take() method signature\nIndex silently ignores allow_fill=True when fill_value=None\nInteger Index can't handle fill values due to NA constraints\nSame operation behaves differently across input types, breaking API consistency"
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the report shows clear violations of the documented contract and inconsistent behavior across supported input types. the docs for pandas api extensions take state that numpy array extension array index and series are supported and when allow fill is true negative one is a sentinel for missing values to be filled with the default na for the dtype or a provided fill value. the report demonstrates that numpy arrays behave correctly but series raises a typeerror since series take does not accept allow fill or fill value which means take nd wrongly delegates to a method with an incompatible signature. for index inputs when fill value is none the method silently ignores allow fill and treats negative one as a regular negative index returning the last element instead of an nan which is silent data corruption. integer index also rejects actual filling with a value by raising a value error even though the top level api advertises filling. these outcomes contradict the documented behavior and differ across array types which is a correctness bug. the report pinpoints the root cause in pandas core array algos take nd where non numpy arrays are sent to arr take with allow fill and fill value regardless of whether the target method supports those parameters and where index take disables fill semantics when fill value is none. the proposed fix to first extract the underlying array from series index or dataframe via values aligns behavior with the ndarray or extension array paths that correctly implement allow fill and nan handling and would restore consistency with the docs. References: https://pandas.pydata.org/docs/reference/api/pandas.api.extensions.ExtensionArray.take.html , https://pandas.pydata.org/docs/reference/api/pandas.Index.take.html , https://pandas.pydata.org/docs/reference/api/pandas.Series.take.html"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The report shows three failures: (a) Series + allow_fill=True raises TypeError, (b) Index + allow_fill=True, fill_value=None silently returns the last element for -1 instead of filling, and (c) integer Index rejects custom fill values. I reproduced all of them: NumPy works ([10., nan]), Series crashes (TypeError: unexpected keyword 'fill_value'), Int64Index returns [10, 30] (treats -1 as \u201clast\u201d), Int64Index + fill_value=-999 errors, and Float64Index only fills correctly when fill_value=np.nan. This violates the documented contract that Series and Index are supported inputs and that allow_fill=True should treat -1 as the fill sentinel."
      }
    ]
  },
  {
    "data_row_id": "cmg36i14m5gzc07152qrrazdi",
    "global_key": "html_bug_report_facc5f87_0b48d7cc",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_facc5f87.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "Infinite recursion prevents basic functionality. The method creates a new SparseArray from dense data and calls cumsum() on it, but the new array has the same non-null fill value, causing infinite recursion. This breaks the fundamental expectation that cumsum() should compute cumulative sums for any valid SparseArray."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because when the sparsearray has a non null fill value such as zero the cumsum implementation takes the branch for non null fill value and constructs a new sparsearray from the dense values and then calls cumsum again. the new sparsearray still has a non null fill value so the same branch is taken repeatedly which causes infinite recursion and a crash. this violates the documented behavior that cumsum should compute the cumulative sum of non na values and preserve na locations. the property test failing on data equal to zero and the minimal example with integer data demonstrate the issue occurs in the default integer case where fill value equals zero, so this is not an exotic edge case but a common one. maintainers would welcome this because it is a clear correctness and stability problem in a public api with a precise root cause and a straightforward change to avoid recursion by computing the dense cumulative sum once and wrapping it while preserving the original fill value. it is not a security bug because there is no code execution or data exposure or boundary bypass involved, only a crash that could at worst enable denial of service if untrusted inputs reach this path.references:\nhttps://pandas.pydata.org/pandas-docs/version/0.24.0rc1/api/generated/pandas.SparseArray.cumsum.html"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Calling SparseArray.cumsum() on integer sparse arrays (default fill_value=0) crashes with RecursionError. The implementation takes the non-null fill path and executes return SparseArray(self.to_dense()).cumsum(), which recreates another SparseArray with the same fill_value=0 and immediately re-enters the same branch (an infinite loop). I reproduced this locally: both [0] and [1,0,2,0,3] raise RecursionError: maximum recursion depth exceeded. The report shows the same stack"
      }
    ]
  },
  {
    "data_row_id": "cmg36i14r5gzd0715hthasgtt",
    "global_key": "html_bug_report_c024260e_59ee5ab5",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_c024260e.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "Silent input mutation violates fundamental programming principles. The function directly modifies the input array with values[mask] = fill_value, breaking the expectation that functions either:\n\nReturn new results without mutating inputs (like NumPy's cumsum)\nExplicitly document and require inplace=True for mutation (like pandas)\nThis causes permanent, silent data corruption in the original array."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the accumulation helper explicitly modifies the input array by replacing masked entries with a fill value before computing the cumulative operation which causes silent data corruption. the bug report shows a minimal failing example and a property based test where the array changes after calling cumsum even though the function returns a new array which violates normal expectations from numpy style apis where cumsum returns a new array and does not mutate inputs and from pandas conventions where mutation requires an explicit inplace parameter. the report also explains that higher level pandas operations like series cumsum on nullable dtypes route through this helper so the underlying data buffer of a series can be altered invisibly which is unexpected and dangerous for correctness. therefore q1 is yes with high confidence. maintainers would welcome this because it is a clear correctness issue with a small reproducible example affects widely used apis and has an obvious low risk fix by copying the input before modification hence q2 is yes. it is not a security issue since there is no code execution no privilege escalation and no boundary bypass it is a data integrity bug that can lead to wrong results but not an exploit hence q3 is no. References: https://pandas.pydata.org/docs/reference/api/pandas.Series.cumsum.html "
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The report shows that the masked accumulation helpers (e.g., cumsum) modify the input values in-place by writing values[mask] = fill_value before computing the cumulative op, so the original array comes back altered (e.g., [10, 20, 30] \u2192 [10, 0, 30]). That violates NumPy and pandas expectations (no mutation unless explicitly requested) and leads to silent data corruption when these helpers are used under Series.cumsum() for nullable dtypes. In my environment I reproduced the exact mechanism: doing an in-place values[mask] = fill_value before np.cumsum(values) mutates the caller\u2019s buffer while the returned cumulative result is computed from the mutated data. This matches the failing Hypothesis example and the repro/output tables in the report."
      }
    ]
  },
  {
    "data_row_id": "cmg36i14v5gze0715dlwcif4c",
    "global_key": "html_bug_report_7784b58b_938cba72",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_7784b58b.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "1_very_uncertain_mostly_guessing"
        },
        "comments": "Violates fundamental spline interpolation mathematics. Cubic splines must pass through their control points (keyframes), but RotationSpline returns the wrong rotation at the last keyframe. The interval search logic incorrectly maps evaluation at the last keyframe time to the second-to-last segment due to improper handling of searchsorted with side='right'."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because an interpolating rotation spline is expected to pass exactly through its keyframes and the report shows a deterministic violation at the last keyframe time with a minimal reproducible example and a property based test. the documented behavior says the method is analogous to cubic spline interpolation which implies knot interpolation. the root cause is a boundary indexing error in the call path that uses numpy searchsorted with side right then clamps the index to n segments minus one which maps the last time to the penultimate base rotation. since the interpolator returns a zero rotation vector at keyframe times the composition with the wrong base rotation yields exactly the penultimate orientation not a numerical tolerance miss which confirms logic error not floating point drift. maintainers would welcome this because it is user visible correctness breakage that impacts common workflows like animation and robotics and has a small clear fix in the index selection logic. it is not a security issue because it does not enable code execution data exposure privilege escalation or denial of service and is purely a correctness boundary condition. References: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.transform.RotationSpline.html , https://github.com/scipy/scipy/blob/main/scipy/spatial/transform/_rotation_spline.py , https://numpy.org/doc/stable/reference/generated/numpy.searchsorted.html"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "For f:R\u2192SO(3) defined by RotationSpline(times, rotations), evaluating at any keyframe time \ud835\udc61\ud835\udc56 should return exactly rotations[i] (up to quaternion sign). The report shows counterexamples where evaluating at the final keyframe t N\u22121 returns the second-to-last keyframe t N\u22122 when keyframes are tightly clustered near the end. The cited code path uses np.searchsorted(..., side=\"right\") followed by - 1 and clamping; at t N\u22121 this produces index N-2, selecting the previous segment and hence rotations[N-2]. I reproduced this indexing outcome locally with the report\u2019s times; the computed index is 3 for five keyframes (segments 0..3), confirming the mechanism that yields the wrong rotation."
      }
    ]
  },
  {
    "data_row_id": "cmg36i1505gzf0715rr2ls3jn",
    "global_key": "html_bug_report_67006f54_d28077c0",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_67006f54.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "Silent data corruption violates round-trip integrity. The table schema uses generic \"integer\" type for all integers, losing signed/unsigned distinction. When reading back, pandas defaults to int64, causing uint64 values > 2^63-1 to overflow to negative values. This breaks the documented round-trip guarantee for table orient."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the table orient path collapses all integer dtypes into a generic integer type and the read side defaults that to int64, so values at or above 2^63 silently wrap to negative int64 on reconstruction, corrupting data. this contradicts pandas round trip expectations for table orient, as shown by the property based test and the minimal repro where 2^63 comes back negative. the bug is isolated to the table schema path since other orients split records index columns round trip the same uint64 values correctly, demonstrating pandas can preserve these values when it does not force an int64 cast. two failure modes were identified by the test values below int64 minimum raising a too small error and 2^63 silently overflowing which is worse because it is undetected. maintainers would welcome this because it is a user visible correctness failure with a clear reproduction and a straightforward remediation path either preserve unsigned via pandas specific dtype metadata or avoid silent overflow by erroring or falling back to a safe type when out of range. it is not a classical security issue because it does not enable code execution access control bypass or data exfiltration, though it is an integrity issue that can mislead downstream logic if untrusted data are consumed. References: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html , https://pandas.pydata.org/docs/reference/api/pandas.read_json.html , https://pandas.pydata.org/docs/user_guide/io.html#json , https://specs.frictionlessdata.io/table-schema/\n"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "A uint64 value at or above 2^63 round-tripped via to_json(..., orient=\"table\") \u2192 read_json(..., orient=\"table\") becomes a negative int64. I reproduced this locally on pandas 1.5.3: 2^63 (9223372036854775808) comes back as -9223372036854775808, and 2^63+123 similarly overflows to a negative int64. Other orients (split, records, index, columns) preserved both value and uint64 dtype on the same data, so the regression is specific to orient=\"table\". The report documents identical behavior and pinpoints the relevant code paths."
      }
    ]
  },
  {
    "data_row_id": "cmg36i1545gzg0715nffe3sk9",
    "global_key": "html_bug_report_c55a5116_268dbd62",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_c55a5116.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because scipy.integrate.tanhsinh crashes with an IndexError when integrating constant functions, which are fundamental mathematical operations that should work reliably. The bug occurs because the function attempts boolean indexing on 0-dimensional arrays returned by constant functions like lambda x: c, which don't preserve the input shape. This violates the principle of least surprise since integrating constants is a basic calculus operation, and the documentation doesn't clearly warn users that functions must return arrays with the same shape as inputs. The crash prevents users from performing even the simplest integration tasks.\nf(xi: ndarray, *argsi) -> ndarray\n\n(see: SciPy Documentation: https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.tanhsinh.html).\n\nThe function provided in the reported case does not return an ndarray and therefore does not comply with the expected function signature. Consequently, this test case is invalid, and the behavior observed cannot be considered a bug."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The documentation clearly states that the tanhsinh method expects the integrand function to return an ndarray. However, the provided function returns a scalar value, which does not align with the method\u2019s requirements. Therefore, this should not be considered a valid bug."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because tanhsinh internally passes array inputs to the user integrand and then immediately applies boolean indexing to the returned values via fj masking with work flags. when the integrand is a constant written in the natural python style lambda x colon c it returns a scalar which becomes a zero dimensional array. boolean indexing on a zero dimensional array raises an indexerror, so the routine crashes on a fundamental case instead of either handling the scalar or clearly rejecting it. the docs state the integrand must be elementwise and show an ndarray signature, but they do not clearly warn that scalar returns will crash, and users reasonably expect constant integrands to work. the proposed minimal change to coerce fj to at least one dimensional before indexing would prevent the crash without altering the intended algorithmic behavior. References: https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.tanhsinh.html , https://github.com/scipy/scipy/blob/v1.16.1/scipy/integrate/_tanhsinh.py , https://numpy.org/doc/stable/user/basics.indexing.html#boolean-or-mask-index-arrays"
      }
    ]
  },
  {
    "data_row_id": "cmg36i1595gzh0715uamvj1wx",
    "global_key": "html_bug_report_89a55682_eb846ff6",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_89a55682.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "1_very_uncertain_mostly_guessing"
        },
        "comments": "Silent data loss violates uniqueness contract. The unique() method should return all distinct intervals, but when all break values are negative, it incorrectly drops valid intervals. This suggests a flaw in the comparison, hashing, or factorization logic specifically for intervals with negative bounds."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug,  the unique() function is incorrectly removing valid intervals. When two intervals are created [-3, -2] & [-2, -1], then concatenated , unique() should return two unique intervals. However only [-3, -2] returned, [-2, -1] seems to be discarded. \n\nThis discard is happening silently as well, which would definitely lead to data corruption. \n"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "This is a critical bug because IntervalArray.unique() violates its fundamental contract to return all distinct values when all interval breaks are negative. The method silently drops valid intervals, causing severe data loss without warnings. This corruption affects analytical results in domains like finance or temperature analysis, where negative values are common, breaking the expected uniqueness invariant and producing incorrect results that undermine data integrity."
      }
    ]
  },
  {
    "data_row_id": "cmg36i15e5gzi0715mdk7lvev",
    "global_key": "html_bug_report_3e59eec7_c8dd361d",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_3e59eec7.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because it violates the fundamental mathematical invariant that all vertices of a Delaunay triangulation must be locatable within the triangulation. The find_simplex() method returns -1 (indicating \"outside all simplices\") for points that are actually vertices of the triangulation, which is mathematically incorrect. The bug occurs due to insufficient default tolerance (100*eps \u2248 2.22e-14) to handle floating-point precision errors when checking if vertices lie on simplex boundaries. The evidence shows that a point which is definitively a vertex (belonging to simplices 0, 1, 17, and 18) cannot be found with the default tolerance, but can be found with slightly higher tolerance (\u2265 1e-13), demonstrating this is a numerical precision issue rather than a mathematical impossibility."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "I'm not very confident this one is a bug, but based on the definition, every vertex of a triangle is part of a simplex. But -1 seems to be outside the vertex, so mathematically this is incorrect. "
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a bug because find_simplex fails to locate vertices of the triangulation with the default tolerance, incorrectly returning -1. Vertices definitionally belong to simplices, so this violates mathematical invariants and user expectations. Numerical precision causes the issue, as increasing the tolerance resolves it, indicating the default is too strict for boundary cases."
      }
    ]
  },
  {
    "data_row_id": "cmg36i15j5gzj0715sm4vc3y3",
    "global_key": "html_bug_report_c93afbbd_fafe27c3",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_c93afbbd.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "Violates interchange protocol specification and causes silent data corruption. The modulo operation wraps -1 sentinel values into valid category indices instead of preserving them as nulls. This converts null values to actual category data, permanently losing null information and violating the protocol's requirement that categorical columns use -1 as the null sentinel."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the categorical interchange path maps the sentinel minus one used for nulls into a valid category via a modulo operation which violates both pandas categorical semantics and the dataframe interchange spec that define nulls via a sentinel and expect them to be preserved end to end this causes silent data corruption since null positions change on round trip and the original null information is unrecoverable after indexing into categories the provided property based test and minimal repro demonstrate the mismatch in isna masks therefore yes to real bug due to documented behavior being broken yes to maintainers welcome because it affects a public api with clear user impact and a straightforward fix to handle sentinel values before indexing and add tests and no to security because it does not enable code execution privilege escalation or boundary bypass though it is a data integrity problem. References: https://pandas.pydata.org/docs/user_guide/categorical.htmlhttps://pandas.pydata.org/docs/user_guide/categorical.html , https://data-apis.org/dataframe-protocol/latest/API.html , https://github.com/pandas-dev/pandas/blob/main/pandas/core/interchange/from_dataframe.py"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The categorical interchange path converts the integer codes to category values using a modulo operation, so the null sentinel -1 becomes a valid index (-1 % n == n-1). This turns NaN into the last category (or the only category when n==1), silently losing null information. I reproduced the exact failures locally on pandas 1.5.3: the one-row falsifier codes=[-1] round-trips as ['cat_1'] (non-null), and a mixed example [-1,0,1,-1,2] returns all non-nulls with nulls replaced by 'C'. The report shows the same failing Hypothesis case, reproducer, stack context, and the specific modulo line in from_dataframe.py."
      }
    ]
  },
  {
    "data_row_id": "cmg36i15o5gzk0715qjn2zptu",
    "global_key": "html_bug_report_9d28f24c_a04cb30b",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_9d28f24c.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The LinearOperator interface contract requires matvec to handle both 1D (N,) and 2D (N,1) input vectors correctly. The current implementation uses broadcasting operations (v * d[:, np.newaxis]) that always produce 2D outputs, causing a reshape failure when the LinearOperator wrapper tries to convert the result back to 1D for 1D inputs."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The LinearOperator is crashing when it's computing matrix vector multiplication, with 1D vectors. It's accepting 1D vectors, for example, with the shape  of f(2,). However an error is being produced, \nas\nError occurred: ValueError: cannot reshape array of size 4 into shape (2,)\n\nThe LinearOperator is somehow incorrectly producing a size of 4 when it should be 2. So this is failing basic linear algebra, and matrix/vector multiplication. "
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This seems to be a bug because the LinearOperator violates SciPy's interface contract by failing to handle standard 1D vector inputs correctly. The matvec method must accept both (N,) and (N,1) shaped vectors, but the current implementation only works with 2D inputs due to incorrect dimension handling in the lambda functions, causing crashes with 1D vectors that should be supported."
      }
    ]
  },
  {
    "data_row_id": "cmg36i15t5gzl07158lmnmc7q",
    "global_key": "html_bug_report_d571aec0_4706e30c",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_d571aec0.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Silent integer overflow violates data integrity expectations. When users specify dtype='int32', they expect either successful conversion or an error - not values silently wrapping around. The inconsistency between small overflows (silent wrap) and large overflows (error) violates the principle of least surprise."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes, it looks like pandas is silently wrapping integers, which is causing an overflow. So essentially positive int32 values are being converted to negative int32 values. \n\nThis is happening when reading CSV data. Silent failures are some of the worst bugs because the user is not notified. This bug is also leading to data corruption "
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "I reproduced it, and this is absolutely a critical data integrity bug that silently corrupts data without any warning. The reproduction clearly shows the devastating problem- when you specify dtype={'value': 'int32'} and provide a value of 2,147,483,648 (just one above int32 max), pandas silently wraps it around to -2,147,483,648 instead of raising an error. This violates fundamental data integrity principles in multiple ways. First, it breaks the user's explicit type specification contract- when someone specifies a dtype, they expect either successful conversion or a clear error, not silent data corruption. The reproduction showed that a positive value of 2,147,483,648 becomes negative -2,147,483,648, representing a massive data corruption of over 4 billion units of error!"
      }
    ]
  },
  {
    "data_row_id": "cmg36i15y5gzm0715ypq2d55i",
    "global_key": "html_bug_report_f408312c_bcd331c5",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_f408312c.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The C++ dia_matmat function cannot handle empty diagonal arrays (zero matrices), causing a low-level STL vector crash. This violates mathematical expectations (zero \u00d7 zero = zero) and creates inconsistency across scipy sparse formats - all other formats (CSR, CSC, COO, etc.) handle zero matrix multiplication correctly."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This should be considered a valid bug, as the dia_array constructor accepts a 2-D NumPy array as input. In this case, both A and B are created with compatible dimensions that should allow matrix multiplication. However, instead of producing the expected result, the operation raises an exception. This indicates that there is an issue in the computation logic for dia_array objects when initialized directly from 2-D NumPy arrays."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The report shows that multiplying two DIA-format sparse arrays that contain no stored diagonals (i.e., zero matrices) raises RuntimeError: vector::_M_default_append inside _sparsetools::dia_matmat. The screenshot/trace and the minimal repro demonstrate the crash even for the 1\u00d71 all-zeros case (A = sp.dia_array(np.zeros((1,1))); A @ A). This violates mathematical correctness (zero\u00d7anything = zero) and diverges from other SciPy formats (CSR/CSC/COO/LIL/DOK/BSR) and NumPy dense, which return a zero matrix instead of crashing. The root cause is that the DIA matmul path is called with empty offsets/data, which the C++ routine does not handle."
      }
    ]
  },
  {
    "data_row_id": "cmg36i1645gzn071532mq0r1k",
    "global_key": "html_bug_report_a8de3fe4_26071321",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_a8de3fe4.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a real bug because it violates the documented API contract that promises valid iteration counts in the zeros_full_output struct. The iterations field contains uninitialized memory (garbage values like -193122112) instead of proper iteration counts when root-finding algorithms terminate early at boundary values. This creates undefined behavior, makes the API unreliable for users who need iteration counts for analysis or debugging, and exhibits non-deterministic behavior across multiple runs with identical inputs."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The test code shows that SciPy's function zeros.full_output_example is returning garbage data from memory that has not been initialized. This is a big problem because this memory should not even be accessed. \n\nThis can be seen in the output iterations=1241536024, which is 1.2 billion iterations. This seems to be random garbage data. \nNo error is being raised, so this is silent data corruption. "
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because SciPy promises that the iterations field in the solver's output will always be a valid count, but when the root is right at the boundary the code skips setting it, so you get random garbage memory instead of 0. It even has a comment that tells us that  /* BUG: iterations field not set here */ "
      }
    ]
  },
  {
    "data_row_id": "cmg36i1695gzo07158yngylh1",
    "global_key": "html_bug_report_adb931bd_a2d73d9a",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_adb931bd.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The middleware uses naive string splitting host.split(\":\")[0] which fails for IPv6 addresses because they contain multiple colons. When IPv6 addresses are properly formatted in brackets with ports [2001:db8::1]:8080, the split extracts [2001 instead of the actual address, violating RFC standards."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a legitimate bug because the middleware fails to parse RFC-compliant IPv6 Host headers with ports, incorrectly rejecting valid requests. The naive string split on colons breaks for IPv6 addresses enclosed in brackets (e.g., \"[::1]:8000\"), violating standards and hindering IPv6 support in networks where it is essential."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The middleware extracts the host via headers.get(\"host\", \"\").split(\":\")[0]. For IPv6 literals with ports\u2014e.g., [::1]:8000, [2001:db8::1]:8080nthis naive split returns '[' or '[2001' instead of the full IPv6 address, causing a false \u201cInvalid host header\u201d (HTTP 400). I validated the failure mechanism locally: \"[::1]:8000\".split(':')[0] -> '[' and \"[2001:db8::1]:8080\".split(':')[0] -> '[2001', while IPv4 and DNS hosts parse fine. The report\u2019s property-based test and minimal reproducer show the same failures and explicitly tie them to the split(':') logic."
      }
    ]
  },
  {
    "data_row_id": "cmg36i16e5gzp07153pzfrf8r",
    "global_key": "html_bug_report_4b145202_e2418313",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_4b145202.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "xarray incorrectly converts axis=None to tuple(range(array.ndim)) causing sequential application along each axis instead of flattening first. For axis=None, numpy requires flattening the array then applying cumulative operations, but xarray applies them axis-by-axis producing different mathematical results."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes this appears to be a bug. Xarray, cumprod and cumsum functions are passed with axis=None, are not flattening the array shape properly.  \n\nXarray, since the flattening process is not occurring, the array is seen as 2,2, but numpy correctly flattens to a 1 dimensional array with 4 elements. \n\nFor example, numpy is taking [[1, 2], [3, 4]] and flattening to 1D array 4 elements. \n\nXarray takes [[1, 2], [3, 4]], and keeps it as (2,2) array. "
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This should be considered a bug because, by default, duck arrays are expected to behave like NumPy arrays. The core philosophy of Xarray is to provide efficient operations on multi-dimensional arrays. Therefore, for cross-compatibility, methods in Xarray applied to NumPy arrays should yield results consistent with NumPy itself. In this case, that contract is being violated, which makes the issue a valid bug."
      }
    ]
  },
  {
    "data_row_id": "cmg36i16j5gzq0715avjwjx0o",
    "global_key": "html_bug_report_1264f480_e0242ed8",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_1264f480.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "CFMaskCoder violates the fundamental round-trip property decode(encode(variable)) == variable documented in its base class. The decoder blindly converts ALL values equal to the fill value to NaN, regardless of whether they were originally missing or valid data, causing silent data corruption when valid data values happen to equal the fill value."
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is absolutely a real bug. this is mathematically broken: cosine similarity is only defined for vectors of the same dimensionality- you literally cannot calculate the angle between vectors that exist in different dimensional spaces. It's like trying to find the angle between a 2D point and a 3D point, which just doesn't make mathematical sense."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes, this appears to be a bug. CFMaskCoder is incorrectly converting zeros to NaN, whenever the fill value is set to zero. The issue seems to stem from how the function is encoding then decoding. \n\nWhen CFMaskCoder sees 0.0 and fill value of 0.0, the function is treating this as if all zeros are missing then returning NaN values. "
      }
    ]
  },
  {
    "data_row_id": "cmg36i16o5gzr0715bql5z4lp",
    "global_key": "html_bug_report_e0a36b7f_2ac276c4",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_e0a36b7f.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "RangeIndex.arange violates its documented behavior by not preserving the step parameter. Instead of using the provided step value directly, it recalculates step as (stop - start) / size, resulting in different spacing between values than requested. This contradicts the documentation promise of \"spacing between values given by step\" and creates incompatibility with numpy.arange behavior."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "I think this is a bug, the RangeIndex.arrange function is not adhering to the step parameter when a range index is created. From the output the expected step is 1.0, but the value being returned is 0.75. \n\nNumpy is producing a range of [0.1], while RangeIndex is producing incorrect values, [0. 0.75]. This is also a silent bug so data is being corrupted. "
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The method explicitly documents that step defines the spacing, but instead it quietly recalculates a different step based on (stop - start) / size."
      }
    ]
  },
  {
    "data_row_id": "cmg36i16s5gzs0715uanz0jx2",
    "global_key": "html_bug_report_8f5995fd_5554bc09",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_8f5995fd.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "build_grid_chunks violates the fundamental chunking invariant that chunks must partition (not exceed) the data size. When chunk_size > size, the function returns chunks that sum to more than the specified size, breaking the core contract of chunking operations and potentially causing array out-of-bounds errors or data corruption"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug. The build_grid_chunks function is returning chunks that are greater then size. chunk_size > size, is exceeding the size. This is violating basic math. \n\nFor example, correct behavior should be build_grid_chunks(size=1, chunk_size=2) = 1\n\nBut test case is output build_grid_chunks(size=1, chunk_size=2) = 3"
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The chunking functions are expected to partition the data exactly, so if build_grid_chunks(1, 2) returns (2, 1) (summing to 3 instead of 1), that breaks the invariant that \"chunks cover exactly the data size.\""
      }
    ]
  },
  {
    "data_row_id": "cmg36i16y5gzt07159chnc449",
    "global_key": "html_bug_report_a3939dd2_8d94e840",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_a3939dd2.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "CombinedLock.locked() incorrectly accesses lock.locked as a property instead of calling lock.locked() as a method. Since method objects are always truthy in Python, the function always returns True when any locks are present, regardless of whether they are actually acquired. This violates the documented behavior and breaks thread synchronization state checking."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes this is a bug. Threading.lock is locking threads while they are active. The lock should return True if the lock is being held by a thread, but False otherwise. This is to prevent multiple threads from accessing resources at the same time. \n\nFrom the bug output it is clear that the CombinedLock.locked() is always returning True.  Due to this all threads will think there is a lock, even if a thread is not. \n\n\nTest 1: No locks acquired\ncombined.locked() returns: True\nExpected: False\n\nTest 2: lock1 acquired\ncombined.locked() returns: True\nExpected: True\n\nTest 3: Demonstrating the bug\nlock1.locked (without parentheses): \nlock1.locked() (with parentheses): False\n\nTest 4: What the buggy code evaluates\nany([lock1.locked, lock2.locked]): True\nany([lock1.locked(), lock2.locked()]): False"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It\u2019s a bug because CombinedLock.locked() checks lock.locked as an attribute instead of calling lock.locked() as a method. In Python, threading.Lock.locked is a bound method; a bound method object is always truthy, so any(lock.locked for lock in self.locks) returns True whenever there are any locks present, regardless of whether any are actually acquired. The correct behavior is to evaluate each lock\u2019s state via lock.locked(). As written, CombinedLock.locked() misreports the lock state (e.g., True even when all locks are free), contradicting its own contract and breaking diagnostics/logic that depend on accurate lock-state queries."
      }
    ]
  }
]