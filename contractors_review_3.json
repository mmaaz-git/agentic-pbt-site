[
  {
    "data_row_id": "cmg1gipxikmvo0772meamnyem",
    "global_key": "html_bug_report_d42f7a27_13aae5c4",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_d42f7a27.html",
    "votes": [
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The report\u2019s core claim \u201cnewline in the returned header value \u21d2 header injection\u201d doesn\u2019t hold for email headers. Python\u2019s email.header.Header.encode() legitimately inserts folding line breaks (newline plus leading whitespace) to comply with RFC 5322/2047, and such folded newlines do not create new headers or enable injection; they are part of the same header value. Django\u2019s forbid_multi_line_headers encodes non-ASCII values via Header(...).encode() and may therefore return folded values; that\u2019s expected behavior for long/complex header text, not a vulnerability. Django\u2019s own code and history show this is normal, and earlier discussions explicitly note that newlines followed by whitespace are fine (folding), with prior fixes targeting accidental newlines introduced by Python\u2019s encoder in HTTP headers, not email injection per se.\n\nSeparately, forbid_multi_line_headers() is deprecated and slated for removal in the Django 6.x timeframe as Django moves to the modern email API, further reducing the likelihood this would be treated as a defect to fix now."
      },
      {
        "rater_id": "cmc0hw57z03j907z313sb4ps6",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is not a bug as this is the standard behavior as defined by the  MIME standard to encode to encode non-ASCII characters and also Email headers have a strict line-length limit, typically 75 characters per line if exceeds that it needs to insert new line character which is what this is doing "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is not a bug. the report treats any newline in an encoded header value as injection, but rfc compliant folding newlines produced by email header encoding are expected and safe. header injection requires raw crlf that starts a new header or ends the header block, not crlf followed by whitespace used for continuation lines. python email header encoding emits continuation lines, not new headers, so it does not violate the security goal. the stated failing input is very short and should not even fold, which suggests the property test is misinterpreting output or conflating display formatting with the actual header value. the correct property is that the output must not contain crlf that is not followed by space or tab and must not introduce crlf within a header value. the function\u2019s job is to reject user supplied newlines in inputs and to encode non ascii safely, not to eliminate standards compliant folding performed by the email library. the proposed fix to disable folding and strip newlines degrades interoperability and does not prevent downstream folding by mail libraries, while risking overlong header lines. the claim of scheduled removal is not substantiated in the report and is not relevant to the security semantics here. Here are some references: https://docs.python.org/3/library/email.header.html ,https://www.rfc-editor.org/rfc/rfc5322 ,https://www.rfc-editor.org/rfc/rfc2047"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The report claims that forbid_multi_line_headers can return values containing newlines when the encoding step (Header(...).encode()) is applied to non-ASCII inputs. My local tests on Python\u2019s email.header.Header confirm that RFC-2047 folding introduces \\n for inputs like val='0\\x0c\\x80' under utf-8 and iso-8859-1, reproducing the exact shape shown in the report (e.g., =?utf-8?q?0?=\\n =?utf-8?b?IMKA?=). Since the function\u2019s stated purpose is to forbid multi-line headers, emitting a newline violates its security contract and enables header injection. The report\u2019s property-based test and manual repro (p\u00e1gs. 2\u20133, 7\u20139) show the same behavior."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The function seems to violate its purpose. It is supposed to prevent header injection, but the test shows it's returning new lines. This input val: '0\\x0c\\x80' non ASCII,  when it's encoded/decoded the new line it outputs should have been removed. \n\nThis becomes a security issue and can lead to header injection, possibly allow someone to run arbitrary malicious code. "
      }
    ]
  },
  {
    "data_row_id": "cmg1gipy1kmvp0772lv7ms6hl",
    "global_key": "html_bug_report_02952f58_dc77e978",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_02952f58.html",
    "votes": [
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "attr.evolve() creates a new instance by calling the class\u2019s __init__ with keyword args, not by cloning internal state. That means converters are executed by the generated __init__ (converters run during initialization), so when evolve() reuses already-converted attribute values and calls __init__ again, the converter runs a second time and changes the value. The result is that evolve(obj) (with no changes) may not equal obj, violating the practical expectation of \u201ca copy of inst with changes incorporated.\u201d While the docs don\u2019t spell out converter semantics for evolve, they do say converters are applied by attrs-generated initializers, and evolve works by instantiating a new object via that initializer. So the behavior is explainable but still surprising and correctness-breaking for non-idempotent converters. This makes the report a real, user-visible correctness issue."
      },
      {
        "rater_id": "cmc0hw57z03j907z313sb4ps6",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is not a bug but can be considered undesirable behavior sometimes as the evolve method simply calls \"__init__\" constructor which will anyway call the  converter but this can not be considered a bug."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because evolve reconstructs the instance through init which runs converters again and it feeds already converted values back into the converter so unchanged fields are transformed a second time which silently corrupts data for non idempotent converters. the property test and minimal repro both show evolve without changes does not preserve values which violates the least surprise expectation that evolving with no changes should produce an equivalent object. the docs for evolve say it creates a new instance with specified changes but do not warn that converters re run on unchanged fields so users would not expect this behavior. the implementation in make shows evolve collects current attribute values then calls cls with those values so init and converters are applied again causing the double conversion. maintainers would likely welcome a report because this is user visible surprising and can break real workloads that use converters for normalization ids timestamps or counters and a documentation update or an opt in api to skip reconversion for unchanged fields is actionable. it is not a security issue on its own because it does not enable privilege escalation data exfiltration or boundary bypass although it can cause correctness problems and potentially availability issues if converters are expensive or stateful. Here is some references: https://www.attrs.org/en/stable/api.html#attr.evolve ,https://www.attrs.org/en/stable/init.html#converters ,https://www.attrs.org/en/stable/api.html#attr.assoc"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "My local runs (attrs 25.3.0) reproduce the behavior: evolve() passes already-converted values back through __init__, which re-applies converters. For a non-idempotent converter like lambda v: v*2, original.x=10 becomes evolved.x=20 with no changes specified. The same occurs with other non-idempotent converters (e.g., appending a suffix or incrementing a counter). This violates user expectations that evolve(obj) (with no overrides) yields an equivalent copy. The report\u2019s Hypothesis test and line-level analysis match the observed behavior. "
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a bug the attr.evolve() function is inaccurately parsing data passed to it. For example the DoubleConvert(x=5) when created the the lambda v: v *2 takes precedence.\n\nObj.X = 10, without any changes, attr.evolve(obj) if called with no changes, should keep x=10, but it's not, it's doubling , 10 * 2 =20 . This breaks idempotence. "
      }
    ]
  },
  {
    "data_row_id": "cmg1gipy8kmvq0772t93gri21",
    "global_key": "html_bug_report_02176655_c20a8cb3",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_02176655.html",
    "votes": [
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "- Django\u2019s own docs say ALLOWED_HOSTS entries \u201cwill be matched against the request\u2019s Host header exactly (case-insensitive, not including port).\u201d The current behavior therefore violates documented expectations.  \n- The implementation of django.utils.http.is_same_domain(host, pattern) lowercases only the pattern and compares it to the raw host, producing asymmetric, case-sensitive results (is_same_domain('a','A')==True, is_same_domain('A','a')==False). This is visible in Django\u2019s published module source.  \n- DNS comparisons must be case-insensitive per the standards (RFC 1035/4343). Returning False for differently cased but otherwise identical domains contradicts that requirement."
      },
      {
        "rater_id": "cmc0hw57z03j907z313sb4ps6",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a bug, as it clearly gives no matching for simple matches as well like \"'sub.EXAMPLE.com', '.example.com'\", due to case sensitive check  and can have security implications as well as used in security critical places like CSRF protection"
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a bug because dns names are case insensitive and the function only lowercases the pattern and not the host which makes equality and suffix checks effectively case sensitive. the property based test and the minimal repro both show asymmetric results and failures even for identical host and pattern in different cases which violates expected and standards compliant behavior. per the report this function is used in csrf cors and redirect host checks so the incorrect case handling can cause false negatives and surprising breakage. this supports yes for real bug due to rfc violation and inconsistent logic yes for maintainer interest because the fix is a simple and safe normalization change and no for security because the failure mode is rejecting legitimate matches not allowing forbidden ones so it is not an exploit path. Here is  some references: https://www.rfc-editor.org/rfc/rfc1035#section-3.1 ,https://docs.djangoproject.com/en/stable/ref/utils/#django.utils.http.url_has_allowed_host_and_scheme\nhttps://docs.djangoproject.com/en/stable/ref/settings/#allowed-hosts\nhttps://docs.djangoproject.com/en/stable/ref/csrf/"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The property-based test in the report finds a counterexample (host='A'). My local shim (faithfully modeling the function that lowercases only pattern) reproduces the same failures: exact-domain matches with different casing return False, and the function is asymmetric (is_same_domain('a','A') is True while is_same_domain('A','a') is False). This contradicts DNS rules (domain comparisons are case-insensitive) and the code\u2019s clear intent (it already lowercases pattern). Adding host = host.lower() fixes all shown cases, including subdomain patterns that start with a dot."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes this is a bug. Domain names are case insensitive. TEST123.COM, and test123.com or TEST123.com , are all the same. The \"is_same_domain\" function is lowercasing the pattern but not the host part. \n\nThere could also be a security issue here since the domain check is failing a malicious author could potentially utilize a fully uppercase site, such as TEST123.COM. "
      }
    ]
  },
  {
    "data_row_id": "cmg1gipyfkmvr0772o80v1q8v",
    "global_key": "html_bug_report_e1d3bcc5_b57837f4",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_e1d3bcc5.html",
    "votes": [
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "read_setup_file() mis-parses -DNAME=VALUE by slicing with value[equals + 2:], which drops the first character of every macro value (e.g., -DBAR=value \u2192 ('BAR','alue')). This off-by-one is visible in distutils\u2019 historical sources and common mirrors of the code. That behavior contradicts the documented contract for define_macros, which requires preserving the full value in the (name, value) tuple."
      },
      {
        "rater_id": "cmc0hw57z03j907z313sb4ps6",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is clearly a bug as it is not able to read the macro value given correctly and truncating the first character consistently."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because read_setup_file is required to parse -d name equals value flags without altering the value, as documented for define_macros in the distutils and setuptools extension api. the current implementation in setuptools vendored distutils slices the macro value starting two characters after the equals sign, which skips the first character of the value. for example dash d bar equals value becomes macro bar with value alue and dash d zero equals zero becomes macro zero with empty string. this contradicts standard c compiler semantics for dash d name equals value and the code comment that shows dash d foo equals blah as the intended case. the fix is to slice starting one character after the equals sign so the full value is preserved. i chose yes for real bug and yes for welcome because it violates documented behavior and is a one character fix with clear impact on builds and runtime behavior. i chose no for security because there is no direct exploit path in setuptools itself, though downstream code could be affected if macros control security sensitive behaviors. Here are some references: https://github.com/pypa/setuptools/blob/main/setuptools/_distutils/extension.py ,https://github.com/cython/cython/blob/master/Cython/Distutils/extension.py"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "Local runs reproduce the exact truncation described: -DBAR=value yields ('BAR','alue'), -DZERO=0 yields ('ZERO',''), which is an off-by-one error when slicing after '='. Inspecting the code in this environment shows ext.define_macros.append((value[0:equals], value[equals + 2 :])), confirming the wrong start index (+2 instead of +1). The report\u2019s property-based test and minimal repro match these outcomes."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This issue constitutes a valid bug, as it results in incorrect parsing of macro values passed to the program. For example, consider the case of a numeric parameter defined as -DPARAM=1000. Due to the truncation, the value is incorrectly interpreted as 000 instead of 1000. Such a discrepancy can lead to severe consequences: an operation intended to process 1000 entities would instead process none, fundamentally altering the expected behavior of the application. This highlights the critical nature of the bug, as it directly impacts both correctness and reliability."
      }
    ]
  },
  {
    "data_row_id": "cmg1gipylkmvs07720y7ylmq9",
    "global_key": "html_bug_report_32fb6287_0ad0ba39",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_32fb6287.html",
    "votes": [
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "django.conf.urls.static.static() builds a regex with re_path(r\"^%s(?P<path>.*)$\" % re.escape(prefix.lstrip(\"/\"))). If prefix is only slashes (e.g., \"/\"), prefix.lstrip(\"/\") becomes \"\", so the pattern degenerates to ^(?P<path>.*)$, a catch-all that matches every URL. That\u2019s contrary to the helper\u2019s documented intention - serve files under a specific prefix during development - and it produces surprising, app-breaking behavior in DEBUG=True. The helper already rejects an empty prefix, so failing to also reject prefixes that become empty after lstrip(\"/\") is an edge-case oversight. Because Django\u2019s URL dispatcher matches the first pattern that fits, this catch-all will swallow all routes in dev."
      },
      {
        "rater_id": "cmc0hw57z03j907z313sb4ps6",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "Yes this is a bug as it clearly he static file handler will intercept all requests, breaking normal Django URL routing when presented with \"/\" and can have security implications as well as it will match all the urls ."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the helper normalizes the prefix with lstrip of forward slashes and a prefix made only of slashes collapses to empty, which then yields a regex that matches any path and becomes a catch all in debug. that contradicts the documented intent that static serves files under a specific prefix during development and it is inconsistent with the existing guard that already rejects an empty raw prefix but fails to reject a normalized empty prefix. the property based test and the minimal repro in the report demonstrate that using a single slash creates a route that captures unrelated urls, which is incorrect routing behavior. maintainers would welcome fixing this because it is a tiny change to validate the normalized prefix while preserving all valid prefixes, and it eliminates a surprising and misleading edge case in a commonly used helper. it is not a security issue under normal documented usage because the helper is active only when debug is true and static serving is intended solely for development, and the static file view remains constrained to its document root, though it can still cause broad routing misbehavior if misused. Here is some references: https://docs.djangoproject.com/en/stable/howto/static-files/#serving-files-uploaded-by-a-user-during-development ,https://docs.djangoproject.com/en/stable/topics/http/urls/ ,https://github.com/django/django/blob/main/django/conf/urls/static.py"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The function builds its regex from prefix.lstrip(\"/\"). When prefix consists only of slashes (e.g., \"/\", \"//\"), lstrip(\"/\") becomes an empty string, so the resulting pattern is ^(?P<path>.*)$, a catch-all that matches every URL. My local shim reproduces this exactly (see the matrix and the printed patterns), which matches the report\u2019s failing Hypothesis example for prefix='/' and the repro that shows all URLs being captured. This contradicts the intent and documentation of static() (serve files under a specific prefix, in DEBUG only)."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a bug, because providing a prefix consisting only of slashes results in the creation of a catch-all URL pattern. Such a pattern should not be supported by Django, since it causes all routes to be intercepted by the static files handler instead of being routed to the intended application views.\n\nDjango already guards against this scenario by explicitly prohibiting the use of an empty prefix, as it would lead to the same behavior. However, this safeguard can be bypassed by using only slashes, which are transformed into a regex that again behaves as a catch-all. Currently, Django does not detect this case.\n\nTherefore, this represents a legitimate bug that should be addressed to ensure consistent and safe behavior of static()."
      }
    ]
  },
  {
    "data_row_id": "cmg1gipyrkmvt0772qfgc899d",
    "global_key": "html_bug_report_67de3404_10c57231",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_67de3404.html",
    "votes": [
      {
        "rater_id": "cmc0hw57z03j907z313sb4ps6",
        "Q1": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is not a bug as it is the intended behavior of the library as both the numpy replace and native replace functions have different behavior, 'numpy' replace won't increase the size of the original array and the original array size is constant will only keep the same size and if we need dynamic sizing  we can create source array with \"dtype\""
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug. the function promises to return a copy with old replaced by new and points users to python str replace which handles replacements of any length. the property test and minimal repro show divergence where the replacement is silently truncated when it is longer than the array dtype width. the root cause is that old and new are cast to the input array dtype before sizing, so lengths are measured after truncation, counts and buffer sizes are computed from the wrong lengths, the output dtype is made too small, and the final result is truncated with no warning. this also risks changing match semantics if old is truncated. none of this behavior is documented and it causes silent data loss, violating user expectations and the described api.references: https://numpy.org/doc/stable/reference/generated/numpy.strings.replace.html"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The report\u2019s Hypothesis test finds a minimal counterexample where NumPy returns 'XX0' instead of Python\u2019s 'XXXXXX0'. My local shim, which mirrors the alleged root cause (casting new to the array\u2019s dtype before computing buffer sizes), reproduces the same truncation and mismatch. The report pinpoints the culprit lines in numpy/_core/strings.py where scalar arguments are cast too early; this explains both the wrong buffer-length computation and the silently truncated result. The behavior violates the documented \u201clike str.replace element-wise\u201d contract and causes silent data loss."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "I don't think this is a bug since the data being truncated is what is exceeding the input array data type width. From my understanding when you create a numpy array such as np.array(['12']), numpy parses this as 2 character long unicode, but if replace tries to insert more than this value say 4 characters it's truncated to 2 characters, fitting the array. \n\nHowever if it's not a bug it's a really bad issue that should be addressed because it's silently failing, one would believe there data is being corrupted. "
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This behavior is expected and not a bug. Unlike Python\u2019s built-in strings, which are dynamically sized, NumPy string arrays use fixed-length dtypes. As a result, when a replacement string exceeds the allocated length of the array\u2019s elements, the values are truncated to fit the fixed size. This is consistent with NumPy\u2019s design and should be considered normal behavior."
      }
    ]
  },
  {
    "data_row_id": "cmg1gipyxkmvu07728ksx09vo",
    "global_key": "html_bug_report_4f2391a5_0cfc54fe",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_4f2391a5.html",
    "votes": [
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": " this is a real bug because the sqlite backend function sqlite date trunc can receive a date only string and the shared parser returns a python datetime date object for that input while the truncation code assumes a datetime datetime and unconditionally applies timezone handling by calling replace with tzinfo which only exists on datetime datetime not on datetime date so python raises typeerror replace got an unexpected keyword argument tzinfo this contradicts django docs which state trunc works with datefield for year quarter month week and day kinds the provided hypothesis test captures an idempotence property for truncation and the minimal repro shows the exact failure when use tz is enabled and a connection timezone is active the bug affects common settings sqlite as default db and use tz true so normal orm queries like annotate with trunc on a datefield can crash the proposed fix to normalize a date to a midnight datetime before timezone handling is correct and aligns with expected semantics and eliminates the type error. references:\nhttps://docs.djangoproject.com/en/stable/ref/models/database-functions/#trunc\nhttps://github.com/django/django/blob/main/django/db/backends/utils.py\nhttps://docs.python.org/3/library/datetime.html#datetime.date.replace"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The report pinpoints _sqlite_datetime_parse() returning a date for date-only strings and later calling replace(tzinfo=...) under _sqlite_date_trunc(). My local execution reproduces the exact TypeError with dt=\"2023-06-15\" and conn_tzname=\"UTC\". The call path aligns with the cited lines (typecast_timestamp(...) - date - replace(tzinfo=...)), so this is a real crash impacting common ORM operations on DateField under USE_TZ=True with SQLite."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "This is a bug, the function is crashing when given \"2023-06-15\", instead of the date and full time data such as \"2023-06-15 00:00:00\". The function can't handle date only strings, which is definitely a bug"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "- Django documents that truncation functions work with DateField for date parts (year, month, week, day). This failing case is exactly that scenario, so crashing violates documented behavior.  \n\n- Internally, SQLite support parses the input via _sqlite_datetime_parse() and, when the connection has a timezone, sets tzinfo with dt.replace(tzinfo=\u2026). That path is visible in vendor mirrors of Django\u2019s _functions.py. When the input is a date-only string, typecast_timestamp can yield a datetime.date, and calling replace(tzinfo=\u2026) on a date is invalid, only datetime.datetime has a tzinfo parameter. Hence the TypeError you observed. \n \n- Python\u2019s docs confirm date.replace() only accepts year, month, day. No tzinfo.  \n\n- Related Django tickets show this same parsing/normalization layer (_sqlite_datetime_parse) is the place where timezone handling must be adjusted, reinforcing that the diagnosis (convert date to datetime at midnight when tz handling is needed) is the right area to change."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This issue qualifies as a bug because it affects a core database function, which should reliably handle valid date inputs without crashing. ISO-8601 date strings containing only the date component are valid and should be supported across all database configurations. Additionally, the current behavior is inconsistent: the same query executes successfully with timezone-naive conditions but fails under timezone-aware conditions, which is unexpected and undesirable."
      }
    ]
  },
  {
    "data_row_id": "cmg1gipz3kmvv077260hvjmgb",
    "global_key": "html_bug_report_b1b8f2a4_51781852",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_b1b8f2a4.html",
    "votes": [
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the code in the described method uses the full list of positional argument names as a dictionary key when handling keyword arguments. the variable sig args is a list produced by the parsed function signature, and lists are unhashable in python so using them as dict keys raises a typeerror. the intended logic is to assign the provided keyword value to the entry keyed by the single argument name found in the loop over kw items. the implementation already checks name in sig args which confirms the correct key is name not the entire list. the property based test and the minimal example show that passing keyword arguments immediately triggers the typeerror at this site which is consistent with using an unhashable key. the suggested one line change to values name equals value aligns with the signature parsing design which supports defaults star args and star star kwargs, and restores correct mapping of keyword names to values. therefore it violates reasonable expectations and the clear intent of the code and is a straightforward logic error. references:\nhttps://docs.python.org/3/library/stdtypes.html#mapping-types-dict\nhttps://docs.python.org/3/glossary.html#term-hashable\nhttps://github.com/cython/cython/blob/master/Cython/Tempita/_tempita.py\nhttps://cython.readthedocs.io/en/latest/src/userguide/tempita.html#templating-syntax"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "My local run hits the same TypeError: unhashable type: 'list' when calling a template function with keyword arguments. The report points to line 469 in _tempita.py where values[sig_args] = value mistakenly uses the entire list of argument names as the dict key. Replacing it with values[name] = value is consistent with the loop for name, value in kw.items() and fixes the logic of mapping each kwarg to its value. The stack trace and failing example match."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The implementation of TemplateDef._parse_signature() assigns keyword-argument values using the entire list of parameter names as a dict key: values[sig_args] = value. Lists are unhashable, so this raises TypeError: unhashable type: 'list' when calling a template function with kwargs. The intent is clearly to use the current kwarg\u2019s name: values[name] = value. Public mirrors of the file show the offending line exactly as reported. "
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This appears to be a bug, it appears that keyword arguments are not parsing correctly , which are standard in python. Due to this it's leading to a TypeError: \n"
      },
      {
        "rater_id": "cmc0hw57z03j907z313sb4ps6",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a bug as it clearly breaks the code when using \"Template\" due to the inherent logic error inside _parse_signature."
      }
    ]
  },
  {
    "data_row_id": "cmg1gipz8kmvw07726osp7hrd",
    "global_key": "html_bug_report_05dbff20_eaf92c04",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_05dbff20.html",
    "votes": [
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Local execution mirrors the reported control flow reproduce the exact behavior: in-memory readinto() performs a first read into b and, due to the missing return, performs a second read that overwrites the buffer with the next bytes and advances the file position twice. The same function also returns a wrong byte count in edge cases. The PDF includes a failing Hypothesis test and a minimal async demo that match my results (buffer shows b'56789' instead of b'01234', position 10 instead of 5). The source excerpt on page 8 pinpoints the missing return and the incorrect call inside readinto1()."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "SpooledTemporaryFile.readinto() (and readinto1()) should read bytes into a provided buffer once and return the number of bytes read. That\u2019s the documented I/O contract for readinto/readinto1 in Python\u2019s io module. The behavior demonstrated in this bug report i.e. first reading into the buffer from the in-memory file object and then (because there\u2019s no return) reading again via the superclass, explains the \u201cdouble-read\u201d symptoms: the buffer ends up containing later bytes, and the file position advances twice as far. That violates the API semantics (and the single-syscall promise of readinto1). The repro and Hypothesis failures are consistent with this analysis."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is definitely a bug! The readinto() function should read bytes from the file position into the buffer. bytearray(5) is setting the buffer to the first 5 bytes.  \n\nIt's not respecting  f.seek(0) which tells it to start from position 0. It should go from 0-4, but it instead outputs 56789. \n\nThis could also be a security issue, since readinto() could return data it was never intended to return. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a bug because in memory mode the spooledtemporaryfile readinto method reads into the buffer once using the underlying file object and then, due to the missing return, calls the async super readinto which performs a second read into the same buffer. this overwrites the first bytes with the next bytes and advances the file position twice, violating the python io contract that readinto reads into the provided buffer and returns the number of bytes read. the report also shows that readinto1 uses the wrong underlying method in memory mode calling readinto instead of readinto1 which breaks the at most one read semantics required by buffered io. the provided property based test and minimal reproduction demonstrate incorrect buffer contents and doubled file position when the file has not rolled to disk. the disk rolled path and text mode pathways are not affected, but binary mode in memory is the default and thus impacted. Here are some references: https://docs.python.org/3/library/io.html#io.iobase.readinto ,https://github.com/agronholm/anyio/blob/master/src/anyio/_core/_tempfile.py"
      },
      {
        "rater_id": "cmc0hw57z03j907z313sb4ps6",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a bug as it causes the incorrect read of the number of bytes and is specific to mthod \"readinto()\"."
      }
    ]
  },
  {
    "data_row_id": "cmg1gipzgkmvx07724qpgzknj",
    "global_key": "html_bug_report_cf353bf7_623194d2",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_cf353bf7.html",
    "votes": [
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "Local reproduction matches the report: when fill_text == \"\" and len(text) < length, _sqlite_lpad returns text unchanged, violating LPAD\u2019s length contract (e.g., \"hi\" with length=5 yields length 2). The root cause is computing padding as (fill_text * length)[:delta], which becomes an empty string for empty fill_text. This contradicts both the LPAD idea (\u201creturn exactly length\u201d) and Django\u2019s own _sqlite_rpad, which always truncates to exactly length."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "_sqlite_lpad(text, length, fill_text) promises LPAD semantics (return a string of exactly length, padding on the left when len(text) < length). With fill_text == \"\", the current implementation returns the un-padded text which is shorter than length because (\"\" * length)[:delta] is \"\". That violates the fixed-width expectation of LPAD and yields surprising, silent truncation/under-padding. Since padding with an empty string is impossible, the function should either reject this input or otherwise guarantee the length invariant, returning a shorter string is incorrect."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This code shows a bug where the text is an empty string. The string output should 5, but it's returning 2. The correct output should be similar to \"    hi\", or \"hi\". LPAD should return the specified length, but this is not happening correctly. \n\nThis can also cause issues with database migrations between SQLite and other databases postgresql, etc since the string data would not copy correctly "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is not a clear bug based on the report. the reports own code shows that when fill_text is empty there is nothing to pad with, so the function returns the original text which is shorter than length. the report asserts a universal lpad contract that the result must always have exact length, but provides no documentation proving that requirement for the empty fill case and the linked django docs do not define empty fill semantics. the rpad claim in the report is also inconsistent with its own code, which with empty fill also yields the original text and not the target length. given no explicit django guarantee for empty fill and that padding with an empty string is logically impossible, the failing property test is enforcing an assumption that is not documented, so this reads as an edge case semantics choice rather than a defect. this is why i answered no for real bug, no for maintainers welcome as a bug, and no for security since it only affects formatting and not safety boundaries. references:\nhttps://docs.djangoproject.com/en/5.2/ref/models/database-functions/#lpad"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "The left-padding contract requires the resulting string to match the specified length provided to the method. However, when an empty string is used as the fill character, this contract is violated because the output string does not attain the intended length. In Python, the equivalent ljust method raises an error when an empty fill string is supplied, thereby maintaining consistency with its contract. Accordingly, a similar behavior is expected from the lpad function as well, which, unfortunately, is not the case."
      }
    ]
  },
  {
    "data_row_id": "cmg1gipzmkmvy0772ld5gp3g9",
    "global_key": "html_bug_report_a9fbb714_6a7f0be7",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_a9fbb714.html",
    "votes": [
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The report\u2019s Hypothesis test and minimal repro show a deterministic counterexample where df.repartition(npartitions=4) yields divisions=(0,1,3,4) (only 3 effective partitions), yet npartitions still reports 4. This contradicts the Dask invariant npartitions == len(divisions) - 1 and leads to runtime errors when accessing a non-existent partition (e.g., index 3). My shim reproduces the exact shape of the inconsistency, consistent with the report\u2019s line-by-line root cause (interpolation -> int cast -> duplicates removed -> npartitions returns requested value). Live confirmation with Dask would likely reproduce identically."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a bug because npartitions should equal len(divisions) -1, but the correct amount of partitions are saying they're being created, when they are not.\n\nThe output of Division count:4 is(expected 5). Actual partitions: 4 is incorrect. There would need to be 5 division counts. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the dataframe invariant that npartitions must equal length of divisions minus one is broken after repartition when integer boundaries collapse to duplicates and are deduplicated, so the actual partitions are fewer than requested while the npartitions property still reports the requested count. the reproduction with five rows and a request for four partitions is a realistic case where interpolated integer boundaries collide, causing fewer unique divisions and therefore fewer actual partitions. this mismatch is user visible and leads to runtime errors when code relies on npartitions to index partitions that do not exist. the documentation allows that the resulting partition count may be slightly lower, but it does not justify a property reporting a count that disagrees with the actual divisions, so the current behavior violates a core contract of the dataframe model. the proposed fix is to have npartitions derive from computed divisions when available, which restores the invariant without forcing eager computation and aligns the metadata with the actual graph.references\nhttps://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.dataframe.repartition,\nhttps://docs.dask.org/en/latest/dataframe-design.html#divisions,\nhttps://github.com/dask/dask/blob/main/dask/dataframe/dask_expr/_repartition.py"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "The Dask documentation firmly establishes the invariant governing DataFrame divisions: the length of the divisions tuple must be exactly equal to the number of partitions plus one (len(divisions) == npartitions + 1). The provided evidence successfully demonstrates a scenario where this fundamental contract is broken. It is important to note that this case does not align with the documented exception where divisions are entirely indeterminate (resulting in all division points being set to None). Instead, the observed behavior presents a state with defined divisions that nonetheless violate the expected length constraint. This confirms that the issue is not an expected edge case but a valid bug, indicating a flaw in the division calculation or maintenance process."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a real bug because Dask's core invariant is that npartitions == len(divisions) - 1, but in this case npartitions reports the requested number of partitions even when duplicates in computed divisions reduce the actual partition count."
      }
    ]
  },
  {
    "data_row_id": "cmg1gipzrkmvz0772fpsxs7f2",
    "global_key": "html_bug_report_0c8a5ae1_a37fc81d",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_0c8a5ae1.html",
    "votes": [
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "On Python 3, types.InstanceType does not exist. The RE.wrong_type() branch that tries to format a \u201cmodule.Class instance\u201d string uses types.InstanceType, causing an AttributeError and masking the intended PlexTypeError. My local repro raises AttributeError exactly as the report states, so the type-validation path is broken on any Python 3 runtime."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes this is a bug. The code is using an exception type that is available in python2, not python3. This is causing AttributeErorr, instead of the intended error of PlexTypeError. \n\nThis is in general an incompatibility issue, and the code would need to be updated. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the wrong_type function in cython plex regexps uses types instancetype which was removed in python 3 and therefore it raises attributeerror instead of the intended plextypeerror. this breaks the module error handling contract since invalid re arguments should trigger plextypeerror with a clear message, not a crash. the bug report shows a minimal repro seq str a then a plain string which consistently triggers attributeerror and the property based test further confirms the failure across inputs. python documentation confirms old style classes and instancetype are gone in python 3 which explains the crash. maintainers would welcome this because it is a simple and precise python 3 compatibility fix that restores correct exceptions and improves developer experience. it is not a security issue because it does not enable code execution or data exposure, though it can cause a crash if callers only catch plextypeerror which is a robustness concern rather than a direct vulnerability. references:\nhttps://docs.python.org/3/library/types.html\nhttps://github.com/cython/cython/blob/3.0.x/Cython/Plex/Regexps.py\nhttps://github.com/cython/cython/blob/3.0.x/Cython/Plex/Errors.py"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "RE.wrong_type tries to branch on types.InstanceType, which only existed in Python 2. On Python 3 the attribute is gone, so the error path itself crashes with AttributeError instead of raising the intended PlexTypeError. That breaks the module\u2019s documented/type-checking contract and prevents helpful diagnostics for all regex constructor validations (e.g., Seq(Str('a'), \"not an RE\")). The behavior is deterministically wrong on any supported Python 3 version, independent of inputs."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Yes, this is a legitimate bug that needs to be fixed. It represents technical debt from the Python 2 to 3 transition that was never properly addressed. The fix should be straightforward - remove the Python 2-specific `types.InstanceType` check and use standard Python 3 type inspection.\n\nThe bug undermines the reliability of error reporting in Cython's Plex module and should be prioritized for fixing."
      }
    ]
  },
  {
    "data_row_id": "cmg1gipzxkmw00772whubs90f",
    "global_key": "html_bug_report_bbad93c5_99ce5ab1",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_bbad93c5.html",
    "votes": [
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It seems when repartitioning is done it's breaking division. Creating a time stamp, say using 2000-01-01 00:00:00, is causing an AssertionError. This AssertionError is probably indicating some internal bug. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the resample helper returns newdivs and outdivs with different lengths when the last outdiv equals the last original division. the adjust ends logic extends newdivs but never extends outdivs in the equality case which violates the implicit contract that both division tuples stay aligned. that mismatch propagates into the resample lowering where outdivs are paired with repartitioned keys and then into repartition where invalid division boundaries trigger an assertion that the input partition count must be greater than the output count. the parameters that trigger this are valid documented resample options closed right and label right so the failure occurs for a legitimate use case and not misuse. maintainers would welcome this because it is a clear correctness issue that causes a user visible crash during common time series resampling and the fix is minimal by handling the equality branch to keep lengths aligned. this is not a security issue because it does not enable code execution data leakage access control bypass or persistent corruption it is a logic error that causes a fail fast assertion rather than silent data compromise. Here is some references: https://pandas.pydata.org/docs/user_guide/timeseries.html#resampling ,https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.resample.html , https://github.com/dask/dask/blob/2024.8.2/dask/dataframe/tseries/resample.py"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "_resample_bin_and_out_divs() is expected to return two division sequences (newdivs, outdivs) that stay aligned. In the \u201cfinal boundary adjustment\u201d block, newdivs is always updated with setter(newdivs, divisions[-1] + res). For label='right', setter is list.append, so newdivs grows by one. outdivs then adjusts only on > or < comparisons with divisions[-1]; the == case is skipped, leaving outdivs one element shorter than newdivs. This Hypothesis test finds exactly this: lengths 2 vs 1 for inputs like divisions=[2000-01-01, 2000-01-02], rule='W', closed='right', label='right'. This mismatch later triggers the repartition assertion during graph construction, causing resample to crash. The proposed equality-branch fix (setter(outdivs, outdivs[-1])) appends the same final boundary when label='right' (and becomes a no-op replacement when label='left'), restoring length parity."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a bug because the function fails to handle the case where outdivs[-1] == divisions[-1], creating a length mismatch between two division tuples that must always be equal. This violation of a fundamental invariant causes downstream operations to fail with AssertionErrors. The logic error occurs because the code only adjusts outdivs for inequality conditions (> and <) but leaves it unchanged when values are equal, while simultaneously modifying newdivs. This inconsistency prevents legitimate resample operations with valid parameters like closed='right', label='right' from completing, breaking functionality that should work correctly."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because Dask's _resample_bin_and_out_divs sometimes returns two division lists of different lengths, which breaks a core assumption in resampling and causes an assertion error when we try to compute"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq03kmw10772admv9kqd",
    "global_key": "html_bug_report_691cf73e_4bfa9c0e",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_691cf73e.html",
    "votes": [
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Numpy allocates memory for 1  unicode character '<U1', which means there's only enough space in the  array for 1 character. However it appears that some unicode characters expand this to 2 characters when the case is converted to upper. This exceeds the memory in the array, and numpy silently truncates this returning improper data. \n\nSo no I do not believe this is a bug, this seems like numpy handles this process in a weird way, not letting users know data will be truncated, and instead silently, and automatically does this. But since the unicode characters transition from 1 character to 2, I think this is a poor design choice of numpy, but not a bug. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because numpy char upper lower title and capitalize are documented as element wise wrappers over python str methods yet they return truncated results when unicode case mappings expand length due to fixed width unicode dtypes in numpy which silently drop extra code points this contradicts the reasonable expectation that element wise application matches python behavior it also breaks a common algebraic property where lower of upper of x equals lower of x as shown by the failing input eszett and other examples like turkish i with dot and ligatures the default dtype inference like u1 for a single character makes silent corruption the default outcome and there is no warning or error further evidence of problematic inconsistency is that numpy char add widens the dtype to fit concatenation while case transforms do not so the observed behavior is not an intentional universal rule but a gap in these specific functions. Here is some references: https://numpy.org/doc/stable/reference/generated/numpy.char.upper.html ,https://numpy.org/doc/stable/user/basics.types.html#string-dtype ,https://docs.python.org/3/library/stdtypes.html#str.upper ,https://github.com/numpy/numpy/issues/12256"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "NumPy\u2019s char case functions operate element-wise but keep the original fixed-width string dtype (e.g., <U1, <U2). When Unicode case mappings expand (e.g., \u00df\u2192\"SS\", \ufb01\u2192\"FI\", \u0130\u2192\"i\u0307\"), the result is silently truncated to the input dtype\u2019s width, breaking basic expectations like lower(upper(x)) == lower(x) and contradicting the intuitive reading of \u201ccalls str.upper()/str.lower() element-wise.\u201d The property test and reproductions show deterministic data loss and wrong round-trip results with default dtype inference (which often yields <U1> for single-char inputs). Even if one argues this stems from fixed-size string dtypes, the silent truncation is a clear correctness issue."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is absolutely a valid and serious bug. The core issue is that NumPy's fixed-width string arrays fundamentally cannot handle Unicode's variable-length case mappings, yet the implementation silently truncates data rather than warning users or handling the expansion properly. It is because the lower and upper ufuncs have not been implemented yet for the unicode and bytes dtypes."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's kinda both. Technically it's not a bug because NumPy's old fixed-width string dtypes were never designed to handle Unicode case mappings that expand length, so truncation is expected; but from a user perspective it feels like a bug because the docs say it just calls Python's str.upper()/lower(), yet the results silently lose characters, so really it's more of a legacy design limitation that looks buggy in practice."
      }
    ]
  },
  {
    "data_row_id": "cmg1giq09kmw20772frbit3g2",
    "global_key": "html_bug_report_ad8b43e4_823d96da",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_ad8b43e4.html",
    "votes": [
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the function treats backslashes inconsistently across its two branches and relies on os path basename without normalizing separators when allow relative path is false. on unix like systems backslash is not a separator so names like uploads\\..\\passwords.txt and 0\\file pass, while forward slashes are correctly rejected. this creates platform dependent behavior where inputs that pass validation on linux can be interpreted as containing path elements on windows, enabling traversal. the property based test failing input with a backslash and the repro both demonstrate this inconsistency. given django\u2019s goal of cross platform consistency and the function\u2019s intent to reject path elements when relative paths are not allowed, the current behavior is a logic flaw. this also explains the answers above: yes it is a bug due to inconsistent and surprising behavior, yes maintainers will likely welcome it because it violates expected validation guarantees, and yes there is a security angle because these filenames can become directory traversal when handled on windows or windows like tooling even if they looked inert on unix. references: https://docs.djangoproject.com/en/stable/ref/files/uploads/ ,https://docs.python.org/3/library/os.path.html#os.path.basename\n"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug, and a security bug as well. This is bypassing any sanity checks. It should not be bypassing the validator, which is only checking for forward slashes not properly verifying back slashes.  \n\nThis is a security issue because \\.. can be used to access directories and traverse them. "
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a valid and significant security vulnerability. The function incorrectly allows backslash path separators when allow_relative_path=False on Unix systems, creating a cross-platform security risk. While backslashes aren't path separators on Unix, they become dangerous when files are transferred to Windows systems. The inconsistency between the two code paths (with and without allow_relative_path) demonstrates a clear logic flaw. The vulnerability enables directory traversal attacks that bypass validation on Unix but execute on Windows, violating Django's cross-platform security guarantees. The proposed fix correctly normalizes path separators consistently across both code paths."
      },
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "The report's reproduction is valid and supported by documentation for referenced python library function https://docs.python.org/3/library/os.path.html"
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "It's a bug because validate_file_name is supposed to stop path-like names, but on Linux it treats backslashes as harmless characters, so something like \"..\\\\secret.txt\" slips through even though it would be a path traversal on Windows. This could also have possibilities on security issues."
      }
    ]
  },
  {
    "data_row_id": "cmg1giq0ekmw30772cwpez9qv",
    "global_key": "html_bug_report_3bf067f4_8bfbf25f",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_3bf067f4.html",
    "votes": [
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the function contract says it should cover exactly the characters in s yet the current loop uses code2 greater than or equal to next character which expands ranges when duplicates exist. for input 00 the sorted list has two zeros and the loop increments code2 from ord zero plus one to include one which was not in the input. the property based test rightly expects set s to equal the covered set and it fails on inputs like 00 proving the violation. this incorrect expansion directly affects public apis any and anybut because any built from chars to ranges then matches extra characters and anybut wrongly excludes them causing unrecognized input. the proposed change to use code2 equal to the next character restores the intended consecutive range extension and correctly handles duplicates. therefore yes it is a real bug yes maintainers would welcome it since it is a clear logic error with minimal safe fix and tests and no it is not inherently a security issue in cython itself though downstream lexers that rely on these apis for input validation could observe correctness impacts. references: https://cython.readthedocs.io"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This appears to be a bug because chars_to_range function should return characters in ranges, but it's incorrectly parsing duplicates. This is an off by one bug. ASCII 0 is supposed to cover 48, 49, but it's returning 50, which is off by one \n\nThis could also be a security issue if used for input validation, incorrect parsing would be applied. "
      },
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The report exposes an evident logic error in a pure function"
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because chars_to_ranges is supposed to cover exactly the characters in the input, but when duplicates show up (like \"00\"), the code wrongly extends the range to include extra characters that were never there (so \"0\" also covers \"1\")."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a valid bug because the function violates its documented contract. The current implementation incorrectly expands character ranges when duplicates are present, causing Any('00') to match unwanted characters like '1'. The logic error occurs when processing duplicates, where the condition code2 >= ord(char_list[i]) incorrectly extends ranges beyond what's needed. The proposed fix changes this to strict equality, ensuring only consecutive characters are merged while properly handling duplicates."
      }
    ]
  },
  {
    "data_row_id": "cmg1giq0lkmw40772tmatb290",
    "global_key": "html_bug_report_896a74bc_544d3585",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_896a74bc.html",
    "votes": [
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the function states it calls a function on every element within a nested container, yet when n is less than or equal to zero and the input is a list with more than one element it only applies the function to the first element and discards the rest, causing silent data loss. the property based test and the failing input in the report show that different multi element inputs can yield the same single element output, which confirms data corruption. the behavior is inconsistent because for non list inputs at non positive depth the entire value is processed, but for list inputs only the first element is processed, and an empty list will raise an index error due to accessing element zero. there is no validation or documentation describing non positive depths, so this surprising behavior contradicts the docstring and violates least surprise, indicating a clear logic bug. based on these facts the correct decisions are yes for real bug with high confidence, yes for maintainers welcoming due to correctness and documentation mismatch, and no for security because it does not cross trust boundaries or enable denial of service, code execution, or information disclosure. references:\nhttps://github.com/dask/dask/blob/main/dask/utils.py"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug, it demonstrates that ndeepmap when n <=0, and the input is a list with multiple elements, only the first element is kept, the others are silently truncated. \n\nFor example, a list of [1, 2, 3, 4, 5], should return [, 2, 3, 4, 5, 6]. But the bug is returning only 2 and not the other elements incremented. "
      },
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The functionality described as a bug is an explicit case included by the developers"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The semantics of ndeepmap imply that depth=0 is only meaningful for non-list inputs; when the input is a list, the minimum depth required to traverse its elements is 1. Therefore, calls with n <= 0 and a list argument are invalid. The current behavior\u2014applying func to only the first element of the list\u2014silently discards data and is undocumented. Instead, the function should raise a clear exception (e.g., ValueError) indicating that depth must be \u22651 for list inputs to prevent unexpected behavior and data loss."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because ndeepmap claims to \"apply a function to every element in a nested container,\" but when you pass n <= 0 and give it a list with more than one element, it just applies the function to the first element only and silently throws the rest away."
      }
    ]
  },
  {
    "data_row_id": "cmg1giq0qkmw50772df22m2zo",
    "global_key": "html_bug_report_918188f5_52b5879f",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_918188f5.html",
    "votes": [
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the public api and docstrings state that sender may be none and has_listeners defaults sender to none yet with use_caching set to true the implementation stores per sender results in a weakref weakkeydictionary and calls cache get and set with the sender as the key weakkeydictionary only accepts keys that are weak referenceable and will raise typeerror for none and for plain object instances this makes the default parameter unusable and violates the documented contract it is also inconsistent because the exact same calls work when caching is disabled which is not documented furthermore django model signals enable caching by default so user code that reasonably calls has_listeners with no sender or send with sender none will crash the property based test and the minimal repro in the report match the implementation and the observed errors so the behavior is a defect not intended design regarding security this is primarily an availability crash and not an exploit path it could be triggered to cause a denial of service only if application code exposes these calls in attacker reachable flows which is uncommon so it is not a security issue. references\nhttps://docs.djangoproject.com/en/5.2/topics/signals/ , https://github.com/django/django/blob/stable/5.2.x/django/dispatch/dispatcher.py , https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a bug, caching is a performance optimization technique , it should never change functionality. This test shows that Djangos signal implementation use_caching=True, fails when sender=None, but works fine with use_caching=False  "
      },
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Reasoning is sound and reproduction cases is valid"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The bug is valid as Django's Signal class indeed crashes with TypeError when use_caching=True and sender is None or non-weakref-able, violating the documented API contract. This affects Django's built-in model signals which use caching, causing production issues when senders aren't weak-referenceable. The inconsistency between cached and non-cached behavior confirms this is a genuine defect requiring the proposed fix."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because Django's docs say None is a valid sender, but with use_caching=True the code tries to store None in a WeakKeyDictionary, which can\u2019t handle it, so it crashes"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq0wkmw60772dkqd1grx",
    "global_key": "html_bug_report_67b798b4_04edbede",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_67b798b4.html",
    "votes": [
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Undocumented failure in specific specific functional usecase"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a valid bug. The AssertionError occurs when resampling to coarser frequencies that produce fewer output bins than input partitions, violating pandas compatibility. This fails for common use cases like aggregating hourly data to daily totals. The assertion incorrectly assumes repartitioning only increases partitions, but coarser resampling naturally reduces them. The crash affects all resample methods and provides opaque errors to users for legitimate operations."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug, it seems Dask's resample function is making an assumption that the output resample bins is => the input partitions. \n\nIt should raise a ValueError vs AssertionError. Dask should combine partitions when multiple fall into the same resample bin or raise the appropriate error which is a ValueErrror"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq11kmw70772qbhrzb9l",
    "global_key": "html_bug_report_e5d26be5_3864dfb7",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_e5d26be5.html",
    "votes": [
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "While result is not a bug, it is unintutitive behavior and under discussion\n\nhttps://github.com/dask/dask/issues/10631\n\n"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a bug because it silently changes integer data to string type, violating the round-trip property and breaking mathematical operations. While the value's text representation is preserved, the semantic meaning and functionality are corrupted, which constitutes a data integrity failure."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Yes, it appears the function is a bug. It's silently changing data from integer to string. The integer is to big for a 64 bit int, , but pandas stores it incorrectly , converting to a string. This can lead to a loss of data, or even data corruption. More over the data fora a string vs integer can be treated differently between databases, and programs.\n\nThis violates roundtrip contract as well, pandas to pandas shouldn't change the data type unless specified.  "
      }
    ]
  },
  {
    "data_row_id": "cmg1giq17kmw807725z5qkwos",
    "global_key": "html_bug_report_f6656e83_b15a7fed",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_f6656e83.html",
    "votes": [
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Truncation is a valid operation in the context. Ex see discussion at: https://stackoverflow.com/questions/3121217/cosine-similarity-of-vectors-of-different-lengths"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a valid bug report. The function violates mathematical principles by silently truncating mismatched vectors, producing incorrect results without warnings. This constitutes silent data corruption that could impact production systems using embedding similarity calculations. The report clearly demonstrates the issue with specific examples and provides a well-reasoned fix addressing both length validation and zero-vector handling."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes this one is a bug. It's using zip(a, b), which is abruptly stopping at the shorter vector.  An error should be raised but instead it truncates silently and returns the wrong data. "
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Cosine similarity is only defined for vectors in the same dimension. The implementation uses zip(a, b) and never checks lengths, so it silently ignores trailing elements of the longer vector, yielding mathematically invalid results (e.g., [1,0] vs [1] \u2192 1.0). That\u2019s silent data corruption. It also divides by zero on zero vectors instead of handling them explicitly."
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "It's a bug. Cosine similarity is only mathematically defined for vectors of the same dimension, and silently truncating breaks that contract. I found the actual implementation at https://github.com/simonw/llm/blob/921fae9a0ad3d664a872e35e4639b16089b61c1d/llm/__init__.py#L461-L465, and it uses `zip()` which stops at the shorter vector. So when you pass `[0.0, 1.0]` and `[1.0]`, it ignores the second element and computes similarity on just `[0.0]` vs `[1.0]`, returning 0.0 when the calculation is actually undefined. I reproduced it locally too - `[1, 0]` and `[1]` returns 1.0 (perfect similarity!), but that's complete nonsense since you're comparing a 2D vector to a 1D vector. The bug also crashes on zero vectors with ZeroDivisionError instead of handling them gracefully. This violates basic vector math where cosine similarity requires equal dimensionality - you literally can't measure the angle between vectors in different mathematical spaces. References: the flawed implementation confirmed at https://github.com/simonw/llm/blob/main/llm/__init__.py"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq1ckmw90772idq4mro2",
    "global_key": "html_bug_report_8ff4627e_de7181b9",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_8ff4627e.html",
    "votes": [
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The report identifies a lack of rigor in the validation logic. The other values to be validated are exposed in the function but not used."
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a valid bug because the incomplete validation creates security vulnerabilities. Single quotes in plugin_name or plugin_setting can inject malicious content into Dockerfile ENV statements and disrupt command-line operations. The current validation only checks setting_value, but all three parameters are used interchangeably in shell/Docker contexts when setting the environment variables. This inconsistency violates the security boundary and could enable command injection or deployment failures."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "validate_plugin_secret enforces the \u201cno single quotes\u201d rule only for setting_value while leaving plugin_name and plugin_setting unchecked. Those two strings are later used to construct environment variable names and appear in generated Dockerfiles/CLI invocations. Accepting single quotes there is inconsistent with the stated constraint and can break generated Dockerfiles or tooling (and, depending on surrounding quoting, create injection/escaping hazards). The property-based tests and minimal repro show inputs with ' in plugin_name or plugin_setting pass validation even though they should be rejected alongside setting_value."
      }
    ]
  },
  {
    "data_row_id": "cmg1giq1ikmwa0772cwl3d4b0",
    "global_key": "html_bug_report_b1cffac9_6784ed0d",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_b1cffac9.html",
    "votes": [
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes this is a bug, it appears the normalise_float_repr function is supposed to take a string representing float and normalize it into a constant parseable form. \n\n"
      },
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a clearcut case of failing edge cases"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq1okmwb0772k97cu6zq",
    "global_key": "html_bug_report_de72ff9a_f3cb0a5e",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_de72ff9a.html",
    "votes": [
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This report exposes an error in a reasonable edge case "
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Silently creates invalid character ranges with reversed bounds, violating documented contract and mathematical invariants. Range('z','a') creates CodeRange(122,98) where no character satisfies 122 <= c < 98, causing silent failures."
      }
    ]
  },
  {
    "data_row_id": "cmg1giq2ykmwj0772ptos6xf3",
    "global_key": "html_bug_report_aa402bff_6cdafb9a",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_aa402bff.html",
    "votes": [
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because read_block is supposed to let you read a file in consecutive chunks without losing any data, but if it seeks past the end while looking for a delimiter, it just returns empty bytes instead of the leftover content so some data silently disappears. "
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "fsspec.utils.read_block with a delimiter is intended to split data cleanly while preserving all bytes. The repro shows that when the second read starts near EOF, the function seeks to start+length, scans for a delimiter, lands at EOF with found_end_delim=False, sets end = f.tell() (== start), and then returns b\"\". That drops any remaining bytes (b'\\x01\\x00' in the example). Reading a file sequentially must never silently lose trailing data; returning an empty block here violates that contract and breaks round-tripping (concatenation of blocks \u2260 original)."
      },
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "I was able to reproduce the bug and it is evident that some data is being lost at the end of the file. There is no errors which makes it dangerous since the data is being corrupted and the root cause is hard to find."
      }
    ]
  },
  {
    "data_row_id": "cmg1giq33kmwk0772w4bk9qkj",
    "global_key": "html_bug_report_7b0094e2_25875d36",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_7b0094e2.html",
    "votes": [
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because the template is supposed to let runtime substitute() arguments override the default namespace values, but right now the defaults actually overwrite the runtime values."
      },
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "substitute method is unable to substitute the value of namespace being this a default value"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq3akmwl0772ruq6um3d",
    "global_key": "html_bug_report_3fb1e2d1_a0073cb8",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_3fb1e2d1.html",
    "votes": [
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "I was able to reproduce this bug and the code is not scaping the title or the openapi_url fields which can in fact allow XSS"
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug. the functions directly inject user input into HTML without escaping, which is a classic Cross-Site Scripting (XSS) vulnerability."
      }
    ]
  },
  {
    "data_row_id": "cmg1giq3gkmwm0772i30iq48c",
    "global_key": "html_bug_report_a5d8163f_ed4b95ac",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_a5d8163f.html",
    "votes": [
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "-"
      },
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "I was able to reproduce the bug and in fact after parsing the string, it separates the two star symbols with a space in between"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq3lkmwn0772b9nuer6h",
    "global_key": "html_bug_report_de251c45_d7c95e49",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_de251c45.html",
    "votes": [
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Although would not be common to use a key ending or starting with colons, this is in fact an unexpected behavior that makes this code susceptible to bugs and unexpected behavior in production"
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "make_template_fragment_key() concatenates vary_on parts with b':' before hashing. Different inputs that include : can yield the same byte stream (e.g., [\"a:\", \"b\"] \u2192 b\"a::b:\" and [\"a\", \":b\"] \u2192 b\"a::b:\"), so the md5 input is identical and the keys collide. That violates the function\u2019s purpose, distinct vary_on should produce distinct keys\u2014and can mis-serve cached fragments."
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Because different vary_on lists produce identical cache keys when elements contain the separator character :, which breaks the fundamental contract that different inputs should yield different keys. The function concatenates list elements with : separators without escaping, so ['a:', 'b'] becomes 'a::b:' and ['a', ':b'] also becomes 'a::b:' - same hash input, same cache key. "
      }
    ]
  },
  {
    "data_row_id": "cmg1giq3rkmwo0772qhc7bk9l",
    "global_key": "html_bug_report_33217daf_8ea1b619",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_33217daf.html",
    "votes": [
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": null
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "Yes this breaks Dasks partition invariants. The function is producing a list where elements at index 1> elements at index 2.  The boundaries in the partition are coming back out of order. "
      }
    ]
  },
  {
    "data_row_id": "cmg1giq3wkmwp0772q2vxhzy3",
    "global_key": "html_bug_report_6f1039ab_4b3c13dd",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_6f1039ab.html",
    "votes": [
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": null
      },
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": null
      }
    ]
  },
  {
    "data_row_id": "cmg1giq41kmwq0772nlh1stek",
    "global_key": "html_bug_report_01879888_c5a83340",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_01879888.html",
    "votes": [
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": null
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a bug because:\n\nViolates API contract: Dask's documentation states resample should work like pandas, but it fails on valid pandas use cases\nIncorrect logic: The function uses pd.date_range(start, end, freq=rule) to validate indices, but anchor-based frequencies naturally create indices outside the original data range\nRejects valid results: Pandas resample correctly produces results (e.g., Jan 31 for January data), but Dask incorrectly rejects these as invalid\nMisleading error message: The suggestion to \"use larger partitions\" is completely irrelevant - the issue occurs regardless of partition size\nBreaks common use cases: Monthly, quarterly, and weekly reporting on partial datasets are legitimate business analytics scenarios"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq47kmwr0772kaz7i0ah",
    "global_key": "html_bug_report_ae56a6e8_a5245ff3",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_ae56a6e8.html",
    "votes": [
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": null
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": null
      }
    ]
  },
  {
    "data_row_id": "cmg1giq4ckmws0772b62tttyu",
    "global_key": "html_bug_report_b7814f3d_7e1dbb51",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_b7814f3d.html",
    "votes": [
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": null
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is definitively a bug because:\n\nViolates API contract: The documentation explicitly states it should behave \"like in the regular Python slice object\"\nIncorrect swap logic: The condition if stop is None: triggers even when multiple arguments are provided, incorrectly swapping start and stop\nSilent data corruption: Returns wrong results without any indication of failure\nTwo distinct failure modes:\nWith step: slice(0, None, 2) becomes slice(None, 0, 2) \u2192 empty slice\nWithout step: slice(2, None) becomes slice(None, 2) \u2192 inverse slice\nPython standard violation: Explicit None should mean \"to the end\" but the function treats it as a trigger for argument swapping"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq4ikmwt0772g3pxybp3",
    "global_key": "html_bug_report_2437f641_dbba5a8b",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_2437f641.html",
    "votes": [
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": null
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "This is definitively a bug because:\n\nViolates Python data model: Breaks the symmetry requirement (x == y iff y == x) which is fundamental to Python\nHash/equality invariant violation: Missing __hash__ implementation means equal objects can have different hashes\nProxy pattern violation: The proxy should behave transparently like the wrapped object, but it doesn't\nInconsistent behavior: proxy == conn is True but conn == proxy is False - this is mathematically and logically inconsistent\nUser-facing impact: Affects Django's public API through django.db.connection\nPractical problems: Makes ConnectionProxy objects unsuitable for use in sets and as dictionary keys\nThe bug is in the __eq__ implementation that only delegates in one direction and the missing __hash__ method that should maintain consistency with equality.\n\n"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq4pkmwu0772h4p26p4v",
    "global_key": "html_bug_report_1526d558_f8290be6",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_1526d558.html",
    "votes": [
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": null
      },
      {
        "rater_id": "cmcn2m4e40ym607zhctft4h6m",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "In pandas, -1 encodes missing; doing codes % len(categories) maps -1 to a valid index (e.g., -1 % 3 = 2), so nulls silently become categories, corrupting data. That violates pandas\u2019 categorical semantics and the interchange protocol\u2019s requirement to preserve missingness"
      }
    ]
  },
  {
    "data_row_id": "cmg1giq4vkmwv0772bjyyaqzo",
    "global_key": "html_bug_report_d25037b9_1c38458f",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_d25037b9.html",
    "votes": [
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": null
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Formula (stop - start) // step is mathematically incorrect, violating fundamental property that lengths must be non-negative."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": null
      }
    ]
  },
  {
    "data_row_id": "cmg1giq50kmww0772cbnbv5rv",
    "global_key": "html_bug_report_15dd124c_8b8fb541",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_15dd124c.html",
    "votes": [
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": null
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Line 1550 creates new SparseArray and calls cumsum() recursively, causing infinite loop for non-null fill values (default for integers)."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": null
      }
    ]
  },
  {
    "data_row_id": "cmg1giq57kmwx0772xvncyhj8",
    "global_key": "html_bug_report_21529c1d_10eb5250",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_21529c1d.html",
    "votes": [
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": null
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Line 1550 creates recursive loop by calling cumsum() on new SparseArray instead of computing cumsum on dense array first."
      }
    ]
  },
  {
    "data_row_id": "cmg1giq5ckmwy0772yr7ilas6",
    "global_key": "html_bug_report_2373fa53_ed4ca75d",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_2373fa53.html",
    "votes": [
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": null
      },
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": null
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": " Function accepts floats despite int | np.integer signature, silently returns wrong values for large integers (>2^53) due to float64 precision limits."
      }
    ]
  },
  {
    "data_row_id": "cmg1giq5ikmwz07724o9wcg9y",
    "global_key": "html_bug_report_0db56c40_5c8919ec",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_0db56c40.html",
    "votes": [
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": null
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Same parenthesis placement error - calls cumsum() on new SparseArray instead of dense array, creating infinite recursion for non-null fill values."
      }
    ]
  },
  {
    "data_row_id": "cmg30jmgp57a70710zrpyrybe",
    "global_key": "html_bug_report_6f46e306_f098cff0",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_6f46e306.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": " Same logic error - while loop condition other_time is None or other_time >= os.path.getmtime(file_path) becomes infinite when newer_than doesn't exist."
      },
      {
        "rater_id": "cmen027n80dql07yaflqa5233",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": null
      }
    ]
  },
  {
    "data_row_id": "cmg30jmgt57a80710y2qs4atb",
    "global_key": "html_bug_report_cc2ea72a_64d19f14",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_cc2ea72a.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Violates three Python object model principles: type safety (Node equals tuple), breaks set operations (can remove Node with tuple), and @total_ordering contract. Allows silent type errors instead of clear TypeErrors."
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The Node.eq implementation at https://github.com/django/django/blob/main/django/utils/connection.py violates fundamental Python object model requirements by comparing self.key == other instead of other.key, causing Node objects to be considered equal to their key tuples. "
      }
    ]
  },
  {
    "data_row_id": "cmg30jmgx57a90710c0y3q7qs",
    "global_key": "html_bug_report_731bf84f_f1e09776",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_731bf84f.html",
    "votes": [
      {
        "rater_id": "cmcn2m4e40ym607zhctft4h6m",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "it breaks the function's own contract, ULID sort by timestamp first, so when the system clock moves backwards the function emits a ULID with a similar ts, making it less than the previous one, violating the monotonicity."
      },
      {
        "rater_id": "cmf5wpk0b03c4070u3lyd5p5h",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It\u2019s a bug because monotonic_ulid() explicitly promises to return identifiers that are strictly larger than any previously returned in the same process, yet when the system clock moves backward (a realistic scenario due to NTP slews, VM migration, or manual corrections) the function emits a ULID with a smaller timestamp prefix, making the whole ULID sort earlier than the previous one. Since ULIDs are ordered primarily by their timestamp component, this breaks the contract and can invalidate ordering assumptions, causality, and de-duplication logic that rely on monotonic growth."
      }
    ]
  },
  {
    "data_row_id": "cmg30jmh157aa0710fthsodas",
    "global_key": "html_bug_report_5a3f3ad9_28c617fe",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_5a3f3ad9.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Violates scanner contract - should return (None, '') on EOF but loops infinitely with nullable patterns. scan_a_token method fails to advance position when empty match occurs at non-EOF position."
      },
      {
        "rater_id": "cmcn2m4e40ym607zhctft4h6m",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a bug because it doesnt make forward progress on zero-width matches. With nullable patterns keep returning empty tokens at the same position instead of advancing or raising, which causes infinite loop"
      }
    ]
  },
  {
    "data_row_id": "cmg30jmh457ab0710exzh496v",
    "global_key": "html_bug_report_00c9f57c_41875228",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_00c9f57c.html",
    "votes": [
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is absolutely a critical bug that breaks the fundamental correctness of lexical analysis- and I reproduced it to see exactly how devastating the impact is. The chars_to_ranges function is supposed to create character ranges that cover exactly the input characters, but instead it incorrectly includes extra characters that were never in the input."
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "The chars_to_ranges function violates its core contract: it should create ranges covering exactly the input characters, no more, no less. When input contains duplicates (like 'aca'), the condition code2 >= ord(char_list[i]) incorrectly treats duplicates as evidence of consecutive characters, extending ranges to include characters not present in the original input (like 'b'). This breaks fundamental lexical analysis semantics where [ac] should match only 'a' or 'c', never 'b'.\n\n"
      }
    ]
  },
  {
    "data_row_id": "cmg30jmh757ac07104cr25wwm",
    "global_key": "html_bug_report_ab8ee3b2_28ad60f1",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_ab8ee3b2.html",
    "votes": [
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Logic error in _resample_bin_and_out_divs function uses single lambda function for both newdivs (5 elements) and outdivs (4 elements) arrays with different lengths, causing partition mismatch that leaves last partition unprocessed. Violates Dask's pandas compatibility contract and causes 100% data loss for final time period."
      },
      {
        "rater_id": "cmfvgajqr08qk070n3vyihwq9",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The report clearly represents a case where erroneous data is returned by the dask dataframe "
      }
    ]
  },
  {
    "data_row_id": "cmg30jmhb57ad0710g059268v",
    "global_key": "html_bug_report_0f07f5bc_2511e9c6",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_0f07f5bc.html",
    "votes": [
      {
        "rater_id": "cmcn2m4e40ym607zhctft4h6m",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because it simply corrupts the data when you change `fill_value`. Constructing `SparseArray(new_fill)` from an existing `SparseArray(old_fill)` reuses the sparse structure, so values equal to the old fill are interpreted as the new fill (all 0.0 become 1.0), violating the expectation that the data stays the same when changing the representation "
      },
      {
        "rater_id": "cmcz60z1n0eda07ze4be3d0d8",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Violates core principle that creating array from another should preserve data values. When fill_value changes, code incorrectly reuses sparse representation (empty sp_values and sp_index) without recalculation, causing implicit values (equal to old fill_value) to be misinterpreted as new fill_value. This is particularly insidious as it fails silently and only affects specific data patterns."
      }
    ]
  },
  {
    "data_row_id": "cmg30jmhe57ae0710bpfeh124",
    "global_key": "html_bug_report_1f6028f7_625b601e",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_1f6028f7.html",
    "votes": [
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's not really a bug, SparseArray.max()/.min() never promised to support skipna in the first place."
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "it's a bug that violates fundamental NumPy and pandas conventions. When you call SparseArray.max(skipna=False) or SparseArray.min(skipna=False) on arrays containing NaN values, the methods completely ignore the skipna parameter and return numeric values instead of NaN."
      }
    ]
  },
  {
    "data_row_id": "cmg30jmhi57af0710x09haa75",
    "global_key": "html_bug_report_25ac8a08_c6fe40df",
    "url": "https://nicholas.carlini.com/tmp/render/enhanced_report_25ac8a08.html",
    "votes": [
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because pandas promises that the dataframe interchange protocol should preserve nulls, but when you pass a nullable boolean column through it, the None/NA values silently get turned into False. A user would expect their missing values to stay missing, not to be rewritten as valid booleans."
      },
      {
        "rater_id": "cmcn2m4e40ym607zhctft4h6m",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a bug because the interchange protocol must preserve Nulls, while here, nulls in pandas \"boolean\" become false after round trip\nNumPy bool silently coerces none -> false, so `set_nulls` doesn't raise and all the nulls are lost.\nThis clearly corrupts the data and changes the Data Types from nullable boolean to non-nullable bool, which violates the rules. Hence, this is a legit bug."
      }
    ]
  }
]