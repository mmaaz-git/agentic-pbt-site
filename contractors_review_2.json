[
  {
    "data_row_id": "cmfrkaqao1msx0742dpqytr34",
    "global_key": "html_bug_report_112a37e1_1d52bb1b",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_112a37e1.html",
    "votes": [
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug, __eq__ ignores namespace while hash includes it, so two items with different namespaces can compare equal but hash differently. that breaks python\u2019s required contract for equality and hashing as documented here https://docs.python.org/3/reference/datamodel.html#object.__hash__. the code shows namespace is in slots, used by key_for, and included in hashing https://github.com/alisaifee/limits/blob/main/limits/limits.py. your repro shows equal objects with different hashes which makes sets and dicts behave wrong. it is not a security issue because enforcement normally uses the string key from key_for which already includes namespace, so this mismatch does not let you bypass limits or expose data."
      },
      {
        "rater_id": "cmc0hw57z03j907z313sb4ps6",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a genuine bug as  it clearly violates the fundamental rule that equal objects must have equal hashes."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yest this is a bug.  Python needs that if 2 objects are equal, then the hash should've matched, but RateLimitItem here when we are using different namespace are equal but they are giving different hashes. This mismatch is the bug, so to fix it we need to add self.namespace == other.namespace so that when we compare 2 of them it matches."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The official Python documentation explicitly requires that objects which compare equal must also have identical hash values. In this case, __eq__ ignores the namespace field while __hash__ includes it, leading to inconsistent behavior. The issue is reproducible with minimal input, where two objects compare equal but produce different hashes. This inconsistency directly breaks Python\u2019s contract and causes incorrect behavior in sets and dictionaries, confirming that this is a real bug rather than expected behavior."
      },
      {
        "rater_id": "cm6k35c1p000p07y633bwd4mu",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a bug because it violates Python's core hash/equality contract (equal objects must have equal hashes). The RateLimitItem.__eq__ method ignores the `namespace` attribute while __hash__ includes it, creating objects that are equal but have different hash values. This breaks fundamental assumptions about object behavior in hash-based collections (sets, dictionaries), causing incorrect duplicate handling and unpredictable behavior in production applications."
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqau1msy0742dsdhle63",
    "global_key": "html_bug_report_e7550d5d_4b7d691a",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_e7550d5d.html",
    "votes": [
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because srsly ujson truncates float precision for large finite numbers so loads dumps f is not equal to f around and above 1e16. the encoder switches to a fixed 15 digit exponential format in its large number path and that ignores the documented double precision setting. ieee 754 double requires up to 17 significant digits to guarantee round trip and python json does exactly that so it round trips the same inputs correctly. i reproduced on srsly 2.5.1 that 1.1983765554677512e16 is encoded as 1.198376555467751e16 and fails equality while python json preserves the original value. the installed c source confirms the threshold and the hardcoded 15 digit format which explains the silent loss. these are some related dicussion here  https://github.com/ultrajson/ultrajson/issues/69 , https://docs.python.org/3/library/json.html "
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It is a bug. It might be an internal decision but from a user's perspective the ujson silently loses precision for large floats and is not documented or any warning is not in the documentation. loads(dumps(x)) doesn't return the original number which violates the expectations and can corrupt data. Precision is very important, so I would consider this as a bug."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a real bug because srsly.ujson loses precision when handling floating-point numbers larger than 1e16. The issue is reproducible using property-based tests (Hypothesis) and direct comparisons with Python\u2019s built-in json module, which correctly preserves 17 significant digits. The root cause is a hardcoded snprintf(\"%.15e\") in the C implementation (ultrajsonenc.c), which restricts precision to 15 digits instead of the 17 required for IEEE 754 doubles. This leads to deterministic and silent data corruption."
      },
      {
        "rater_id": "cm6k35c1p000p07y633bwd4mu",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because it violates IEEE 754 standards for double precision floating-point representation. The hardcoded 15-digit precision limit (%.15e) in ultrajsonenc.c causes systematic precision loss for numbers >= 1e16; the standard requires 17 significant digits for accurate round-trip conversion of 64-bit doubles. This breaks the core expectation that JSON serialization libraries preserve numerical precision: loads(dump(x)) should equal x for all representable floating-point numbers. The bug causes silent data corruption in applications that depend on numerical precision (particularly in financial and scientific computing contexts)."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "1_very_uncertain_mostly_guessing"
        },
        "comments": "I believe this should be classified as a bug because it appears ujson is losing data or precessions when it is encoding/decoding, large floats ,while regular json does not appear to do the same. This is breaking the expectation of  encode ->decode"
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqay1msz0742jxxk3x5e",
    "global_key": "html_bug_report_6442c22f_01606d7a",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_6442c22f.html",
    "votes": [
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because the validate_arguments decorator is supposed to check every argument against its validator, but when a function uses **kwargs, it completely skips those checks. That means we could pass invalid values and nothing would complain, which defeats the whole point of having validation in the first place"
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The property-based test shows two failures: (a) no ValueError is raised when a validator should fail, and (b) validators are not called at all when the target function uses **kwargs. This matches how inspect.Signature.bind() creates a BoundArguments where extra keyword args are nested under a single kwargs entry, so checking arg_name in bound.arguments skips them. The proposed fix that also looks inside bound.arguments['kwargs'] (or equivalently bound.kwargs) correctly restores validation for **kwargs."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a bug because the decorator is failing to validate **kwargs but this is failing on x=-10. This a silent bypass, and would probably lead to inconsistent behavior. "
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "The decorator assumes each validated name will show up directly in bound.arguments, but for def func(**kwargs), inspect.Signature.bind() puts user-supplied keys under the VAR_KEYWORD parameter name instead. So the validator lookup misses those values and never runs- that's why uses_kwargs(x=-10) returns -10 with no validator calls in my test as well, while uses_regular(-10) properly raises ValueError. It's silent validation failure which contradicts what you'd reasonably expect from an argument-validation decorator. The behavior is documented in Python's inspect.Signature.bind and BoundArguments.arguments (https://docs.python.org/3/library/inspect.html#inspect.Signature.bind and https://docs.python.org/3/library/inspect.html#inspect.BoundArguments.arguments). The failure mechanism is completely explained by stdlib's binding rules and is reproducible"
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqb11mt0074204cchf4i",
    "global_key": "html_bug_report_0fc2005e_1b57b5b9",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_0fc2005e.html",
    "votes": [
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because numpy char case transform functions state they call the corresponding python str methods element wise yet when the unicode mapping expands length the output is silently truncated to the input array fixed width unicode dtype which loses data without any warning this violates reasonable user expectations from the docs and from python str behavior for example python str upper on sharp s returns ss while numpy char upper on an array with dtype u1 returns only s unless you widen the dtype this is also inconsistent within numpy char because operations like numpy char add do upcast the output dtype length to fit results while the case transforms do not making the api surprising and risky the truncation is caused by fixed width unicode dtypes in numpy so any write longer than the dtype width is cut off however the functions could pre compute needed lengths or warn or error instead of failing silently the property based test about swapcase involution is not itself a proof of a bug because that identity does not hold for all unicode but the reproduced silent truncation on expansion characters like sharp s and ligatures clearly shows data loss and a mismatch with the documented element wise str behavior and thus is a valid bug. These are some refereces https://github.com/numpy/numpy/issues/12256 ,  https://docs.python.org/3/library/stdtypes.html#str.upper"
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because NumPy\u2019s char functions promise to apply Python string methods element-wise, but when a transformation like '\u00df'.upper() expands the string to 'SS', NumPy silently truncates it to 'S' due to fixed-size dtypes. It breaks expected behavior, causes silent data loss, and violates the documented contract, so users can't rely on these functions for correct Unicode text processing."
      },
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "The property-based test (swapcase(swapcase(x)) == x) fails on inputs like '\u00b5' and '\u00df'. NumPy\u2019s numpy.char.upper/swapcase/capitalize/title silently truncate results when the Unicode case transformation expands string length beyond the allocated dtype. For example, '\u00df' uppercases to 'S' instead of 'SS'. This contradicts the documentation promise (\u201cCall str.upper() element-wise\u201d), violates Unicode case-mapping rules, and causes silent data loss. Python\u2019s built-in str.upper('\u00df') correctly returns 'SS', proving NumPy\u2019s behavior diverges from expected semantics"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Numpy has fixed string length, which is part of its design. I think the overall issue is probably the test it's self.\n\nassert np.array_equal(arr, swapped_twice)\n\nwhich is expecting 100% accuracy with unicode swapping, which I don't believe is the case. "
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The NumPy documentation explicitly promises that char.upper \"Calls str.upper element-wise\", but my test shows '\u00df'.upper() returns 'SS' while np.char.upper(np.array(['\u00df']))[0] returns 'S'. This happens because NumPy creates arrays with fixed-width dtypes (like <U1) and truncates results that don't fit, while Python strings can expand dynamically. The same issue affects swapcase, capitalize, and title functions. My reproduction confirmed the broken involution property where swapcase(swapcase('\u00df')) should equal '\u00df' but returns 's' instead due to the truncation chain: '\u00df' -> 'S' (truncated from 'SS') -> 's'."
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqb51mt107429623pl6e",
    "global_key": "html_bug_report_63a73733_e0202df5",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_63a73733.html",
    "votes": [
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "Local execution reproduces the unguarded self-reference: inserting a StringIOTree into itself and then calling getvalue() triggers infinite recursion. In my environment, this yields a RecursionError; in the report\u2019s environment it escalates to a segmentation fault, which is plausible if a C-optimized path exhausts the C stack. Either way, the API allows a self-insertion that violates safety expectations and can crash the interpreter on some builds. The behavior matches the report\u2019s analysis and call chain (insert \u2192 getvalue \u2192 _collect_in on children \u2192 self). "
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "A tree referencing it's self is possible, but I think the issue is how .StringIOTree is handling the implementation, it should not allow it since get value being called on the children causes infinite recursion, then segmentation fault. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug, inserting a stringiotree into itself creates a cycle in its child lists. the traversal used by getvalue via the internal method collect in walks children recursively without cycle detection. when self is present in its own children the traversal never terminates and recursion grows until the interpreter raises a recursionerror or in some environments the process may crash. the insert api and typical python expectations do not warn about or require callers to avoid self reference and standard containers tolerate self reference without crashing. therefore allowing self insertion without a guard or cycle detection violates reasonable expectations and leads to non graceful failure. There is no direct reference but here  are some related reference: https://github.com/cython/cython/blob/master/Cython/StringIOTree.py ,"
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "insert() accepts any StringIOTree including self. When you insert self, commit() preserves prior content as a child, then appends the passed tree to prepended_children, forming a cycle. getvalue() and copyto() recursively traverse children, so the tree calls itself indefinitely. I reproduced this locally (Cython 0.29.28, Python 3.10.12) with RecursionError; the original report's segfault is probably environment/version-dependent, but the core infinite recursion bug is the same. The source shows no self-cycle check in Cython's StringIOTree implementation.\nLinks: https://github.com/cython/cython/blob/master/Cython/StringIOTree.py"
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Python libraries must raise exceptions, not crash the process. The API doesn\u2019t document a self/ancestor restriction, and there\u2019s no cycle detection, so behavior violates reasonable expectations and safety."
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqb91mt2074210msnwg4",
    "global_key": "html_bug_report_c9662323_4befb1d2",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_c9662323.html",
    "votes": [
      {
        "rater_id": "cmb8h9y3u0bl9072c3ysl0or7",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The function dereferences cdc_response_dict[0] without checking if the list is empty. My local reproduction raises IndexError when CDCResponse is []. This matches the report\u2019s traceback and is a legitimate \u201cno changes since timestamp\u201d scenario that should not crash. The proposed guard (if not cdc_response_dict: return CDCResponse.from_json(resp)) prevents the crash and preserves expected semantics."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "On just looking at this, I don't believe this is a bug. It seems the text expects change_data_capture function to return a non empty list, but CDCResponse returns an empty list, and the test is written to access index 0. so it returns an error. "
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "it\u2019s a bug, and it\u2019s reproducible. The implementation in quickbooks/cdc.py unconditionally indexes the first element of the CDCResponse list (cdc_response_dict[0]['QueryResponse']) without checking if the list is empty. When the QuickBooks Online CDC endpoint has no changes since the given timestamp, returning an empty list is a perfectly legitimate outcome, and this exact path deterministically raises IndexError (no guesswork needed). That behavior violates reasonable expectations for a polling/sync helper: \u201cno changes\u201d should yield a valid empty result, not a crash. There\u2019s also prior evidence from the project\u2019s issue tracker that \u201cno changes\u201d responses do happen and have caused failures before. Resources I checked to verify this: the current source showing the unguarded index in change_data_capture https://github.com/ej2/python-quickbooks/blob/master/quickbooks/cdc.py, the original QuickBooks Online CDC docs establishing that CDC returns only recent changes (so \u201cno changes\u201d is a normal state) https://developer.intuit.com/app/developer/qbo/docs/learn/explore-the-quickbooks-online-api/change-data-capture, and a prior discussion confirming breakage when there are no changes https://github.com/ej2/python-quickbooks/issues/149. The mocked reproduction in bug report aligns exactly with that code path, so the IndexError is expected and confirms the bug."
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The CDC API legitimately returns {'CDCResponse': []} when there are no changes. The function blindly indexes [0], throwing IndexError and crashing instead of returning an empty result which causes DoS"
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because the QuickBooks CDC API can return an empty CDCResponse when no changes exist, but the library's change_data_capture function assumes the list is non-empty and unconditionally accesses index [0], causing an IndexError. Instead of crashing, the function should gracefully handle the \"no changes\" case by returning an empty result."
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqbd1mt30742691ec9xz",
    "global_key": "html_bug_report_6205b865_9cc29d8d",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_6205b865.html",
    "votes": [
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "'1a0' in '>=0,>=0a0' returns True while '1a0' in '>=0' is False\u2014blatant logic violation with a minimal reproduction."
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this isn\u2019t a bug. SpecifierSet does use AND logic across its component specifiers, but the \u201callow prereleases\u201d decision is intentionally computed at the set level, not per individual comparator. Per the current design, if any specifier in the set allows prereleases (e.g., because it explicitly mentions one like >=0a0), the whole set evaluates with prereleases allowed; that\u2019s why 1a0 is rejected by '>=0' on its own but accepted by '>=0,>=0a0'. This behavior matches packaging\u2019s documented and discussed semantics around the three-valued prereleases policy (True/False/None), where \u201cany allow\u201d lifts the prerelease gate for the set, rather than violating AND logic on the comparisons themselves. the specifiers documentation in packaging explains prerelease handling and set semantics https://github.com/pypa/packaging/blob/main/docs/specifiers.rst and multiple design discussions capture the intended behavior and edge cases, e.g. https://github.com/pypa/packaging/issues/48, https://github.com/pypa/packaging/issues/895, and https://github.com/pypa/packaging/issues/917. The reproduced behaviour is accurate, but it demonstrates the designed \u201cset-level prerelease policy,\u201d not a broken AND."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "I think that it's not a bug but an intentional design choice: in packaging.SpecifierSet, prerelease handling is global, meaning if any specifier mentions a prerelease, prereleases are allowed for the entire set. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because PEP 440 says comma separated specifiers use and logic and prereleases are excluded by default unless a specifier explicitly mentions a prerelease. the specifier >=0 on its own rejects prereleases and the specifier >=0a0 on its own accepts prereleases. when combined as >=0,>=0a0 the current implementation in specifierset sets a single prereleases policy for the whole set based on any member accepting prereleases and then applies that to every member check. this makes 1a0 pass the combined set even though it fails >=0 by itself which violates the required and logic. the cause is that specifierset dot prereleases returns true if any member accepts prereleases and specifierset dot contains forwards that unified prereleases flag to each member. the correct behavior is to evaluate each specifier under its own prerelease policy unless the caller explicitly overrides it. it is not a direct security issue because there is no exploit path or boundary bypass, but it can lead to unintended prerelease installs which is a reliability and quality risk rather than a vulnerability. here are some references: https://peps.python.org/pep-0440/#pre-releases ,https://packaging.pypa.io/en/latest/specifiers.html"
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "As per PYPA guidelines, prereleases are excluded by default unless explicitly included in the specifier set. In \">=0,>=0a0\", the presence of >=0a0 explicitly requests a prerelease, so the prereleases flag is auto-evaluated to True, which is expected behavior. The AND logic applies only when the prereleases setting is consistent across specifiers, which is not the case here. Explicitly setting prereleases=False would produce the expected result. Therefore, this is expected behavior, not a bug."
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqbh1mt40742vznlqhny",
    "global_key": "html_bug_report_2b0e6d1c_3914ee39",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_2b0e6d1c.html",
    "votes": [
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "it\u2019s not a bug. dparse is parsing pip-style requirement lines, and pip\u2019s hash-checking mode uses hex-encoded digests (base16), not base64; the examples and typical output from pip show long lowercase hex strings for sha256. Given that, a test that feeds base64-like digests with \u201c+/=\u201d is asserting a behavior pip itself doesn\u2019t produce, so the truncation you\u2019re seeing is just the regex not matching an unsupported format rather than corrupting a valid pip hash. I verified the current dparse pattern is defined as --hash[=| ]\\w+:\\w+ in the source, and cross-checked pip\u2019s \u201cSecure installs\u201d docs that demonstrate hashes in the hex form used by requirements files. References: dparse\u2019s HASH_REGEX in source (https://github.com/pyupio/dparse/blob/65e709188aee398a1b97a4d497e60f1024d88b8f/dparse/regex.py); pip\u2019s docs on hash-checking mode with example requirement lines showing hex digests (https://pip.pypa.io/en/stable/topics/secure-installs.html, https://github.com/pypa/pip/blob/main/docs/html/topics/secure-installs.md)."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "This is a bug because dparse advertises support for parsing pip requirement hashes, but its regex \\w+ only matches [A-Za-z0-9_], which silently truncates valid base64-encoded hashes containing +, /, or =\u2014characters pip itself emits. This might be intended behaviour but it is not mentioned in the documentation. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is not a real bug for pip requirements because pip hash checking mode expects hexadecimal digests while the reports failing inputs and property test use base64 characters such as plus slash and equals which are not valid in a requirements file hash value. the reproducer uses a base64 style value after algo colon which pip does not accept so the observed truncation does not contradict pip documented behavior for valid inputs. dparse\u2019s regex therefore is not shown to mishandle valid pip hashes and the claim that standard pip functionality is broken does not follow from the provided examples. the wheel record format does use urlsafe base64 with an algo equals digest syntax which is a different context from requirements files and is not evidence that requirements hashes should be base64. there is a separate minor regex quality issue the use of a character class that includes the pipe and an overly loose digest matcher but that is unrelated to the base64 claim raised in the report. Here are some references: https://pip.pypa.io/en/stable/cli/pip_hash/ ,https://pip.pypa.io/en/stable/topics/repeatable-installs/ ,https://peps.python.org/pep-0427/#the-record-file"
      },
      {
        "rater_id": "cmc0hw57z03j907z313sb4ps6",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a genuine bug as \"HASH_REGEX\" doesn't take into account all the Hashes that can be for base64 and hence the failure to parse."
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "parse_hashes truncates hashes and corrupts the \u201ccleaned\u201d line\u2014repro is minimal and deterministic."
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqbk1mt50742t2vt9hpe",
    "global_key": "html_bug_report_8a20a7aa_f3a920b1",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_8a20a7aa.html",
    "votes": [
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "no",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It is not a bug. I tried to reproduce the \u201c>1.3 error at training points with smoothing=0\u201d on my machine (Python 3.10.12, NumPy 1.21.5, SciPy 1.8.0). With the provided nearly colinear inputs, I consistently saw either an exact fit to machine precision or a LinAlgError: Singular matrix at construction. That lines up with the docs: the interpolant \u201cperfectly fits the data when smoothing=0\u201d provided the system is well\u2011posed-e.g., P(y) has full column rank (for degree=1, points aren\u2019t all colinear) and locations are distinct. Nearly colinear geometries can be numerically rank\u2011deficient; throwing an error in that case is expected and doesn\u2019t violate the guarantee. If there\u2019s a specific SciPy/NumPy version where the solver returns without error yet evaluating at the training points yields a large residual, I\u2019d consider that a real bug in that version\u2019s solver path. I just couldn\u2019t reproduce that behavior here."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug in practice but not in theory. The math behind RBFInterpolator guarantees exact interpolation at training points when smoothing=0, yet in nearly colinear cases the implementation silently returns incorrect values due to numerical instability instead of warning or failing."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because it violates the explicit documentation guarantee that with smoothing equal to zero the interpolant perfectly fits the data at the training points. the bug report shows a concrete failing input with distinct points that are nearly but not exactly colinear and a default configuration where rbfinterpolator returns large errors at the training points greater than one rather than roundoff level differences. the property based test also detects this behavior and the minimal repro prints a maximum error that is far above numerical noise. the report explains that the underlying linear system is ill conditioned in this geometry and that the current implementation solves it without conditioning checks or regularization which can yield inaccurate coefficients and thus inaccurate values even at the data locations. this contradicts the user visible contract in the docs and therefore qualifies as a correctness bug. maintainers would likely welcome the report because it highlights a mismatch between documented behavior and actual behavior and it comes with a clear reproducer and actionable directions to mitigate such as condition number checks warnings or a small automatic smoothing and potentially a more robust solver. this is not a security issue because it is an accuracy and numerical stability problem without any pathway to code execution memory corruption privilege escalation or information disclosure. Here is some references: https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.RBFInterpolator.html , "
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "deterministic case matches to machine epsilon."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "The expected behavior for RBFinterpolator  is that it should return the training values, exactly  but since it's returning errors, > 1.3, it's violating this. "
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqbo1mt60742z74v0ojv",
    "global_key": "html_bug_report_86bf5e2f_cca94dbd",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_86bf5e2f.html",
    "votes": [
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It\u2019s a bug. I reproduced the behavior locally by constructing a multipart part where the filename is interpolated directly into the Content-Disposition header without sanitization. When the filename contains CRLF sequences, the header is broken across lines and the payload gains extra headers; I observed two Content-Type headers and the injected \u201cX-Injected-Header\u201d inside the same part. That violates basic HTTP/MIME header grammar and the intent of RFC 7578 for multipart/form-data, and it\u2019s a textbook CRLF injection (CWE\u2011113) stemming from trusting an unvalidated header parameter. Allowing CRLF in the quoted filename enables header injection within the multipart part, which can override or add headers like Content-Type, confuse parsers, bypass filters, or corrupt the multipart structure."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "This is a bug because the MultipartDataGenerator directly inserts the filenames into HTTP headers without any checks, allowing CRLF sequences to break the head structure. "
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the report shows that the multipartdatagenerator directly inserts the raw filename into the content disposition header without sanitizing control characters, and a property based test demonstrates that a filename containing crlf causes an extra content type header to appear. that violates rfc 7578 which requires proper encoding and forbids control characters within header parameter values, so the behavior breaks protocol correctness and reasonable developer expectations. maintainers would want this fixed because it is a clear standards violation at a user facing boundary and the report includes a minimal failing example and a straightforward remediation plan. it is a security issue because crlf in a header value enables http header injection consistent with cwe 113, allowing an attacker to inject or override headers within the multipart part, which can alter server side parsing and potentially bypass validation or content scanning. Here are some references: https://datatracker.ietf.org/doc/html/rfc7578 ,https://cwe.mitre.org/data/definitions/113.html"
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes. They interpolate the raw filename into Content-Disposition without sanitizing, so CRLF in the name breaks the header boundary and injects extra headers\u2014classic CRLF/header-injection."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes this is a bug. CRLF sequences like \\r\\n can be injected into http headers, with arbitrary data. This should not happen. This could also potentially lead to XSS attacks. HTTP headers should never allow this. "
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqbs1mt707423vxb7atf",
    "global_key": "html_bug_report_86140cbe_987d0aa0",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_86140cbe.html",
    "votes": [
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because the ExponentialRetry class calculates timeouts using the attempt number directly in the exponent, which assumes 0-based indexing, but the client passes 1-based indices. As a result, the first retry waits factor times longer than intended."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the retry loop passes a one based attempt number into timeout calculations that clearly expect zero based semantics. the exponential strategy multiplies start timeout by factor to the power of attempt which makes the first retry wait start timeout times factor instead of start timeout. that violates the meaning of start timeout as the first wait and deviates from standard exponential backoff which starts at power zero. the client code increments current attempt before calling get timeout so the exponent is off by one on every retry. list retry uses the attempt as a list index which is naturally zero based so it will skip the first element and risks indexing issues at the end. jitter retry subclasses exponential retry and therefore inherits the same off by one behavior. the provided property test and minimal reproduction both demonstrate that every valid input produces longer than intended waits confirming a consistent logic error rather than an edge case. here are some related references: https://github.com/inyutin/aiohttp_retry/issues/91 ,https://github.com/inyutin/aiohttp_retry/blob/master/aiohttp_retry/retry_options.py#L189-L227"
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The client increments current_attempt to 1 before calling get_timeout, and ExponentialRetry then uses start_timeout * (factor*attempt). That means the very first backoff is startfactor^1 instead of the base (start*factor^0). ListRetry does the same kind of off-by-one: it indexes timeouts[attempt], so it skips index 0 on the first retry. The class docstring literally says \u201cBase timeout time, then it exponentially grow,\u201d which strongly implies the first backoff should equal the base. I reproduced this locally, and the first backoff prints 0.2 (upstream) vs 0.1 (expected) for start=0.1, factor=2. That\u2019s a clean, minimal mismatch, not just theory."
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes. ExponentialRetry.get_timeout() treats the 1-based attempt from the caller as the exponent, so the first retry waits start_timeout * factor\u00b9 instead of start_timeout\u2014an off-by-one that shifts the whole backoff curve."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This seems to be a bug because the retry is increasing twice as long, which I don't believe it should. It goes from 2, 4 to 8 "
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqbv1mt80742molx6ndl",
    "global_key": "html_bug_report_084c479a_88ab275e",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_084c479a.html",
    "votes": [
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "This is a bug because Django's make_template_fragment_key is supposed to give a unique cache key for each unique set of inputs, but using a colon as a separator means inputs like [\"user:123\"] and [\"user\", \"123\"] end up with the same key, causing cache collisions and potentially exposing the wrong data to users. This can cause a security issue like data leakage."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because the function builds the hash by concatenating each vary on item with a colon as a separator which makes the serialization ambiguous when an item itself contains a colon. as a result the single item list user colon 123 and the two item list user and 123 both serialize to the same bytes user colon 123 colon and therefore hash to the same value which produces identical cache keys. the property based test and the concrete reproductions in the report demonstrate this collision including the failing input fragment name zero and vary item zero colon zero. this violates the documentation promise for the cache template tag that each unique set of vary on arguments yields a unique cache entry so it is a logic error in key generation rather than expected behavior. because template fragment caching relies on unique keys this can cause cache poisoning data leakage and incorrect content being served. Here is some reference: https://docs.djangoproject.com/en/stable/topics/cache/#template-fragment-caching"
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It\u2019s a bug. The implementation concatenates each vary_on value followed by a colon and hashes the result, so [\"user:123\"] serializes to the same bytes as [\"user\", \"123\"] (user:123:). That\u2019s ambiguous serialization, not a cryptographic hash collision, and it violates the contract that distinct vary arguments produce distinct cache entries as documented in Django\u2019s docs https://docs.djangoproject.com/en/stable/topics/cache/#template-fragment-caching. You can see the exact code path in make_template_fragment_key (it updates the MD5 with str(arg).encode() and then b\":\"): https://github.com/django/django/blob/main/django/core/cache/utils.py, and how vary_on flows into it via the cache template tag: https://github.com/django/django/blob/main/django/templatetags/cache.py. I reproduced it locally: [\"user:123\"] and [\"user\", \"123\"] generate identical keys; same for [\"a:b\"] vs [\"a\", \"b\"], and multi-colon variants. The hashing isn\u2019t at fault https://docs.python.org/3/library/hashlib.html;"
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "different vary_on inputs serialize to the same byte sequence because \":\" is used as an unescaped separator, so [\"a:b\"] and [\"a\",\"b\"] hash to the same key. C"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "The function is not handing colons properly when inputting value. This is causing a collision and generating identical keys. Of course this leads to data corruption,  and keys could be used by an attacker. "
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqbz1mt90742zqsu08wb",
    "global_key": "html_bug_report_fbea9bbd_63a854d9",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_fbea9bbd.html",
    "votes": [
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug, not intentional design: shortest_prefix is supposed to return the actual prefix key found in the trie (per pygtrie's documentation and longest_prefix's function), but instead it returns the query key paired with a value from a different key, creating an impossible state. The mismatch with the sibling method also confirms this is an implementation error, almost certainly a typo."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a bug because the method violates the documented contract of finding and returning the actual shortest prefix key and its value. the implementation returns the user supplied query key instead of the matched prefix key which creates an impossible key value pair that does not exist in the trie. this contradicts both the underlying pygtrie behavior and the librarys own longest prefix and prefixes methods which return the real matched keys. the property based test and minimal reproduction in the report demonstrate the discrepancy with clear failing inputs and show the inconsistency versus longest prefix. therefore it is a real bug with high confidence. maintainers would welcome the report because it is a one line fix that restores api correctness and consistency and the report includes a precise fix and good repros. it is not a security issue because it does not enable code execution data exfiltration or denial of service it is a correctness error that could lead to wrong routing or logic only if an application misuses the returned key. Here is some references: https://pygtrie.readthedocs.io/en/latest/ ,https://github.com/iterative/sqltrie/blob/main/src/sqltrie/serialized.py"
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "It\u2019s a bug. SerializedTrie.shortest_prefix returns the query key instead of the found prefix key. You can see it in src/sqltrie/serialized.py where it unpacks (skey, raw) from the inner trie but returns (key, self._load(skey, raw)): https://github.com/iterative/sqltrie/blob/main/src/sqltrie/serialized.py. This contradicts the contract of the underlying pygtrie (\u201cfinds the shortest prefix of a key with a value\u201d and returns that prefix): https://pygtrie.readthedocs.io/en/stable/#pygtrie.Trie.shortest_prefix. The wrapper delegates straight to pygtrie via PyGTrie.shortest_prefix: https://github.com/iterative/sqltrie/blob/main/src/sqltrie/pygtrie.py. I reproduced it locally in this repo with sqltrie_shortest_prefix_repro.py: querying ('a','b') after storing at ('a',) and ('a','b') prints \u201cActual key: ('a','b')\u201d and \u201cValue: base\u201d, confirming it returns the query key paired with the base value."
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "shortest_prefix should return the prefix key that matched, not the query key. That contract is explicit in pygtrie (\u201creturns (k, value) where k is the shortest prefix of key\u201d) and sqltrie advertises pygtrie-style tries; your repro shows it returns ('a','b') with the value stored at ('a',), i.e., a key/value pair that doesn\u2019t exist"
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqc31mta0742mtwuuorh",
    "global_key": "html_bug_report_c29321f6_7c95251a",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_c29321f6.html",
    "votes": [
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a bug in practical terms because the composite simpson implementation on irregular x uses weights that divide by the product of adjacent spacings h0 and h1 so the weight contains a factor like hsum squared over h0 times h1. when two consecutive x points are extremely close this h product becomes tiny and the weights become huge which makes the result explode. the report shows a minimal repro where simpson is larger than trapezoid by tens of thousands of times and a property based test that the ratio between simpson and trapezoid should stay within a wide band but it does not when there are near duplicate x values. the function also fails silently with no warning which violates reasonable user expectations and the documentation does not warn about this instability for close x values. even if the underlying three point nonuniform simpson formula is mathematically ill conditioned near coincident nodes the library should not return enormous values without signaling the issue. this justifies yes for real bug due to silent and extreme numerical instability on realistic irregular data yes for maintainers welcome due to clear reproduction high user impact obvious mitigation and documentation gap and no for security since it is a correctness and reliability problem not an exploit path. Here are some reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.simpson.html ,https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.trapezoid.html ,https://github.com/scipy/scipy/blob/main/scipy/integrate/_quadrature.py"
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a pure correctness break that makes shortest_prefix unreliable for routing, prefix lookups, and ACL decisions: callers receive a key/value pair that never existed in the trie (e.g., (('a','b'), 'base') when 'base' lives at ('a',)). It also contradicts the documented behavior of pygtrie and is inconsistent with longest_prefix in the same file, which correctly returns (lkey, ...), so this is almost certainly a typo rather than design. The fix is trivial and low-risk- return skey instead of key in src/sqltrie/serialized.py and immediately realigns the API with user expectations and the upstream contract. https://github.com/scipy/scipy/blob/v1.16.2/scipy/integrate/_quadrature.py; https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.simpson.html; https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.trapezoid.html;"
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This looks more like a numerical limitation than a true \"bug.\" Simpson\u2019s rule assumes intervals aren't wildly uneven, so if two x values are almost identical, the math (dividing by a tiny product of spacings) naturally blows up. The code is doing exactly what the algorithm says, but the docs don't warn you about instability in that edge case. So it\u2019s not really wrong, it just needs clearer explanation or maybe a guard/warning for users."
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a known numerical instability of composite Simpson\u2019s rule with highly uneven/near-duplicate x; the huge weights make blow-ups expected rather than a logic error. "
      },
      {
        "rater_id": "cmabh1zs405as070wbkmy521g",
        "Q1": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The issue does not arise due to the spacing values being too small, but rather occurs because the dataset includes irregularly spaced points (e.g., between \ud835\udc65 = 2.0 and \ud835\udc65 = 2.0000076313128856). \n\nIn such cases, the integral is evaluated using the composite Simpson\u2019s rule for irregular intervals, as described in Wikipedia \u2013 https://en.wikipedia.org/wiki/Simpson%27s_rule#cite_note-FOOTNOTEShklov1960-9. \n\nThe result aligns with the expected outcome, consistent with the Python example in the same reference, and is therefore expected behavior."
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqc71mtb07427ltdi9g5",
    "global_key": "html_bug_report_7e872ea6_e6b42717",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_7e872ea6.html",
    "votes": [
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "it\u2019s a bug. For plain strings, disp_trim(data, length) literally slices with data[:length], so a negative length triggers Python\u2019s negative indexing and you get \u201ctrim all but the last |length| chars\u201d instead of \u201creturn empty,\u201d e.g., 'hello'[:-1] == 'hell', which contradicts \u201ctrim-to-display-width\u201d semantics; that slicing behavior is exactly how Python defines negative indices in sequences per the official docs: https://docs.python.org/3/library/stdtypes.html#common-sequence-operations. For ANSI-formatted strings, the function loops while disp_len(data) > length; if length < 0 and data is non-empty, disp_len(data) is always \u2265 0, so the condition never becomes false and the loop never terminates, and ''[:-1] makes no progress- hence the hang. You can see both behaviors in the current implementation of disp_trim and its helpers (RE_ANSI, disp_len, _text_width) in tqdm\u2019s source: https://github.com/tqdm/tqdm/blob/d8ac65641ddfa87c3c6b1f729b3e89bb002fa600/tqdm/utils.py, where disp_len accounts for ANSI control sequences and character cell widths via wcwidth. I reproduced it with a local mirror: plain 'abcdef' at \u22121 yields 'abcde' (unexpected), and ANSI '\\x1b[31mred\\x1b[0m' at \u22121 would hang (capped iterations to avoid a freeze)."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's not really a bug because disp_trim is only meant to trim strings to a non-negative display width, so passing a negative length is basically invalid input. The weird results ('hello'[:-1] \u2192 'hell') and even the infinite loop with ANSI codes just come from using the function outside its intended range, so at most it's missing input validation rather than a logic error."
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Yes. disp_trim treats negative lengths as Python slice indices (returning truncated text) and, with ANSI text, the while disp_len(data) > length condition never terminates for length < 0, leading to a hang."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug. the implementation of tqdm utils disp_trim slices plain ascii data with python negative indexing when length is negative which returns a non empty prefix instead of an empty string so disp_trim hello minus one returns hell which violates the intuitive contract of trimming to a target display width. for ansi or wide character strings the while loop condition uses disp_len data greater than length and since disp_len is always greater than or equal to zero any negative length makes the condition permanently true even after data becomes empty because zero is still greater than a negative length which yields an infinite loop. this inconsistency between ascii and non ascii and the possibility of a hang clearly indicate incorrect logic not an intentional design. the function is used to fit rendered bars to ncols and negative ncols can reach disp_trim because ncols is treated as truthy and there is no validation so user or environment supplied negative widths can trigger this path. the minimal fix is to clamp length to zero before trimming which makes ascii behavior correct and guarantees loop termination for ansi and wide characters. Here is some related references: https://github.com/tqdm/tqdm/blob/v4.67.1/tqdm/utils.py#L386-L399 ,https://docs.python.org/3/library/unicodedata.html#unicodedata.east_asian_width"
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqcb1mtc0742u86831qr",
    "global_key": "html_bug_report_734be9c7_541aa3b2",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_734be9c7.html",
    "votes": [
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "Why is this a bug or not a bug? It\u2019s a bug. OrderingList promises to \u201csync position in a Python list with a position attribute on the mapped objects,\u201d but common list operations don\u2019t uphold that contract. The implementation overrides a few methods (append/insert/remove/pop/reorder) but not others, so extend() and += inherit from list and never set positions. Worse, slice assignment is logically wrong: in __setitem__ for slices it loops for i in range(start, stop, step): self.__setitem__(i, entities[i]), indexing the replacement items by absolute list index rather than by the slice-relative offset. That loses data and violates normal Python list semantics when the replacement length differs. I reproduced it with a local mirror: after extend and +=, positions remain None, and replacing olist[1:2] = [Item(10), Item(20)] keeps only the second replacement and the list length stays 3 instead of 4. Source for confirmation: https://github.com/sqlalchemy/sqlalchemy/blob/main/lib/sqlalchemy/ext/orderinglist.py; https://docs.sqlalchemy.org/en/20/orm/extensions/orderinglist.html; https://docs.python.org/3/library/stdtypes.html#typesseq-mutable"
      },
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "Yes. OrderingList promises to \u201csync position in a Python list with a position attribute,\u201d but extend/+= leave position=None and slice assignment drops/reshuffles items; this violates its stated contract and normal list semantics."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because OrderingList promises to keep item positions in sync with their place in the list, but some common list operations (extend, +=, slice assignment) either don't update positions or even lose data, which breaks both the documented contract and normal Python list behavior."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug because orderinglist states it manages and sync positions, yet extend and in place add do not set the position attribute, leaving none, so the core contract is broken. slice assignment is also wrong because the code indexes the replacement items using absolute indices from the original slice, which drops items and produces the wrong final length, violating normal python list semantics. append and insert do update positions, creating an inconsistent and surprising api where common list operations behave differently. on security, there is no direct exploit, but it can silently corrupt data and misorder records, which can indirectly impact business logic that depends on ordering, so this is a correctness and integrity issue rather than a security flaw."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "comments": "I believe this is a bug, the extend () isn't setting positions correctly. Moreover the ordering list function purpose is to keep track of positions of a the list, since this is failing it can lead to data integrity loss. "
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqce1mtd0742bo7qyjdy",
    "global_key": "html_bug_report_15700d95_d116e882",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_15700d95.html",
    "votes": [
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "_partial_application_url splits host on the first \":\" to extract a port (L93\u2013L101), which mangles IPv6 literals (e.g., ::1, 2001:db8::1) and yields invalid URLs; RFC 3986 requires IPv6 hosts to be bracketed and not parsed this way."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because _partial_application_url can't correctly handle even valid bracketed IPv6 addresses. So, URLs like http://[::1]:8080/ come out broken, which means the function fails at its main job of generating usable URLs."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "the is a real bug because it uses host splitting on the first colon which is incompatible with ipv6 literals that contain multiple colons. this logic corrupts the host by mistaking parts of the ipv6 address for a port and produces malformed urls. the property based test and minimal repro in the report show concrete failures including complete loss of the host and truncated addresses. it violates rfc 3986 which requires ipv6 literals in urls to be enclosed in square brackets and not parsed via naive colon splitting. it also affects public api flows because parse url overrides routes host and _port through this method which means route url and other url generators will emit broken urls in ipv6 scenarios. maintainers would welcome it because it breaks standards compliant behavior and real world ipv6 deployments. it is not a security issue because it causes incorrect url generation rather than enabling injection or bypass it mainly leads to broken links or failed redirects not data exposure or auth bypass. Here is some references: https://www.rfc-editor.org/rfc/rfc3986 .https://docs.pylonsproject.org/projects/pyramid/en/latest/api/request.html#pyramid.request.Request.route_url"
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "It\u2019s a bug. Pyramid\u2019s URLMethodsMixin._partial_application_url uses naive colon-splitting to separate host and port, which corrupts IPv6 literals. In the current code, if port is None it does host, port = host.split(':', 1), and when port is provided it still strips anything after the first colon via host.split(':', 1)\u2014both conflict with IPv6 where colons are part of the address. This yields outputs like http://:8080, http://[:8080, and http://2001:9000 instead of properly bracketed forms per RFC 3986. I reproduced it with a local mirror the runs show:\nhost='::1', port='8080' -> http://:8080\nhost='[::1]', port='8080' -> http://[:8080\nhost='2001:db8::1', port='9000' -> http://2001:9000\nhost='[2001:db8::1]', port='9000' -> http://[2001:9000\nhost='::1', port=None -> http://::1\nhost='example.com', port='8080' -> http://example.com:8080 (control OK) This violates RFC 3986 bracket requirements for IPv6 literals in URIs and breaks real deployments on IPv6/dual-stack.\nhttps://github.com/Pylons/pyramid/blob/main/src/pyramid/url.py; https://www.rfc-editor.org/rfc/rfc3986; https://docs.pylonsproject.org/projects/pyramid/en/latest/api/request.html#pyramid.request.Request.application_url"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "The colons in ipv6 seem to be conflict the port. URLS that are unbracketed violate RFC3986, moreover, malformed URLS of course are in correct and would lead to routing errors. Browsers would reject pyramid if ipv6 is being used"
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqch1mte07423ly2ovfi",
    "global_key": "html_bug_report_f431a5fe_ed5e3458",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_f431a5fe.html",
    "votes": [
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "client.py passes 1-based attempt numbers into strategies that expect 0-based (e.g., ListRetry.timeouts[attempt]), so the first timeout is skipped and the last access raises IndexError."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because ListRetry is supposed to use the timeouts you configure in order, but due to a mismatch between the client using 1-based attempts and the retry strategies expecting 0-based indexing, it skips the first timeout and crashes on the last attempt."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "this is a real bug caused by a one based versus zero based attempt mismatch between the client and the retry strategies. the client increments current attempt before the first call and passes attempt equal to one into get timeout. listretry indexes by attempt so it skips index zero and on the last retry it accesses index len timeouts which raises indexerror even though attempts equals len timeouts. exponentialretry uses start timeout times factor to the power of attempt so the first retry uses factor to the power of one instead of zero and returns a larger wait than configured. fibonacciretry also starts one step ahead because the first call happens after the increment. the correct fix is to pass attempt minus one to get timeout so the first retry uses the first configured value and no indexerror occurs when attempts equals len timeouts. Here are some related references:  https://github.com/inyutin/aiohttp_retry/blob/master/aiohttp_retry/retry_options.py"
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "comments": "It's a bug. The client increments the attempt counter before calling the retry strategy, but ListRetry/ExponentialRetry treat the argument as if it were 0\u2011based. That mismatch skips the first configured timeout/backoff and can raise IndexError on the last attempt. I reproduced it. The manual client\u2011flow demo in my local mirror shows that with timeouts [1.0, 2.0, 3.0], attempt 1 returns 2.0 instead of 1.0 and attempt 3 raises IndexError; with [0.1, 0.2], attempt 1 returns 0.2 and attempt 2 raises IndexError. The sequence check in aiohttp_retry_backoff_repro.py also shows the first exponential step doubled (expected [0.1, 0.2, 0.4, 0.8] vs observed [0.2, 0.4, 0.8, 1.6]) and the first list value skipped. The single\u2011timeout case makes the off\u2011by\u2011one obvious: with timeouts = [1.0] and attempts = 1, the first call passes attempt = 1 and hits timeouts[1], raising IndexError immediately. This flows directly from the pre\u2011increment and direct pass\u2011through in the retry loop in the caller, where the 1\u2011based attempt is handed to the strategy without normalization https://github.com/inyutin/aiohttp_retry/blob/master/aiohttp_retry/client.py. The strategies then use that value as a 0\u2011based index or exponent, for example `self.timeouts[attempt]` in ListRetry and `start_timeout * factor**attempt` in ExponentialRetry, which both assume the first attempt is 0\u2011based https://github.com/inyutin/aiohttp_retry/blob/master/aiohttp_retry/retry_options.py."
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q3": {
          "vote": "no",
          "confidence": "2_low_confidence_significant_uncertainty"
        },
        "comments": "On the third retry it seems that off by one is crashing. Retries are using the next timeout, but not the actual one it should use. The last timeout, in this case 3 is not used and crashing. "
      }
    ]
  },
  {
    "data_row_id": "cmfrkaqck1mtf0742wg93oty1",
    "global_key": "html_bug_report_3ac89196_0186bc92",
    "url": "https://nicholas.carlini.com/tmp/pbt/enhanced_report_3ac89196.html",
    "votes": [
      {
        "rater_id": "cmdjeye5h16lr07y6by2zgaea",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "__rsub__ must compute other - self; the implementation calls self.__sub__(other) (i.e., self - other), flipping the sign and violating Python\u2019s reflected-operator contract."
      },
      {
        "rater_id": "cmcve7ayr0eo907xycl7zf5pb",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug because __rsub__ is supposed to compute other - self when the left operand can't handle subtraction, but the current implementation just does self - other, which flips the sign."
      },
      {
        "rater_id": "cmdcy125l1hqw0716hxbg3200",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "this is a real bug because the money class defines rsub by delegating to sub which computes self minus other rather than the required reflected subtraction other minus self as defined by the python data model. this violates the operator protocol where when a minus b falls back to b rsub a it must still compute a minus b not b minus a, leading to sign inversion and mathematically incorrect results. the behavior is reproducible as m1 minus m2 equals positive seven while m2 rsub m1 returns negative seven, confirming the wrong operand order and sign. maintainers would welcome fixing it because it is a user visible correctness error in a financial type which can affect balances or reports when reflected subtraction paths are used and the change is simple and low risk. it is not a security issue because there is no code execution or boundary bypass, only calculation correctness risk. Here is some related references: https://docs.python.org/3/reference/datamodel.html#object.rsub ,https://pypi.org/project/py-money/"
      },
      {
        "rater_id": "cm3r5nibj00ja070259887rki",
        "Q1": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "5_very_confident_in_decision"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "It's a bug. Because reflected subtraction must compute other - self, not self - other. Python\u2019s data model is explicit: when a - b isn\u2019t handled by a, Python calls b.__rsub__(a), which still has to produce a - b, not flip the operands. I reproduced it. Using a minimal mirror that emulates py\u2011money\u2019s current __rsub__ (it calls self.__sub__(other)), m2.__rsub__(m1) returns USD -7.00 while the correct result is USD 7.00. This matches the Hypothesis counterexample in the report as well: for amount1='0.00', amount2='0.01', __rsub__ returns +0.01 but should be -0.01. The root cause is the operand order inside __rsub__: it computes self - other instead of other - self, violating the reflected operator contract and yielding the wrong sign. This surfaces whenever Python dispatches to the reflected path (e.g., mixed\u2011type expressions or when the left operand returns NotImplemented), so it can flip balances and downstream financial logic. In money/money.py, __rsub__ currently delegates to __sub__; the fix is to validate type/currency, then return other.amount - self.amount (and for unsupported types, return NotImplemented or raise the library\u2019s InvalidOperandError consistently with the rest of the API). Python docs: https://docs.python.org/3/reference/datamodel.html#object.__rsub__"
      },
      {
        "rater_id": "cmd7rudq70kwi07z6exkaf7bc",
        "Q1": {
          "vote": "yes",
          "confidence": "4_confident_with_minor_uncertainty"
        },
        "Q2": {
          "vote": "yes",
          "confidence": "3_moderate_confidence_some_ambiguity"
        },
        "Q3": {
          "vote": "no",
          "confidence": "5_very_confident_in_decision"
        },
        "comments": "This is a bug because the subtraction is being done in reverse order. Which totally  corrupts the value. m1= 10 , m2 = 3(7). However reversing this goes to -7. __rsub__ is incorrectly calculating m2-m1. "
      }
    ]
  }
]