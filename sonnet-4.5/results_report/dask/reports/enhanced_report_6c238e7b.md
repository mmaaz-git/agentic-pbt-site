# Bug Report: dask.bytes.read_bytes Generates Duplicate Offsets with not_zero Parameter

**Target**: `dask.bytes.core.read_bytes`
**Severity**: Medium
**Bug Type**: Logic
**Date**: 2025-09-25

## Summary

The `read_bytes` function generates duplicate offsets when `not_zero=True` is combined with small file sizes, violating the fundamental invariant that file block offsets must be strictly increasing.

## Property-Based Test

```python
#!/usr/bin/env python3
"""
Property-based test for dask.bytes.read_bytes offset generation.
Tests the invariant that offsets should be strictly increasing.
"""

from hypothesis import given, strategies as st, settings

@given(
    st.integers(min_value=1, max_value=100000),
    st.integers(min_value=1, max_value=10000),
    st.booleans()
)
@settings(max_examples=500)
def test_blocksize_calculation_invariants(size, blocksize, not_zero):
    """
    Test that offsets generated by the read_bytes logic are strictly increasing.
    This is a fundamental invariant - blocks should not overlap.
    """
    # Simulate the exact logic from dask.bytes.core.read_bytes
    if size % blocksize and size > blocksize:
        blocksize1 = size / (size // blocksize)
    else:
        blocksize1 = blocksize

    place = 0
    off = [0]
    length = []

    while size - place > (blocksize1 * 2) - 1:
        place += blocksize1
        off.append(int(place))
        length.append(off[-1] - off[-2])
    length.append(size - off[-1])

    if not_zero:
        off[0] = 1
        length[0] -= 1

    # Check the invariant: offsets must be strictly increasing
    for i in range(1, len(off)):
        assert off[i] > off[i-1], (
            f"Offsets not increasing at index {i}: "
            f"off={off}, size={size}, blocksize={blocksize}, not_zero={not_zero}"
        )

if __name__ == "__main__":
    print("Running property-based test for dask.bytes.read_bytes offset generation...")
    print("Testing invariant: offsets must be strictly increasing")
    print()

    try:
        test_blocksize_calculation_invariants()
        print("All tests passed!")
    except AssertionError as e:
        print(f"Test failed with assertion error:")
        print(f"  {e}")
        print()
        print("This demonstrates a bug where duplicate offsets are generated,")
        print("violating the fundamental assumption that file blocks don't overlap.")
```

<details>

<summary>
**Failing input**: `size=2, blocksize=1, not_zero=True`
</summary>
```
Running property-based test for dask.bytes.read_bytes offset generation...
Testing invariant: offsets must be strictly increasing

Test failed with assertion error:
  Offsets not increasing at index 1: off=[1, 1], size=2, blocksize=1, not_zero=True

This demonstrates a bug where duplicate offsets are generated,
violating the fundamental assumption that file blocks don't overlap.
```
</details>

## Reproducing the Bug

```python
#!/usr/bin/env python3
"""
Minimal reproduction case for dask.bytes.read_bytes duplicate offset bug.
This demonstrates how the function generates duplicate offsets when
not_zero=True and the file size is small relative to blocksize.
"""

# Simulate the exact logic from dask.bytes.core.read_bytes (lines 124-141)
def simulate_read_bytes_offsets(size, blocksize, not_zero):
    """
    Simulates the offset calculation logic from dask.bytes.core.read_bytes.
    This is extracted directly from the implementation.
    """
    # From lines 124-127: Adjust blocksize if needed
    if size % blocksize and size > blocksize:
        blocksize1 = size / (size // blocksize)
    else:
        blocksize1 = blocksize

    # From lines 128-130: Initialize
    place = 0
    off = [0]
    length = []

    # From lines 133-136: Generate offsets
    while size - place > (blocksize1 * 2) - 1:
        place += blocksize1
        off.append(int(place))
        length.append(off[-1] - off[-2])

    # From line 137: Add final length
    length.append(size - off[-1])

    # From lines 139-141: Apply not_zero adjustment
    if not_zero:
        off[0] = 1
        length[0] -= 1

    return off, length

# Reproduce the bug with the failing input
size = 2
blocksize = 1
not_zero = True

print("=== Reproducing dask.bytes.read_bytes duplicate offset bug ===")
print(f"Input parameters:")
print(f"  size = {size}")
print(f"  blocksize = {blocksize}")
print(f"  not_zero = {not_zero}")
print()

offsets, lengths = simulate_read_bytes_offsets(size, blocksize, not_zero)

print(f"Generated offsets: {offsets}")
print(f"Generated lengths: {lengths}")
print()

# Check for the bug: duplicate offsets
print("=== Bug Analysis ===")
if len(offsets) > 1 and offsets[0] == offsets[1]:
    print(f"BUG DETECTED: Duplicate offsets found!")
    print(f"  offsets[0] = {offsets[0]}")
    print(f"  offsets[1] = {offsets[1]}")
    print()
    print("This violates the invariant that offsets should be strictly increasing.")
    print("Two blocks would start reading from the same position in the file,")
    print("causing data duplication or loss.")
else:
    print("No duplicate offsets found.")

# Verify the invariant that offsets should be strictly increasing
print()
print("=== Offset Invariant Check ===")
for i in range(1, len(offsets)):
    if offsets[i] <= offsets[i-1]:
        print(f"INVARIANT VIOLATED at index {i}:")
        print(f"  offsets[{i-1}] = {offsets[i-1]}")
        print(f"  offsets[{i}] = {offsets[i]}")
        print(f"  Expected: offsets[{i}] > offsets[{i-1}]")
        break
else:
    print("All offsets are strictly increasing (invariant satisfied).")
```

<details>

<summary>
Output showing duplicate offsets at positions 0 and 1
</summary>
```
=== Reproducing dask.bytes.read_bytes duplicate offset bug ===
Input parameters:
  size = 2
  blocksize = 1
  not_zero = True

Generated offsets: [1, 1]
Generated lengths: [0, 1]

=== Bug Analysis ===
BUG DETECTED: Duplicate offsets found!
  offsets[0] = 1
  offsets[1] = 1

This violates the invariant that offsets should be strictly increasing.
Two blocks would start reading from the same position in the file,
causing data duplication or loss.

=== Offset Invariant Check ===
INVARIANT VIOLATED at index 1:
  offsets[0] = 1
  offsets[1] = 1
  Expected: offsets[1] > offsets[0]
```
</details>

## Why This Is A Bug

This bug violates a fundamental invariant of file block reading systems: **offsets must be strictly increasing to ensure non-overlapping blocks**. The issue occurs in the implementation at `/home/npc/pbt/agentic-pbt/envs/dask_env/lib/python3.13/site-packages/dask/bytes/core.py:139-141`.

When `not_zero=True` is set, the code unconditionally modifies the first offset:
```python
if not_zero:
    off[0] = 1
    length[0] -= 1
```

This modification doesn't check whether `off[1]` (if it exists) is also 1, which can happen when:
1. The file size is very small (e.g., 2 bytes)
2. The blocksize is 1 or comparable to the file size
3. The offset generation loop creates `off = [0, 1]`
4. Setting `off[0] = 1` creates duplicate offsets `[1, 1]`

The duplicate offsets mean:
- Two blocks would start reading from the same byte position
- The first block would have zero length (`length[0] = 0`)
- Data would be read incorrectly, potentially causing duplication or loss
- The blocks would not properly cover the entire file sequentially

This contradicts the function's documented purpose of "cleanly breaking data" and the implicit contract that blocks should partition the file into non-overlapping segments.

## Relevant Context

The `not_zero` parameter is documented as "Force seek of start-of-file delimiter, discarding header." It's intended to skip header bytes by starting reads from position 1 instead of 0. This is useful for files with headers that should be ignored.

The bug manifests specifically when:
- Files are very small (< 3 bytes with blocksize=1)
- The combination of size and blocksize results in the second offset being 1
- `not_zero=True` is enabled

While this is an edge case involving tiny files, it's still a correctness issue that violates the core invariants of the block reading system. Production systems using Dask for processing many small files with headers could encounter silent data corruption.

The code is located at: `/home/npc/pbt/agentic-pbt/envs/dask_env/lib/python3.13/site-packages/dask/bytes/core.py`
- Offset generation: lines 124-137
- Bug location: lines 139-141

## Proposed Fix

```diff
--- a/dask/bytes/core.py
+++ b/dask/bytes/core.py
@@ -137,8 +137,12 @@ def read_bytes(
                 length.append(size - off[-1])

                 if not_zero:
-                    off[0] = 1
-                    length[0] -= 1
+                    if len(off) > 1 and off[1] <= 1:
+                        # If setting off[0]=1 would create duplicate/invalid offsets,
+                        # remove the first block entirely
+                        off = off[1:]
+                        length = length[1:]
+                    else:
+                        off[0] = 1
+                        length[0] -= 1
                 offsets.append(off)
                 lengths.append(length)
```