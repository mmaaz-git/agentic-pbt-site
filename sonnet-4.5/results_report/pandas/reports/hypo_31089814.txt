============================= test session starts ==============================
platform linux -- Python 3.13.2, pytest-8.4.1, pluggy-1.5.0 -- /home/npc/miniconda/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default'
rootdir: /home/npc/pbt/agentic-pbt/worker_/14
plugins: anyio-4.9.0, hypothesis-6.139.1, asyncio-1.2.0, langsmith-0.4.29
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 1 item

hypo.py::test_clean_column_name_returns_hashable FAILED                  [100%]

=================================== FAILURES ===================================
___________________ test_clean_column_name_returns_hashable ____________________

    @given(st.text(min_size=1, max_size=50))
>   def test_clean_column_name_returns_hashable(name):
                   ^^^

hypo.py:5: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
hypo.py:7: in test_clean_column_name_returns_hashable
    result = clean_column_name(name)
             ^^^^^^^^^^^^^^^^^^^^^^^
/home/npc/miniconda/lib/python3.13/site-packages/pandas/core/computation/parsing.py:130: in clean_column_name
    tokval = next(tokenized)[1]
             ^^^^^^^^^^^^^^^
/home/npc/miniconda/lib/python3.13/site-packages/pandas/core/computation/parsing.py:189: in tokenize_string
    for toknum, tokval, start, _, _ in token_generator:
                                       ^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

source = <built-in method readline of _io.StringIO object at 0x751394d96b00>
encoding = None, extra_tokens = True

    def _generate_tokens_from_c_tokenizer(source, encoding=None, extra_tokens=False):
        """Tokenize a source reading Python code as unicode strings using the internal C tokenizer"""
        if encoding is None:
            it = _tokenize.TokenizerIter(source, extra_tokens=extra_tokens)
        else:
            it = _tokenize.TokenizerIter(source, encoding=encoding, extra_tokens=extra_tokens)
        try:
            for info in it:
                yield TokenInfo._make(info)
        except SyntaxError as e:
            if type(e) != SyntaxError:
                raise e from None
            msg = _transform_msg(e.msg)
>           raise TokenError(msg, (e.lineno, e.offset)) from None
E           tokenize.TokenError: ('source code cannot contain null bytes', (1, 0))
E           Falsifying example: test_clean_column_name_returns_hashable(
E               name='\x00',
E           )

/home/npc/miniconda/lib/python3.13/tokenize.py:588: TokenError
=========================== short test summary info ============================
FAILED hypo.py::test_clean_column_name_returns_hashable - tokenize.TokenError...
============================== 1 failed in 0.45s ===============================
