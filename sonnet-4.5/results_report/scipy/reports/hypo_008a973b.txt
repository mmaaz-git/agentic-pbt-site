Traceback (most recent call last):
  File "/home/npc/pbt/agentic-pbt/worker_/1/hypo.py", line 27, in <module>
    test_entropy_kl_divergence_nonnegative()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/home/npc/pbt/agentic-pbt/worker_/1/hypo.py", line 9, in test_entropy_kl_divergence_nonnegative
    pk=st.lists(st.floats(min_value=0.0, max_value=1.0, allow_nan=False, allow_infinity=False), min_size=2, max_size=50),
               ^^^
  File "/home/npc/miniconda/lib/python3.13/site-packages/hypothesis/core.py", line 2124, in wrapped_test
    raise the_error_hypothesis_found
  File "/home/npc/pbt/agentic-pbt/worker_/1/hypo.py", line 24, in test_entropy_kl_divergence_nonnegative
    assert kl >= 0, f"KL divergence {kl} is negative"
           ^^^^^^^
AssertionError: KL divergence -7.041887703187054e-211 is negative
Falsifying example: test_entropy_kl_divergence_nonnegative(
    pk=[2.0007560879758843e-213, 1.0],
    qk=[1.432375450419533e-60, 1.0],
)
